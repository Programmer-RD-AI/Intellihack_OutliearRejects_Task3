{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e54dc6c1d454098ab19a5d1d6fd67e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b3a47ea0bd14e18b8804d43d552deb9",
              "IPY_MODEL_fcce5364759648ca97594c2892bf6b95",
              "IPY_MODEL_7b7048c4e587449eab6b5303af33857e",
              "IPY_MODEL_a679f98d08194954800d1c64d85960a6",
              "IPY_MODEL_f4b675bdb3194731ab0e0360c64fbda4"
            ],
            "layout": "IPY_MODEL_1d982ea057284702a6b6bf05ba3b58d5"
          }
        },
        "0b3a47ea0bd14e18b8804d43d552deb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8169bf53ce304ecd9ac00c91d570e20c",
            "placeholder": "​",
            "style": "IPY_MODEL_d90d69f3387346159084bd4867545b74",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "fcce5364759648ca97594c2892bf6b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_31abc2f11d334cf39e28a991a12eb154",
            "placeholder": "​",
            "style": "IPY_MODEL_6f1045c9653f4be5b8c8f40ded72b7eb",
            "value": ""
          }
        },
        "7b7048c4e587449eab6b5303af33857e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_475c2d59ac1443acb5411153ee68022d",
            "style": "IPY_MODEL_5a8fa1fced454d2b87a09527e6f87454",
            "value": true
          }
        },
        "a679f98d08194954800d1c64d85960a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f57ab3ff023b4b7991f7a8d36a1ae8c8",
            "style": "IPY_MODEL_30bd8400222645969d280f7d1fe630ce",
            "tooltip": ""
          }
        },
        "f4b675bdb3194731ab0e0360c64fbda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57793c57a7a640e6a0d7e4c83f8730c6",
            "placeholder": "​",
            "style": "IPY_MODEL_74dfaf0c73a243cf90f570b5fd972dd4",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "1d982ea057284702a6b6bf05ba3b58d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "8169bf53ce304ecd9ac00c91d570e20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d90d69f3387346159084bd4867545b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31abc2f11d334cf39e28a991a12eb154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f1045c9653f4be5b8c8f40ded72b7eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "475c2d59ac1443acb5411153ee68022d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8fa1fced454d2b87a09527e6f87454": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f57ab3ff023b4b7991f7a8d36a1ae8c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30bd8400222645969d280f7d1fe630ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "57793c57a7a640e6a0d7e4c83f8730c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74dfaf0c73a243cf90f570b5fd972dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "678872e552c34725a9a907c9624869d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3efa823b01c347d3b6d960ce443de94d",
              "IPY_MODEL_b556f1fdba364632934ff1e3ab578547",
              "IPY_MODEL_cb2cf43a95cb42e9a98f68578d4e63b7"
            ],
            "layout": "IPY_MODEL_21057437fcd24899baacc80e56cb2fc0"
          }
        },
        "3efa823b01c347d3b6d960ce443de94d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3b14c663e0b4f1c8634a9fcbd9ae85d",
            "placeholder": "​",
            "style": "IPY_MODEL_7c2c8682896b4932b5cc5397354e190e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b556f1fdba364632934ff1e3ab578547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ed243f128c45d1ae88e489a42bc8ec",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0040bece314d4eb59f5b330c65906a82",
            "value": 4
          }
        },
        "cb2cf43a95cb42e9a98f68578d4e63b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca73c47260fb457980d45f48b96fac80",
            "placeholder": "​",
            "style": "IPY_MODEL_b1e2e1251f2b446bb3b732cd63b3df4f",
            "value": " 4/4 [00:10&lt;00:00,  2.64s/it]"
          }
        },
        "21057437fcd24899baacc80e56cb2fc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b14c663e0b4f1c8634a9fcbd9ae85d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c2c8682896b4932b5cc5397354e190e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ed243f128c45d1ae88e489a42bc8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0040bece314d4eb59f5b330c65906a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca73c47260fb457980d45f48b96fac80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1e2e1251f2b446bb3b732cd63b3df4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0df93d1dfa7c45eba3e1d75fd3a3c8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef31d749be7743518d6bfaaef91058a7",
              "IPY_MODEL_8c47876410254de5ab13c05e9df5c472",
              "IPY_MODEL_599fa7eea4a94625ab94afddb170d66f"
            ],
            "layout": "IPY_MODEL_656aaf03f81e47eb9d0ac9f698e1d170"
          }
        },
        "ef31d749be7743518d6bfaaef91058a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8340881399d4f959d34ff02a5ef3bdc",
            "placeholder": "​",
            "style": "IPY_MODEL_cece402e7eb1478085b54e805e32232e",
            "value": "Creating json from Arrow format: 100%"
          }
        },
        "8c47876410254de5ab13c05e9df5c472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3570eaa4fcf40d7919bf13b165541cf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8249a98efa3d4a07af5accc968e65d0d",
            "value": 1
          }
        },
        "599fa7eea4a94625ab94afddb170d66f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44df9d65b07b43458c5ec7b9918750de",
            "placeholder": "​",
            "style": "IPY_MODEL_f80260627ad9403eb83050a73ddb4b46",
            "value": " 1/1 [00:00&lt;00:00, 84.46ba/s]"
          }
        },
        "656aaf03f81e47eb9d0ac9f698e1d170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8340881399d4f959d34ff02a5ef3bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cece402e7eb1478085b54e805e32232e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3570eaa4fcf40d7919bf13b165541cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8249a98efa3d4a07af5accc968e65d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44df9d65b07b43458c5ec7b9918750de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f80260627ad9403eb83050a73ddb4b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d2d29cd7c49490883e4ef03c20ba6fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b609b0df17b14d929736cce942e3e253",
              "IPY_MODEL_b1173f22eed04627b5c48dab7e99a306",
              "IPY_MODEL_86b4f8a2653d4f2ebf1a1f53141a9006"
            ],
            "layout": "IPY_MODEL_a1e9d5bae0364910a356f43c88f5fb43"
          }
        },
        "b609b0df17b14d929736cce942e3e253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a37ece513d634c658add2608b545ab23",
            "placeholder": "​",
            "style": "IPY_MODEL_e9aa71453fcf4f7281a8f22a45e739e0",
            "value": "Creating json from Arrow format: 100%"
          }
        },
        "b1173f22eed04627b5c48dab7e99a306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffee3ec15bd84dd68ea80fc8f9d710d9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04d9f68ad4434febafed493cfc9dc87e",
            "value": 1
          }
        },
        "86b4f8a2653d4f2ebf1a1f53141a9006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd2fb1f16254414aa9b28866d41f90b1",
            "placeholder": "​",
            "style": "IPY_MODEL_e70e9860ee56466188e2fa6dc572a1a6",
            "value": " 1/1 [00:00&lt;00:00, 110.48ba/s]"
          }
        },
        "a1e9d5bae0364910a356f43c88f5fb43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a37ece513d634c658add2608b545ab23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9aa71453fcf4f7281a8f22a45e739e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffee3ec15bd84dd68ea80fc8f9d710d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04d9f68ad4434febafed493cfc9dc87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd2fb1f16254414aa9b28866d41f90b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e70e9860ee56466188e2fa6dc572a1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1847e804ef604423b752065435028b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea9c5bbdc5c24618a4a3129dd98413a6",
              "IPY_MODEL_702085fb7af143beba4740be2791a865",
              "IPY_MODEL_7eb7465711254ddb966c7fc3791943c0"
            ],
            "layout": "IPY_MODEL_5ec8b608d664457a9648a48c1dc29e7a"
          }
        },
        "ea9c5bbdc5c24618a4a3129dd98413a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85e02a8e9e394ac4805511bac25d8d1f",
            "placeholder": "​",
            "style": "IPY_MODEL_ee36070d15614388ba35e6fe6bd17cde",
            "value": "Creating json from Arrow format: 100%"
          }
        },
        "702085fb7af143beba4740be2791a865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97868945923440c0b2813ab5bbb90f49",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a968b0f9f1f54137aa579ddfca212da9",
            "value": 1
          }
        },
        "7eb7465711254ddb966c7fc3791943c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dd6f7a38f67420c93d18178cc4a4449",
            "placeholder": "​",
            "style": "IPY_MODEL_3edf7e821bc04f11a4e50bd7e21c9409",
            "value": " 1/1 [00:00&lt;00:00, 103.46ba/s]"
          }
        },
        "5ec8b608d664457a9648a48c1dc29e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e02a8e9e394ac4805511bac25d8d1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee36070d15614388ba35e6fe6bd17cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97868945923440c0b2813ab5bbb90f49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a968b0f9f1f54137aa579ddfca212da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0dd6f7a38f67420c93d18178cc4a4449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3edf7e821bc04f11a4e50bd7e21c9409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57f4e0cd651041bd9a202bc55ffe6c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a922fb8c90774f4eb65fcc3310751e81",
              "IPY_MODEL_07ebf603efcd4cd6b82fc3418d140f9f",
              "IPY_MODEL_da47bd67a80941bc9a1acb9f2dea12cd"
            ],
            "layout": "IPY_MODEL_29e12f867d264fc28e1353114debd3b4"
          }
        },
        "a922fb8c90774f4eb65fcc3310751e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5757162d9d2e4ec09a39946d10994dd5",
            "placeholder": "​",
            "style": "IPY_MODEL_adb78ee77ccf4b96afaecdf1c6892ee4",
            "value": "Generating train split: "
          }
        },
        "07ebf603efcd4cd6b82fc3418d140f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3880bf427ac3401999c2fc64d75fe319",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bc202bcfa4d4c0aa135eebb453e35c4",
            "value": 1
          }
        },
        "da47bd67a80941bc9a1acb9f2dea12cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c253c8fe7b424976981b1ac74b44ac3c",
            "placeholder": "​",
            "style": "IPY_MODEL_589d53df87a24c8ca293e896f646d7ad",
            "value": " 3/0 [00:00&lt;00:00, 204.08 examples/s]"
          }
        },
        "29e12f867d264fc28e1353114debd3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5757162d9d2e4ec09a39946d10994dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adb78ee77ccf4b96afaecdf1c6892ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3880bf427ac3401999c2fc64d75fe319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5bc202bcfa4d4c0aa135eebb453e35c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c253c8fe7b424976981b1ac74b44ac3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "589d53df87a24c8ca293e896f646d7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70377359d69e481da01170212bf021c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b2ecf41eb3a49d8995f0929c2e2f718",
              "IPY_MODEL_66c1eba5238844eaa144f9979348793b",
              "IPY_MODEL_dfeb0458e7214a48b045cd357ff22453"
            ],
            "layout": "IPY_MODEL_1853883e6b3540679ec39e5e94f1324a"
          }
        },
        "2b2ecf41eb3a49d8995f0929c2e2f718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efaedeed2a6e492e8ac64aa0fed89f9d",
            "placeholder": "​",
            "style": "IPY_MODEL_feb2971e003b4398b4e1a63fe25a5914",
            "value": "Generating train split: "
          }
        },
        "66c1eba5238844eaa144f9979348793b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39b377489e7649cca582ea7533d95678",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef439ef9f0d4407b9fead5036ff65f70",
            "value": 1
          }
        },
        "dfeb0458e7214a48b045cd357ff22453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd6bafe15ed14e83adb0584fd6d31cd6",
            "placeholder": "​",
            "style": "IPY_MODEL_d3f7fcffff584f37b50c3c34e6e14d3d",
            "value": " 1/0 [00:00&lt;00:00, 76.34 examples/s]"
          }
        },
        "1853883e6b3540679ec39e5e94f1324a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efaedeed2a6e492e8ac64aa0fed89f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feb2971e003b4398b4e1a63fe25a5914": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39b377489e7649cca582ea7533d95678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ef439ef9f0d4407b9fead5036ff65f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd6bafe15ed14e83adb0584fd6d31cd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3f7fcffff584f37b50c3c34e6e14d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef6f332aa7584fba869d3d125de8579c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_851839054ed14d0e918a649ea32453f3",
              "IPY_MODEL_5516ae75adcf4e3b8b979e544aa1f33f",
              "IPY_MODEL_162281bfffea48f08e4680be2bcf2709"
            ],
            "layout": "IPY_MODEL_872a257fbcee49398cd3b9c032ccaf8a"
          }
        },
        "851839054ed14d0e918a649ea32453f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_574d0296086049698c0a3d61b65cbe72",
            "placeholder": "​",
            "style": "IPY_MODEL_0c1b90c0af8e448596aad503195cc513",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5516ae75adcf4e3b8b979e544aa1f33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bc2f64b1ea24b9daf33834088330eaf",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06a2adc7755c4a37a74d10d797251401",
            "value": 2
          }
        },
        "162281bfffea48f08e4680be2bcf2709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da503f55405427d87d181f7ecbd2e81",
            "placeholder": "​",
            "style": "IPY_MODEL_6e550baf83b845e0aaa6b853543aa4e0",
            "value": " 2/2 [00:03&lt;00:00,  1.85s/it]"
          }
        },
        "872a257fbcee49398cd3b9c032ccaf8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "574d0296086049698c0a3d61b65cbe72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c1b90c0af8e448596aad503195cc513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bc2f64b1ea24b9daf33834088330eaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a2adc7755c4a37a74d10d797251401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9da503f55405427d87d181f7ecbd2e81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e550baf83b845e0aaa6b853543aa4e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e732b7b8ed744cfa214824ca2807fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85453a7eb8f549f28dcc94ca8729ee8c",
              "IPY_MODEL_5bb141b572004083862de5cbda456093",
              "IPY_MODEL_ef3ebe9e4d074e98850c2f6ccdeb1930"
            ],
            "layout": "IPY_MODEL_5cf6366ce8634280bbf226133d5ac842"
          }
        },
        "85453a7eb8f549f28dcc94ca8729ee8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c094ba2a2c04d7f89f99c7969455370",
            "placeholder": "​",
            "style": "IPY_MODEL_77e8be90908248da9be66a44a032b879",
            "value": "Map: 100%"
          }
        },
        "5bb141b572004083862de5cbda456093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19b9a70e82334c48aff2db32f5112bbb",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0291855d486e4286b4e541253739140c",
            "value": 3
          }
        },
        "ef3ebe9e4d074e98850c2f6ccdeb1930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_527bad67f3ab427189dafe99b8d2fea0",
            "placeholder": "​",
            "style": "IPY_MODEL_a180c8764d4b467aa304b0da545c3019",
            "value": " 3/3 [00:00&lt;00:00, 70.67 examples/s]"
          }
        },
        "5cf6366ce8634280bbf226133d5ac842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c094ba2a2c04d7f89f99c7969455370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77e8be90908248da9be66a44a032b879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19b9a70e82334c48aff2db32f5112bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0291855d486e4286b4e541253739140c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "527bad67f3ab427189dafe99b8d2fea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a180c8764d4b467aa304b0da545c3019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "968f660799784483acf00e9e6a9a6334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b661bad389541e38f9cd8fc78c415fa",
              "IPY_MODEL_94432ece8c3b48bc8984a609b5f14fa0",
              "IPY_MODEL_3644107f63d04938b00f0590e60346bb"
            ],
            "layout": "IPY_MODEL_ea29502dae19445c858b6df182916ad2"
          }
        },
        "4b661bad389541e38f9cd8fc78c415fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_540feca764824764bcf80555ee2b5258",
            "placeholder": "​",
            "style": "IPY_MODEL_28ecef50860e45e89e448e39026aefe9",
            "value": "Map: 100%"
          }
        },
        "94432ece8c3b48bc8984a609b5f14fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e377b3927c445779a5344d4a983ab43",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b67a7b482404aa69e083bc5e707da28",
            "value": 1
          }
        },
        "3644107f63d04938b00f0590e60346bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a8616989bb14a4eb24c6554f9529043",
            "placeholder": "​",
            "style": "IPY_MODEL_7b6a399c510d409489c970bb5884b1d8",
            "value": " 1/1 [00:00&lt;00:00, 51.50 examples/s]"
          }
        },
        "ea29502dae19445c858b6df182916ad2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "540feca764824764bcf80555ee2b5258": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ecef50860e45e89e448e39026aefe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e377b3927c445779a5344d4a983ab43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b67a7b482404aa69e083bc5e707da28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a8616989bb14a4eb24c6554f9529043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6a399c510d409489c970bb5884b1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "836074441c104f5c8c808e2c05232e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a534ac40f7ee4aaabd40e7a285160a32",
              "IPY_MODEL_aade4cdbcf8242b3a8c06df51d70b506",
              "IPY_MODEL_5067606ef6154958a4ef02f408c697a9"
            ],
            "layout": "IPY_MODEL_d588b13a41824b2797d2a923367cdd08"
          }
        },
        "a534ac40f7ee4aaabd40e7a285160a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf28cb8d945449395694444237c8654",
            "placeholder": "​",
            "style": "IPY_MODEL_70781fa1342343ef8712929d052876c4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "aade4cdbcf8242b3a8c06df51d70b506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c15623d67b0c4a9182b0912692b39ed5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e2f4c4701b24f04a3a0de89133f9322",
            "value": 2
          }
        },
        "5067606ef6154958a4ef02f408c697a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51378e3b93c14f4aa54debe5cabcb14b",
            "placeholder": "​",
            "style": "IPY_MODEL_6975f6a60fd649839885638d64b049de",
            "value": " 2/2 [00:04&lt;00:00,  2.05s/it]"
          }
        },
        "d588b13a41824b2797d2a923367cdd08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf28cb8d945449395694444237c8654": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70781fa1342343ef8712929d052876c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c15623d67b0c4a9182b0912692b39ed5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e2f4c4701b24f04a3a0de89133f9322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51378e3b93c14f4aa54debe5cabcb14b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6975f6a60fd649839885638d64b049de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5534a5f8b1ac45b39edbac5c6a164521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e78ca0d8bb4470d8225796a7c1d7c59",
              "IPY_MODEL_db1fc4e220734ecfbd86d6d7edd0ecdc",
              "IPY_MODEL_495832ceb7bb4759a9b40286200b4bd9"
            ],
            "layout": "IPY_MODEL_826e765975454e209c89ae5e395a98e3"
          }
        },
        "1e78ca0d8bb4470d8225796a7c1d7c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c60a06edc316476893948b4a4277bf8e",
            "placeholder": "​",
            "style": "IPY_MODEL_749ad764db4c404e9aedecdf225daf8b",
            "value": "Generating train split: "
          }
        },
        "db1fc4e220734ecfbd86d6d7edd0ecdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30d081531ac4408295275595b4ec0530",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd57d86bf8234461856e7d26bbcc4147",
            "value": 1
          }
        },
        "495832ceb7bb4759a9b40286200b4bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a3ba5dbb8794d5aaad0bcec5fa62c66",
            "placeholder": "​",
            "style": "IPY_MODEL_36ba926d1e614ced9cdb55c05cadf5de",
            "value": " 1/0 [00:00&lt;00:00, 74.28 examples/s]"
          }
        },
        "826e765975454e209c89ae5e395a98e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60a06edc316476893948b4a4277bf8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749ad764db4c404e9aedecdf225daf8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30d081531ac4408295275595b4ec0530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "dd57d86bf8234461856e7d26bbcc4147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a3ba5dbb8794d5aaad0bcec5fa62c66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36ba926d1e614ced9cdb55c05cadf5de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2eca93c6e26e441fa5bc6aee92d51fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a694757687504b32b83eb6d67ef73e2d",
              "IPY_MODEL_c5686ab94f4542c7901b727a6b302fa7",
              "IPY_MODEL_2c4cdb148ea041bd9193b576021f600d"
            ],
            "layout": "IPY_MODEL_7472e5b662b6472487c5d7c56ada62de"
          }
        },
        "a694757687504b32b83eb6d67ef73e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_436964aa562d42368dd8c640cc538e90",
            "placeholder": "​",
            "style": "IPY_MODEL_9983e63c8b664b6aa6c4c1565438037c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c5686ab94f4542c7901b727a6b302fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26bafdb9e3c74da08ad5072ace3248a4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a545abf008e45dcbd40f803e84254da",
            "value": 2
          }
        },
        "2c4cdb148ea041bd9193b576021f600d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9b6e53aedf2458595d615ed8f61d652",
            "placeholder": "​",
            "style": "IPY_MODEL_032efa8da42843028a35a28197f2176b",
            "value": " 2/2 [00:02&lt;00:00,  1.30s/it]"
          }
        },
        "7472e5b662b6472487c5d7c56ada62de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "436964aa562d42368dd8c640cc538e90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9983e63c8b664b6aa6c4c1565438037c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26bafdb9e3c74da08ad5072ace3248a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a545abf008e45dcbd40f803e84254da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9b6e53aedf2458595d615ed8f61d652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "032efa8da42843028a35a28197f2176b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!cd drive/ && ls"
      ],
      "metadata": {
        "id": "opdLejGbD7J6",
        "outputId": "afc1f3e3-8c5e-46d6-cf34-aab6beab6cfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login(\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386,
          "referenced_widgets": [
            "5e54dc6c1d454098ab19a5d1d6fd67e1",
            "0b3a47ea0bd14e18b8804d43d552deb9",
            "fcce5364759648ca97594c2892bf6b95",
            "7b7048c4e587449eab6b5303af33857e",
            "a679f98d08194954800d1c64d85960a6",
            "f4b675bdb3194731ab0e0360c64fbda4",
            "1d982ea057284702a6b6bf05ba3b58d5",
            "8169bf53ce304ecd9ac00c91d570e20c",
            "d90d69f3387346159084bd4867545b74",
            "31abc2f11d334cf39e28a991a12eb154",
            "6f1045c9653f4be5b8c8f40ded72b7eb",
            "475c2d59ac1443acb5411153ee68022d",
            "5a8fa1fced454d2b87a09527e6f87454",
            "f57ab3ff023b4b7991f7a8d36a1ae8c8",
            "30bd8400222645969d280f7d1fe630ce",
            "57793c57a7a640e6a0d7e4c83f8730c6",
            "74dfaf0c73a243cf90f570b5fd972dd4"
          ]
        },
        "id": "wad34zPjLNq0",
        "outputId": "6f13f002-7064-495a-d3e4-ac842d5d2cd4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'notebook_login': pass new_session='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' as keyword args. From version 1.0 passing these as positional arguments will result in an error,\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e54dc6c1d454098ab19a5d1d6fd67e1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip dataset.zip"
      ],
      "metadata": {
        "id": "Fcdqt04JELEr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets llama-cpp-python faiss-cpu rouge_score bitsandbytes\n",
        "!pip install -U bitsandbytes\n",
        "!pip install --upgrade transformers\n",
        "!pip install mpi4py\n",
        "!pip install -U deepseed"
      ],
      "metadata": {
        "id": "PcRs8428LSge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0951e5b1-a57a-4dfb-a53b-745e722b35a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.11/dist-packages (4.0.3)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement deepseed (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for deepseed\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    AutoModel,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "import wandb\n",
        "import subprocess\n",
        "import shutil\n",
        "import argparse\n",
        "from typing import List, Optional, Dict, Any\n",
        "import time\n",
        "from llama_cpp import Llama\n",
        "import faiss\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "import gc\n",
        "import nltk\n",
        "import json\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "# Set this environment variable to avoid memory fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Clear cache at startup\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "IFVejZVgLOw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bb48b5-9583-425e-a69b-a4e63cd69611"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # or \"Qwen/Qwen2.5-3B\"\n",
        "DATA_DIR = \"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/fine_tuned_model\"\n",
        "MAX_LENGTH = 2048\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 10\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_CONFIG = LoraConfig(\n",
        "    r=32,  # rank\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "plKpXvlhGkPp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    def __init__(self, documents_dir):\n",
        "        self.documents_dir = Path(documents_dir)\n",
        "        self.documents = {}\n",
        "        self.load_documents()\n",
        "\n",
        "    def load_documents(self):\n",
        "        \"\"\"Load all markdown documents from the specified directory.\"\"\"\n",
        "        for file_path in self.documents_dir.glob(\"*.md\"):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                # Extract title from filename or first heading in the document\n",
        "                title = file_path.stem\n",
        "                self.documents[title] = content\n",
        "        print(f\"Loaded {len(self.documents)} documents.\")\n",
        "\n",
        "    def chunk_documents(self, chunk_size=1500, overlap=150):\n",
        "        \"\"\"Chunk documents into smaller pieces with overlap.\"\"\"\n",
        "        chunked_docs = []\n",
        "\n",
        "        for title, content in self.documents.items():\n",
        "            # Remove markdown formatting for cleaner text\n",
        "            text = re.sub(r\"```.*?```\", \"\", content, flags=re.DOTALL)\n",
        "            text = re.sub(r\"#+ \", \"\", text)\n",
        "            text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)\n",
        "\n",
        "            # Split into sentences (rough approximation)\n",
        "            sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "\n",
        "            chunks = []\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "\n",
        "            for sentence in sentences:\n",
        "                sentence_length = len(sentence.split())\n",
        "                if current_length + sentence_length > chunk_size:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "                    # Start new chunk with overlap\n",
        "                    overlap_tokens = (\n",
        "                        current_chunk[-overlap:]\n",
        "                        if overlap < len(current_chunk)\n",
        "                        else current_chunk\n",
        "                    )\n",
        "                    current_chunk = overlap_tokens + [sentence]\n",
        "                    current_length = len(current_chunk)\n",
        "                else:\n",
        "                    current_chunk.append(sentence)\n",
        "                    current_length += sentence_length\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunked_docs.append(\n",
        "                    {\n",
        "                        \"title\": title,\n",
        "                        \"chunk_id\": i,\n",
        "                        \"text\": chunk.strip(),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        return chunked_docs\n",
        "\n",
        "\n",
        "class QAGenerator:\n",
        "    def __init__(self, model_name=\"Qwen/Qwen1.5-7B-Chat\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "    def generate_qa_pairs(self, chunks, num_questions_per_chunk=3):\n",
        "        qa_pairs = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i + 1}/{len(chunks)}\")\n",
        "\n",
        "            # Better prompt with examples to avoid placeholders\n",
        "            prompt = f\"\"\"Given the text below from an AI research paper, generate {num_questions_per_chunk} detailed question-answer pairs.\n",
        "\n",
        "    TEXT:\n",
        "    {chunk[\"text\"]}\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    - Create substantive, specific questions about key concepts in the text\n",
        "    - Write comprehensive answers using information directly from the text\n",
        "    - DO NOT generate generic or placeholder questions\n",
        "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
        "    - Use this exact format for each pair:\n",
        "\n",
        "    Q1: What is [specific concept from text]?\n",
        "    A1: [Detailed answer explaining the concept based on the text]\n",
        "\n",
        "    Here are {num_questions_per_chunk} question-answer pairs about this text:\n",
        "    \"\"\"\n",
        "            try:\n",
        "                inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\n",
        "                    self.model.device\n",
        "                )\n",
        "\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=1024,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                    repetition_penalty=1.2,\n",
        "                    do_sample=True,\n",
        "                )\n",
        "\n",
        "                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                # Debug output\n",
        "                print(f\"\\nModel response for chunk {i + 1}:\")\n",
        "                print(response[-200:])  # Show the last 200 characters\n",
        "\n",
        "                # Extract questions and answers\n",
        "                pairs = self.extract_qa_pairs(response)\n",
        "\n",
        "                print(f\"Extracted {len(pairs)} QA pairs from chunk {i + 1}\")\n",
        "\n",
        "                if len(pairs) == 0:\n",
        "                    # Fallback prompt with even more explicit instructions\n",
        "                    fallback_prompt = f\"\"\"I need exactly {num_questions_per_chunk} question-answer pairs about this AI research text.\n",
        "\n",
        "    TEXT:\n",
        "    {chunk[\"text\"]}\n",
        "\n",
        "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
        "    Q1: [Real specific question about the content]\n",
        "    A1: [Real detailed answer from the content]\n",
        "\n",
        "    Q2: [Real specific question about the content]\n",
        "    A2: [Real detailed answer from the content]\n",
        "\n",
        "    Q3: [Real specific question about the content]\n",
        "    A3: [Real detailed answer from the content]\n",
        "    \"\"\"\n",
        "                    inputs = self.tokenizer(fallback_prompt, return_tensors=\"pt\").to(\n",
        "                        self.model.device\n",
        "                    )\n",
        "\n",
        "                    outputs = self.model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=1024,\n",
        "                        temperature=0.8,  # Slightly higher to encourage creativity\n",
        "                        top_p=0.92,\n",
        "                        repetition_penalty=1.3,\n",
        "                        do_sample=True,\n",
        "                    )\n",
        "\n",
        "                    response = self.tokenizer.decode(\n",
        "                        outputs[0], skip_special_tokens=True\n",
        "                    )\n",
        "                    pairs = self.extract_qa_pairs(response)\n",
        "                    print(f\"Fallback attempt extracted {len(pairs)} QA pairs\")\n",
        "\n",
        "                for q, a in pairs:\n",
        "                    # Improved filtering logic\n",
        "                    placeholder_phrases = [\n",
        "                        \"write a\",\n",
        "                        \"specific detailed question\",\n",
        "                        \"comprehensive answer\",\n",
        "                        \"[question\",\n",
        "                        \"[answer\",\n",
        "                        \"question here\",\n",
        "                        \"answer here\",\n",
        "                    ]\n",
        "\n",
        "                    # Check if either question or answer has placeholder text\n",
        "                    is_placeholder = False\n",
        "                    for phrase in placeholder_phrases:\n",
        "                        if phrase.lower() in q.lower() or phrase.lower() in a.lower():\n",
        "                            is_placeholder = True\n",
        "                            break\n",
        "\n",
        "                    if (\n",
        "                        not is_placeholder\n",
        "                        and len(q.strip()) > 10\n",
        "                        and len(a.strip()) > 20\n",
        "                    ):  # Better length checks\n",
        "                        qa_pairs.append(\n",
        "                            {\n",
        "                                \"title\": chunk[\"title\"],\n",
        "                                \"chunk_id\": chunk[\"chunk_id\"],\n",
        "                                \"context\": chunk[\"text\"],\n",
        "                                \"question\": q.strip(),\n",
        "                                \"answer\": a.strip(),\n",
        "                            }\n",
        "                        )\n",
        "                    else:\n",
        "                        print(\n",
        "                            f\"Rejected pair - Q: {q[:30]}... ({len(q.strip())} chars), A: {a[:30]}... ({len(a.strip())} chars)\"\n",
        "                        )\n",
        "                        if is_placeholder:\n",
        "                            print(\"  Reason: Contains placeholder text\")\n",
        "                        else:\n",
        "                            print(\"  Reason: Too short\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing chunk {i + 1}: {e}\")\n",
        "                continue  # Skip this chunk but continue with others\n",
        "\n",
        "        # Ensure we have at least some data\n",
        "        if len(qa_pairs) == 0:\n",
        "            # Try one more time with a different model if available, using a smaller chunk\n",
        "            try:\n",
        "                print(\"Trying with a different approach for at least some data...\")\n",
        "                # Take a small subset of chunks to ensure we get something\n",
        "                small_chunks = chunks[: min(5, len(chunks))]\n",
        "\n",
        "                # Manually create at least one QA pair as a last resort\n",
        "                for chunk in small_chunks:\n",
        "                    # Extract a simple question from first sentence\n",
        "                    sentences = chunk[\"text\"].split(\". \")\n",
        "                    if len(sentences) > 1:\n",
        "                        first_sentence = sentences[0].strip()\n",
        "                        # Create a \"what\" question from first sentence\n",
        "                        words = first_sentence.split()\n",
        "                        if len(words) > 5:\n",
        "                            question = (\n",
        "                                f\"What does the text say about {' '.join(words[1:4])}?\"\n",
        "                            )\n",
        "                            answer = (\n",
        "                                first_sentence + \". \" + sentences[1]\n",
        "                                if len(sentences) > 1\n",
        "                                else first_sentence\n",
        "                            )\n",
        "\n",
        "                            qa_pairs.append(\n",
        "                                {\n",
        "                                    \"title\": chunk[\"title\"],\n",
        "                                    \"chunk_id\": chunk[\"chunk_id\"],\n",
        "                                    \"context\": chunk[\"text\"],\n",
        "                                    \"question\": question,\n",
        "                                    \"answer\": answer,\n",
        "                                }\n",
        "                            )\n",
        "            except Exception as e:\n",
        "                print(f\"Emergency data creation also failed: {e}\")\n",
        "                # If all else fails, raise the error\n",
        "                raise ValueError(\n",
        "                    \"No QA pairs were generated. Check the model outputs and extraction logic.\"\n",
        "                )\n",
        "\n",
        "        return qa_pairs\n",
        "\n",
        "    def extract_qa_pairs(self, text):\n",
        "        \"\"\"Extract question-answer pairs with robust pattern matching\"\"\"\n",
        "        # Try multiple regex patterns for different possible formats\n",
        "        patterns = [\n",
        "            # Standard format: Q1: question\\nA1: answer\n",
        "            r\"Q(\\d+)[\\s:]+(.*?)[\\s\\n]+A\\1[\\s:]+(.*?)(?=[\\s\\n]+Q\\d+[\\s:]|$)\",\n",
        "            # Alternative format: Question 1: question\\nAnswer 1: answer\n",
        "            r\"Question\\s*(\\d+)[\\s:]+(.*?)[\\s\\n]+Answer\\s*\\1[\\s:]+(.*?)(?=[\\s\\n]+Question\\s*\\d+[\\s:]|$)\",\n",
        "            # Simple format: Q: question\\nA: answer\n",
        "            r\"Q:[\\s]+(.*?)[\\s\\n]+A:[\\s]+(.*?)(?=[\\s\\n]+Q:[\\s]|$)\",\n",
        "        ]\n",
        "\n",
        "        all_pairs = []\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "            # Process matches based on capture group structure\n",
        "            pairs = []\n",
        "            for match in matches:\n",
        "                if len(match) == 3:  # Numbered format with 3 capture groups\n",
        "                    _, question, answer = match\n",
        "                elif len(match) == 2:  # Simple format with 2 capture groups\n",
        "                    question, answer = match\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                question = question.strip()\n",
        "                answer = answer.strip()\n",
        "\n",
        "                # Filter out template placeholders\n",
        "                if (\"[\" in question and \"]\" in question) or (\n",
        "                    \"[\" in answer and \"]\" in answer\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                if question and answer:  # Ensure both are non-empty\n",
        "                    pairs.append((question, answer))\n",
        "\n",
        "            if pairs:  # If we found pairs with this pattern, add them\n",
        "                all_pairs.extend(pairs)\n",
        "                print(f\"Pattern matched {len(pairs)} valid pairs\")\n",
        "\n",
        "        # Add detailed debugging output\n",
        "        print(f\"Total extracted: {len(all_pairs)} valid pairs\")\n",
        "        if len(all_pairs) == 0:\n",
        "            print(\"DEBUG - Model response excerpt:\")\n",
        "            print(text[:500])  # Print beginning of response\n",
        "            print(\"...\")\n",
        "            print(text[-500:])  # Print end of response\n",
        "\n",
        "        return all_pairs\n",
        "\n",
        "def create_synthetic_data(documents_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/dataset/q3_dataset\", output_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\"):\n",
        "    # Process documents\n",
        "    processor = DocumentProcessor(documents_dir)\n",
        "    chunks = processor.chunk_documents()\n",
        "\n",
        "    # Generate QA pairs\n",
        "    generator = QAGenerator()\n",
        "    qa_pairs = generator.generate_qa_pairs(chunks)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = Dataset.from_list(qa_pairs)\n",
        "\n",
        "    # Create train/validation/test splits\n",
        "    splits = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_valid = splits[\"train\"]\n",
        "    test = splits[\"test\"]\n",
        "\n",
        "    # Further split train into train and validation\n",
        "    splits = train_valid.train_test_split(\n",
        "        test_size=0.25, seed=42\n",
        "    )  # 0.25 * 0.8 = 0.2 of original data\n",
        "    train = splits[\"train\"]\n",
        "    validation = splits[\"test\"]\n",
        "\n",
        "    # Save datasets\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    train.to_json(os.path.join(output_dir, \"train.json\"))\n",
        "    validation.to_json(os.path.join(output_dir, \"validation.json\"))\n",
        "    test.to_json(os.path.join(output_dir, \"test.json\"))\n",
        "\n",
        "    print(\n",
        "        f\"Dataset created with {len(train)} training, {len(validation)} validation, and {len(test)} test examples.\"\n",
        "    )\n",
        "    return train, validation, test"
      ],
      "metadata": {
        "id": "TZ-OKGRYGo2P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QAFineTuner:\n",
        "    def __init__(self, model_name, data_dir, output_dir):\n",
        "        self.model_name = model_name\n",
        "        self.data_dir = data_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.train_dataset = None\n",
        "        self.validation_dataset = None\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and preprocess the datasets.\"\"\"\n",
        "        train_path = os.path.join(self.data_dir, \"train.json\")\n",
        "        validation_path = os.path.join(self.data_dir, \"validation.json\")\n",
        "\n",
        "        self.train_dataset = load_dataset(\"json\", data_files=train_path)[\"train\"]\n",
        "        self.validation_dataset = load_dataset(\"json\", data_files=validation_path)[\n",
        "            \"train\"\n",
        "        ]\n",
        "\n",
        "        print(\n",
        "            f\"Loaded {len(self.train_dataset)} training examples and {len(self.validation_dataset)} validation examples.\"\n",
        "        )\n",
        "\n",
        "    def prepare_model(self):\n",
        "        \"\"\"Load and prepare the model with LoRA.\"\"\"\n",
        "        # Clear memory before model loading\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        try:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            import bitsandbytes\n",
        "\n",
        "            print(f\"Using bitsandbytes version: {bitsandbytes.__version__}\")\n",
        "\n",
        "            # Configure quantization for memory efficiency\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "            )\n",
        "\n",
        "            # Load model with quantization\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\",\n",
        "                quantization_config=bnb_config,\n",
        "                use_cache=False,  # Important for training\n",
        "            )\n",
        "\n",
        "            # Apply LoRA adapter\n",
        "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
        "\n",
        "            print(\"Successfully loaded model with 4-bit quantization and LoRA adapters\")\n",
        "\n",
        "        except (ImportError, ModuleNotFoundError) as e:\n",
        "            print(f\"Warning: Could not use quantization: {e}\")\n",
        "            print(\"Falling back to CPU loading with offloading\")\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"cpu\",\n",
        "                low_cpu_mem_usage=True,\n",
        "            )\n",
        "\n",
        "            # Apply LoRA adapter\n",
        "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
        "\n",
        "            # Move to GPU selectively if possible\n",
        "            try:\n",
        "                self.model.to_bettertransformer()\n",
        "            except:\n",
        "                print(\"Could not convert to BetterTransformer\")\n",
        "\n",
        "        # Print trainable parameters info\n",
        "        self.model.print_trainable_parameters()\n",
        "\n",
        "    def format_instruction(self, example):\n",
        "        \"\"\"Format the input as an instruction.\"\"\"\n",
        "        context = example[\"context\"]\n",
        "        question = example[\"question\"]\n",
        "        answer = example[\"answer\"]\n",
        "\n",
        "        instruction = f\"\"\"### System:\n",
        "You are an AI assistant that specializes in answering questions about AI research papers.\n",
        "Your responses should be comprehensive, accurate, and based on the provided context.\n",
        "\n",
        "### Human:\n",
        "I have a question about an AI research paper.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "### Assistant:\n",
        "{answer}\n",
        "\"\"\"\n",
        "        return instruction\n",
        "\n",
        "    def tokenize_function(self, examples):\n",
        "        \"\"\"Tokenize and format the examples.\"\"\"\n",
        "        instructions = []\n",
        "\n",
        "        for i in range(len(examples[\"context\"])):\n",
        "            example = {\n",
        "                \"context\": examples[\"context\"][i],\n",
        "                \"question\": examples[\"question\"][i],\n",
        "                \"answer\": examples[\"answer\"][i],\n",
        "            }\n",
        "            instructions.append(self.format_instruction(example))\n",
        "\n",
        "        tokenized = self.tokenizer(\n",
        "            instructions,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "        return tokenized\n",
        "\n",
        "    def prepare_datasets(self):\n",
        "        \"\"\"Prepare tokenized datasets for training.\"\"\"\n",
        "        tokenize_batch_size = 8\n",
        "\n",
        "        self.train_dataset = self.train_dataset.map(\n",
        "            self.tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=tokenize_batch_size,\n",
        "            remove_columns=self.train_dataset.column_names,\n",
        "        )\n",
        "\n",
        "        self.validation_dataset = self.validation_dataset.map(\n",
        "            self.tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=tokenize_batch_size,\n",
        "            remove_columns=self.validation_dataset.column_names,\n",
        "        )\n",
        "\n",
        "        print(f\"Tokenized datasets: {self.train_dataset}, {self.validation_dataset}\")\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the model.\"\"\"\n",
        "        # Clear CUDA cache\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Initialize wandb for tracking\n",
        "        wandb.init(project=\"qwen-ai-research-qa\", name=\"qwen-2.5-3b-qlora\")\n",
        "\n",
        "        # Make sure no DeepSpeed configurations are active\n",
        "        for key in list(os.environ.keys()):\n",
        "            if \"DEEPSPEED\" in key or \"DS_\" in key:\n",
        "                del os.environ[key]\n",
        "\n",
        "        # Configure training arguments with NO DeepSpeed\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            num_train_epochs=NUM_EPOCHS,\n",
        "            per_device_train_batch_size=1,\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=16,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "            weight_decay=0.01,\n",
        "            warmup_ratio=0.1,\n",
        "            logging_dir=os.path.join(self.output_dir, \"logs\"),\n",
        "            logging_steps=50,\n",
        "            eval_steps=1000,\n",
        "            save_steps=1000,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            save_total_limit=2,\n",
        "            load_best_model_at_end=True,\n",
        "            report_to=\"wandb\",\n",
        "            # Switch to standard FP32 precision\n",
        "            bf16=False,\n",
        "            fp16=False,\n",
        "            # DeepSpeed settings - force disable\n",
        "            deepspeed=None,\n",
        "            local_rank=-1,\n",
        "            ddp_backend=None,  # Don't use any distributed backend\n",
        "        )\n",
        "\n",
        "        # Create trainer with standard optimizer\n",
        "        from transformers import AdamW\n",
        "\n",
        "        optimizer = AdamW(self.model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "        # Create data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "        )\n",
        "\n",
        "        # Create trainer with explicit optimizer\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=self.train_dataset,\n",
        "            eval_dataset=self.validation_dataset,\n",
        "            data_collator=data_collator,\n",
        "            optimizers=(optimizer, None),  # Use our optimizer, no scheduler\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the final model\n",
        "        self.model.save_pretrained(os.path.join(self.output_dir, \"final\"))\n",
        "        self.tokenizer.save_pretrained(os.path.join(self.output_dir, \"final\"))\n",
        "\n",
        "        print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "DpckIntfGs6K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ModelQuantizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/fine_tuned_model/final\",\n",
        "        base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        output_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model\",\n",
        "    ):\n",
        "        self.model_path = model_path\n",
        "        self.base_model = base_model\n",
        "        self.output_dir = output_dir\n",
        "        self.quantized_model_path = os.path.join(output_dir, \"model.gguf\")\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def load_and_merge_model(self):\n",
        "        \"\"\"Load the LoRA model and merge with the base model.\"\"\"\n",
        "        print(\"Loading base model...\")\n",
        "\n",
        "        # Load base model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.base_model,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "        # Load LoRA weights\n",
        "        print(\"Loading and merging LoRA weights...\")\n",
        "        model = PeftModel.from_pretrained(base_model, self.model_path)\n",
        "\n",
        "        # Merge LoRA weights with base model\n",
        "        model = model.merge_and_unload()\n",
        "\n",
        "        # Save merged model and tokenizer\n",
        "        merged_model_path = os.path.join(self.output_dir, \"merged\")\n",
        "        os.makedirs(merged_model_path, exist_ok=True)\n",
        "\n",
        "        print(f\"Saving merged model to {merged_model_path}...\")\n",
        "        model.save_pretrained(merged_model_path)\n",
        "\n",
        "        # Save tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.base_model)\n",
        "        tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "        print(\"Model and tokenizer saved successfully.\")\n",
        "        return merged_model_path\n",
        "\n",
        "    def convert_to_gguf(self, merged_model_path):\n",
        "        \"\"\"Convert the merged model to GGUF format with 4-bit quantization.\"\"\"\n",
        "        print(\"Converting to GGUF format with 4-bit quantization...\")\n",
        "\n",
        "        # Check for existing GGUF model\n",
        "        if os.path.exists(self.quantized_model_path):\n",
        "            print(f\"GGUF model already exists at {self.quantized_model_path}\")\n",
        "            user_input = input(\"Do you want to rebuild it? (y/n): \").lower()\n",
        "            if user_input != \"y\":\n",
        "                print(\"Using existing GGUF model.\")\n",
        "                return self.quantized_model_path\n",
        "\n",
        "        # Clone llama.cpp repository if needed\n",
        "        if not os.path.exists(\"llama.cpp\"):\n",
        "            try:\n",
        "                print(\"Cloning llama.cpp repository...\")\n",
        "                subprocess.run(\n",
        "                    [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\"],\n",
        "                    check=True,\n",
        "                )\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"Error cloning llama.cpp repository.\")\n",
        "                raise RuntimeError(\"Failed to clone llama.cpp repository\")\n",
        "\n",
        "        # Build llama.cpp with better error handling\n",
        "        try:\n",
        "            print(\"Building llama.cpp with CMake (this may take a few minutes)...\")\n",
        "            os.makedirs(\"llama.cpp/build\", exist_ok=True)\n",
        "\n",
        "            # Configure with CMake\n",
        "            subprocess.run(\n",
        "                [\"cmake\", \"-S\", \"llama.cpp\", \"-B\", \"llama.cpp/build\"], check=True\n",
        "            )\n",
        "\n",
        "            # Build with CMake\n",
        "            subprocess.run(\n",
        "                [\"cmake\", \"--build\", \"llama.cpp/build\", \"--parallel\"], check=True\n",
        "            )\n",
        "\n",
        "            print(\"llama.cpp built successfully with CMake\")\n",
        "\n",
        "            # Use convert_hf_to_gguf.py with verbose output to see what's happening\n",
        "            convert_script = \"llama.cpp/convert_hf_to_gguf.py\"\n",
        "\n",
        "            if not os.path.exists(convert_script):\n",
        "                print(f\"ERROR: {convert_script} not found!\")\n",
        "                print(\"Please verify your llama.cpp installation.\")\n",
        "                raise RuntimeError(f\"Conversion script not found: {convert_script}\")\n",
        "\n",
        "            print(f\"\\nRunning conversion script with enhanced debugging...\")\n",
        "\n",
        "            # Try conversion with detailed error output\n",
        "            try:\n",
        "                result = subprocess.run(\n",
        "                    [\n",
        "                        \"python3\",\n",
        "                        convert_script,\n",
        "                        merged_model_path,\n",
        "                        \"--outfile\",\n",
        "                        self.quantized_model_path,\n",
        "                        \"--outtype\",\n",
        "                        \"q4_0\",\n",
        "                        \"--verbose\",  # Add verbose output\n",
        "                    ],\n",
        "                    check=True,\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                )\n",
        "                print(result.stdout)\n",
        "\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(\"\\n===== Conversion Error Details =====\")\n",
        "                print(f\"Exit code: {e.returncode}\")\n",
        "                print(f\"STDOUT: {e.stdout}\")\n",
        "                print(f\"STDERR: {e.stderr}\")\n",
        "                print(\"===================================\\n\")\n",
        "\n",
        "                print(\n",
        "                    \"Trying alternate conversion approach with arch-specific parameters...\"\n",
        "                )\n",
        "                try:\n",
        "                    # Try with explicit model architecture parameters\n",
        "                    result = subprocess.run(\n",
        "                        [\n",
        "                            \"python3\",\n",
        "                            convert_script,\n",
        "                            merged_model_path,\n",
        "                            \"--outfile\",\n",
        "                            self.quantized_model_path,\n",
        "                            \"--outtype\",\n",
        "                            \"q4_0\",\n",
        "                            \"--model-type\",\n",
        "                            \"llama\",  # Try forcing llama architecture\n",
        "                            \"--ctx\",\n",
        "                            \"4096\",\n",
        "                        ],\n",
        "                        check=True,\n",
        "                        capture_output=True,\n",
        "                        text=True,\n",
        "                    )\n",
        "                    print(result.stdout)\n",
        "\n",
        "                except subprocess.CalledProcessError as e2:\n",
        "                    print(f\"Alternate approach also failed\")\n",
        "                    print(f\"STDOUT: {e2.stdout}\")\n",
        "                    print(f\"STDERR: {e2.stderr}\")\n",
        "                    raise RuntimeError(\"All conversion methods failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during build or conversion process: {e}\")\n",
        "            raise RuntimeError(\"Failed to convert model to GGUF format\")\n",
        "\n",
        "        print(\n",
        "            f\"Model successfully converted to GGUF format: {self.quantized_model_path}\"\n",
        "        )\n",
        "\n",
        "        # Copy tokenizer files to output directory\n",
        "        tokenizer_files = [\"tokenizer_config.json\", \"tokenizer.json\"]\n",
        "        for file in tokenizer_files:\n",
        "            src_path = os.path.join(merged_model_path, file)\n",
        "            if os.path.exists(src_path):\n",
        "                dst_path = os.path.join(self.output_dir, file)\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "\n",
        "        return self.quantized_model_path\n",
        "\n",
        "    def quantize(self):\n",
        "        \"\"\"Perform the complete quantization process.\"\"\"\n",
        "        merged_model_path = self.load_and_merge_model()\n",
        "        gguf_path = self.convert_to_gguf(merged_model_path)\n",
        "        return gguf_path"
      ],
      "metadata": {
        "id": "iWliG5x-GwPY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model_path=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/model.gguf\",\n",
        "                 data_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\"):\n",
        "        self.data_dir = data_dir\n",
        "        self.inference = ModelInference(model_path=model_path, use_rag=True)\n",
        "        self.inference_no_rag = ModelInference(model_path=model_path, use_rag=False)\n",
        "\n",
        "        # Download necessary NLTK data\n",
        "        try:\n",
        "            nltk.data.find(\"punkt\")\n",
        "        except LookupError:\n",
        "            nltk.download(\"punkt\")\n",
        "\n",
        "        # Initialize ROUGE scorer\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
        "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True\n",
        "        )\n",
        "        self.smooth = SmoothingFunction().method1\n",
        "\n",
        "    def load_test_data(self):\n",
        "        \"\"\"Load the test dataset.\"\"\"\n",
        "        test_path = os.path.join(self.data_dir, \"test.json\")\n",
        "        return load_dataset(\"json\", data_files=test_path)[\"train\"]\n",
        "\n",
        "    def calculate_metrics(self, reference, candidate):\n",
        "        \"\"\"Calculate BLEU and ROUGE scores.\"\"\"\n",
        "        # ROUGE scores\n",
        "        rouge_scores = self.rouge_scorer.score(reference, candidate)\n",
        "\n",
        "        # BLEU score\n",
        "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
        "        candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
        "        bleu_score = sentence_bleu(\n",
        "            [reference_tokens], candidate_tokens, smoothing_function=self.smooth\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"bleu\": bleu_score,\n",
        "            \"rouge1\": rouge_scores[\"rouge1\"].fmeasure,\n",
        "            \"rouge2\": rouge_scores[\"rouge2\"].fmeasure,\n",
        "            \"rougeL\": rouge_scores[\"rougeL\"].fmeasure,\n",
        "        }\n",
        "\n",
        "    def evaluate(self, sample_size=None):\n",
        "        \"\"\"Evaluate the model on the test set and save results to a JSON file.\"\"\"\n",
        "        test_data = self.load_test_data()\n",
        "\n",
        "        # Limit evaluation to sample_size if specified\n",
        "        if sample_size is not None:\n",
        "            test_data = test_data.select(range(min(sample_size, len(test_data))))\n",
        "\n",
        "        results_with_rag = []\n",
        "        results_without_rag = []\n",
        "\n",
        "        print(f\"Evaluating on {len(test_data)} test examples...\")\n",
        "\n",
        "        for i, example in enumerate(test_data):\n",
        "            print(f\"Processing example {i + 1}/{len(test_data)}...\")\n",
        "\n",
        "            question = example[\"question\"]\n",
        "            reference_answer = example[\"answer\"]\n",
        "\n",
        "            # Generate answers with and without RAG\n",
        "            answer_with_rag = self.inference.generate_answer(question)\n",
        "            answer_without_rag = self.inference_no_rag.generate_answer(question)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics_with_rag = self.calculate_metrics(reference_answer, answer_with_rag)\n",
        "            metrics_without_rag = self.calculate_metrics(reference_answer, answer_without_rag)\n",
        "\n",
        "            # Store results\n",
        "            results_with_rag.append(\n",
        "                {\n",
        "                    \"question\": question,\n",
        "                    \"reference\": reference_answer,\n",
        "                    \"prediction\": answer_with_rag,\n",
        "                    **metrics_with_rag,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            results_without_rag.append(\n",
        "                {\n",
        "                    \"question\": question,\n",
        "                    \"reference\": reference_answer,\n",
        "                    \"prediction\": answer_without_rag,\n",
        "                    **metrics_without_rag,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_metrics_with_rag = {\n",
        "            \"bleu\": sum(r[\"bleu\"] for r in results_with_rag) / len(results_with_rag),\n",
        "            \"rouge1\": sum(r[\"rouge1\"] for r in results_with_rag) / len(results_with_rag),\n",
        "            \"rouge2\": sum(r[\"rouge2\"] for r in results_with_rag) / len(results_with_rag),\n",
        "            \"rougeL\": sum(r[\"rougeL\"] for r in results_with_rag) / len(results_with_rag),\n",
        "        }\n",
        "\n",
        "        avg_metrics_without_rag = {\n",
        "            \"bleu\": sum(r[\"bleu\"] for r in results_without_rag) / len(results_without_rag),\n",
        "            \"rouge1\": sum(r[\"rouge1\"] for r in results_without_rag) / len(results_without_rag),\n",
        "            \"rouge2\": sum(r[\"rouge2\"] for r in results_without_rag) / len(results_without_rag),\n",
        "            \"rougeL\": sum(r[\"rougeL\"] for r in results_without_rag) / len(results_without_rag),\n",
        "        }\n",
        "\n",
        "        print(\"\\nEvaluation Results:\")\n",
        "        print(\"\\nWith RAG:\")\n",
        "        for metric, value in avg_metrics_with_rag.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\nWithout RAG:\")\n",
        "        for metric, value in avg_metrics_without_rag.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        # Prepare the overall results dictionary\n",
        "        results = {\n",
        "            \"with_rag\": {\n",
        "                \"detailed_results\": results_with_rag,\n",
        "                \"average_metrics\": avg_metrics_with_rag,\n",
        "            },\n",
        "            \"without_rag\": {\n",
        "                \"detailed_results\": results_without_rag,\n",
        "                \"average_metrics\": avg_metrics_without_rag,\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Save the results to a JSON file named \"metrics.json\"\n",
        "        with open(\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/metrics.json\", \"w\") as json_file:\n",
        "            json.dump(results, json_file, indent=4)\n",
        "\n",
        "        print(\"\\nResults saved to metrics.json\")\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "SosmF2eRtCiI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel:\n",
        "    def __init__(self, model_name=\"BAAI/bge-small-en-v1.5\"):\n",
        "        self.model_name = model_name\n",
        "        # Use CPU for embeddings to save GPU memory\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        # Get embedding dimension from the model\n",
        "        self.embedding_dim = self.model.config.hidden_size\n",
        "\n",
        "    def get_embedding_dim(self):\n",
        "        \"\"\"Return the embedding dimension of the model.\"\"\"\n",
        "        return self.embedding_dim\n",
        "\n",
        "    def get_embeddings(self, texts: List[str], batch_size=16) -> np.ndarray:\n",
        "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i : i + batch_size]\n",
        "\n",
        "            # Tokenize\n",
        "            inputs = self.tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(self.device)\n",
        "\n",
        "            # Generate embeddings\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**inputs)\n",
        "                batch_embeddings = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
        "\n",
        "            embeddings.append(batch_embeddings)\n",
        "\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "class VectorStore:\n",
        "    def __init__(self, embedding_dim=768):\n",
        "        self.index = faiss.IndexFlatL2(\n",
        "            embedding_dim\n",
        "        )  # L2 distance for similarity search\n",
        "        self.texts = []\n",
        "\n",
        "    def add_texts(self, texts: List[str], embeddings: np.ndarray):\n",
        "        \"\"\"Add texts and their embeddings to the vector store.\"\"\"\n",
        "        # Add embeddings to index\n",
        "        self.index.add(embeddings)\n",
        "        # Store original texts\n",
        "        self.texts.extend(texts)\n",
        "\n",
        "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for most similar texts given a query embedding.\"\"\"\n",
        "        # Reshape query embedding\n",
        "        query_embedding = query_embedding.reshape(1, -1)\n",
        "\n",
        "        # Search in the index\n",
        "        distances, indices = self.index.search(query_embedding, k)\n",
        "\n",
        "        # Build results\n",
        "        results = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx < len(self.texts) and idx >= 0:\n",
        "                results.append(\n",
        "                    {\n",
        "                        \"text\": self.texts[idx],\n",
        "                        \"score\": float(distances[0][i]),\n",
        "                        \"id\": int(idx),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self, data_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\"):\n",
        "        self.embedding_model = EmbeddingModel()\n",
        "        # Use the actual embedding dimension from the model\n",
        "        self.vector_store = VectorStore(\n",
        "            embedding_dim=self.embedding_model.get_embedding_dim()\n",
        "        )\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def build_index(self, force_rebuild=False):\n",
        "        \"\"\"Build the vector index from the dataset chunks.\"\"\"\n",
        "        index_file = os.path.join(self.data_dir, \"vector_index.faiss\")\n",
        "        texts_file = os.path.join(self.data_dir, \"vector_texts.npy\")\n",
        "\n",
        "        # Load from disk if exists and not forced to rebuild\n",
        "        if (\n",
        "            os.path.exists(index_file)\n",
        "            and os.path.exists(texts_file)\n",
        "            and not force_rebuild\n",
        "        ):\n",
        "            self.vector_store.index = faiss.read_index(index_file)\n",
        "            self.vector_store.texts = np.load(texts_file, allow_pickle=True).tolist()\n",
        "            print(\n",
        "                f\"Loaded existing index with {len(self.vector_store.texts)} documents.\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        # Load datasets\n",
        "        print(\"Building vector index...\")\n",
        "\n",
        "        # Load train, validation, test datasets\n",
        "        train_path = os.path.join(self.data_dir, \"train.json\")\n",
        "        validation_path = os.path.join(self.data_dir, \"validation.json\")\n",
        "        test_path = os.path.join(self.data_dir, \"test.json\")\n",
        "\n",
        "        train_data = load_dataset(\"json\", data_files=train_path)[\"train\"]\n",
        "        validation_data = load_dataset(\"json\", data_files=validation_path)[\"train\"]\n",
        "        test_data = load_dataset(\"json\", data_files=test_path)[\"train\"]\n",
        "\n",
        "        # Combine all contexts\n",
        "        all_contexts = []\n",
        "        seen_contexts = set()\n",
        "\n",
        "        # Helper to add unique contexts\n",
        "        def add_unique_contexts(dataset):\n",
        "            for item in dataset:\n",
        "                context = item[\"context\"]\n",
        "                if context not in seen_contexts:\n",
        "                    all_contexts.append(context)\n",
        "                    seen_contexts.add(context)\n",
        "\n",
        "        add_unique_contexts(train_data)\n",
        "        add_unique_contexts(validation_data)\n",
        "        add_unique_contexts(test_data)\n",
        "\n",
        "        print(f\"Found {len(all_contexts)} unique contexts.\")\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeddings = self.embedding_model.get_embeddings(all_contexts)\n",
        "\n",
        "        # Add to vector store\n",
        "        self.vector_store.add_texts(all_contexts, embeddings)\n",
        "\n",
        "        # Save to disk\n",
        "        faiss.write_index(self.vector_store.index, index_file)\n",
        "        np.save(texts_file, np.array(self.vector_store.texts, dtype=object))\n",
        "\n",
        "        print(f\"Built and saved index with {len(all_contexts)} documents.\")\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve relevant contexts for a query.\"\"\"\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.get_embeddings([query])[0]\n",
        "\n",
        "        # Search in vector store\n",
        "        results = self.vector_store.search(query_embedding, k=k)\n",
        "\n",
        "        # Return contexts\n",
        "        return [item[\"text\"] for item in results]"
      ],
      "metadata": {
        "id": "b64r9EuDG0Cc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelInference:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str = \"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/model.gguf\",\n",
        "        use_rag: bool = True,\n",
        "        context_length: int = 4096,\n",
        "        num_retrieved_docs: int = 3,\n",
        "    ):\n",
        "        self.model_path = model_path\n",
        "        self.use_rag = use_rag\n",
        "        self.num_retrieved_docs = num_retrieved_docs\n",
        "\n",
        "        # Initialize Llama model\n",
        "        self.llm = Llama(\n",
        "            model_path=model_path,\n",
        "            n_ctx=context_length,\n",
        "            n_batch=512,\n",
        "            n_gpu_layers=-1,  # Use all layers on GPU if available\n",
        "        )\n",
        "\n",
        "        # Initialize RAG system if needed\n",
        "        if use_rag:\n",
        "            self.rag = RAGSystem()\n",
        "            self.rag.build_index()\n",
        "\n",
        "    def retrieve_context(self, query: str) -> str:\n",
        "        \"\"\"Retrieve relevant context using RAG with token count limiting.\"\"\"\n",
        "        if not self.use_rag:\n",
        "            return \"\"\n",
        "\n",
        "        contexts = self.rag.retrieve(query, k=self.num_retrieved_docs)\n",
        "\n",
        "        # Calculate token budgets\n",
        "        system_prompt = \"You are an AI assistant that specializes in answering questions about AI research papers.\"\n",
        "        query_prompt = f\"Question: {query}\"\n",
        "        combined_prompt = system_prompt + query_prompt\n",
        "\n",
        "        # Fix: Use the more reliable approach with llama_cpp\n",
        "        # Reserve tokens for the system prompt, query, and generated response\n",
        "        try:\n",
        "            # Use the proper encoding with llama_cpp\n",
        "            reserved_tokens = (\n",
        "                len(self.llm.tokenize(bytes(combined_prompt, \"utf-8\"))) + 1024\n",
        "            )\n",
        "        except TypeError:\n",
        "            # Fallback method if bytes conversion doesn't work\n",
        "            reserved_tokens = len(combined_prompt.split()) * 2 + 1024  # Approximate\n",
        "\n",
        "        max_context_tokens = self.llm.n_ctx() - reserved_tokens\n",
        "\n",
        "        # Start with all contexts and trim as needed\n",
        "        selected_contexts = []\n",
        "        current_tokens = 0\n",
        "\n",
        "        for context in contexts:\n",
        "            try:\n",
        "                context_tokens = len(self.llm.tokenize(bytes(context, \"utf-8\")))\n",
        "            except TypeError:\n",
        "                # Fallback approximation\n",
        "                context_tokens = len(context.split()) * 2\n",
        "\n",
        "            if current_tokens + context_tokens <= max_context_tokens:\n",
        "                selected_contexts.append(context)\n",
        "                current_tokens += context_tokens\n",
        "            else:\n",
        "                # Try to add a truncated version if it's the first context\n",
        "                if len(selected_contexts) == 0:\n",
        "                    # Estimate truncation point (rough approximation)\n",
        "                    max_chars = int(max_context_tokens / context_tokens * len(context))\n",
        "                    truncated = context[:max_chars]\n",
        "                    selected_contexts.append(truncated)\n",
        "                break\n",
        "\n",
        "        return \"\\n\\n\".join(selected_contexts)\n",
        "\n",
        "    def format_prompt(self, query: str, context: Optional[str] = None) -> str:\n",
        "        \"\"\"Format the prompt for the model.\"\"\"\n",
        "        system_message = \"You are an AI assistant that specializes in answering questions about AI research papers. Provide comprehensive, accurate responses based on the information available to you.\"\n",
        "\n",
        "        if context:\n",
        "            prompt = f\"\"\"### System:\n",
        "{system_message}\n",
        "\n",
        "### Human:\n",
        "I have a question about an AI research paper.\n",
        "\n",
        "Here is some relevant context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "### Assistant:\n",
        "\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"### System:\n",
        "{system_message}\n",
        "\n",
        "### Human:\n",
        "Question about AI research: {query}\n",
        "\n",
        "### Assistant:\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def generate_answer(self, query: str) -> str:\n",
        "        \"\"\"Generate an answer for a query.\"\"\"\n",
        "        # Retrieve context if using RAG\n",
        "        context = self.retrieve_context(query) if self.use_rag else None\n",
        "\n",
        "        # Format prompt\n",
        "        prompt = self.format_prompt(query, context)\n",
        "\n",
        "        # Generate response\n",
        "        start_time = time.time()\n",
        "        response = self.llm(\n",
        "            prompt,\n",
        "            max_tokens=1024,\n",
        "            stop=[\"### Human:\", \"### System:\"],\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "        )\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Extract answer text\n",
        "        answer = response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "        # Log performance\n",
        "        print(f\"Generation time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "        return answer"
      ],
      "metadata": {
        "id": "bc070G4AOhdw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ModelQuantizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/fine_tuned_model/final\",\n",
        "        base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        output_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model\",\n",
        "    ):\n",
        "        self.model_path = model_path\n",
        "        self.base_model = base_model\n",
        "        self.output_dir = output_dir\n",
        "        self.quantized_model_path = os.path.join(output_dir, \"model.gguf\")\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def load_and_merge_model(self):\n",
        "        \"\"\"Load the LoRA model and merge with the base model.\"\"\"\n",
        "        print(\"Loading base model...\")\n",
        "\n",
        "        # Load base model\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.base_model,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "        )\n",
        "\n",
        "        # Load LoRA weights\n",
        "        print(\"Loading and merging LoRA weights...\")\n",
        "        model = PeftModel.from_pretrained(base_model, self.model_path)\n",
        "\n",
        "        # Merge LoRA weights with base model\n",
        "        model = model.merge_and_unload()\n",
        "\n",
        "        # Save merged model and tokenizer\n",
        "        merged_model_path = os.path.join(self.output_dir, \"merged\")\n",
        "        os.makedirs(merged_model_path, exist_ok=True)\n",
        "\n",
        "        print(f\"Saving merged model to {merged_model_path}...\")\n",
        "        model.save_pretrained(merged_model_path)\n",
        "\n",
        "        # Save tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.base_model)\n",
        "        tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "        print(\"Model and tokenizer saved successfully.\")\n",
        "        return merged_model_path\n",
        "\n",
        "    def convert_to_gguf(self, merged_model_path):\n",
        "        \"\"\"Convert the merged model to GGUF format with quantization.\"\"\"\n",
        "        print(\"Converting to GGUF format with quantization...\")\n",
        "\n",
        "        # Check for existing GGUF model\n",
        "        if os.path.exists(self.quantized_model_path):\n",
        "            print(f\"GGUF model already exists at {self.quantized_model_path}\")\n",
        "            user_input = input(\"Do you want to rebuild it? (y/n): \").lower()\n",
        "            if user_input != \"y\":\n",
        "                print(\"Using existing GGUF model.\")\n",
        "                return self.quantized_model_path\n",
        "\n",
        "        # Clone llama.cpp repository if needed\n",
        "        if not os.path.exists(\"llama.cpp\"):\n",
        "            try:\n",
        "                print(\"Cloning llama.cpp repository...\")\n",
        "                subprocess.run(\n",
        "                    [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\"],\n",
        "                    check=True,\n",
        "                )\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"Error cloning llama.cpp repository.\")\n",
        "                raise RuntimeError(\"Failed to clone llama.cpp repository\")\n",
        "\n",
        "        # Build llama.cpp with better error handling\n",
        "        try:\n",
        "            print(\"Building llama.cpp with CMake (this may take a few minutes)...\")\n",
        "            os.makedirs(\"llama.cpp/build\", exist_ok=True)\n",
        "\n",
        "            # Configure with CMake\n",
        "            subprocess.run(\n",
        "                [\"cmake\", \"-S\", \"llama.cpp\", \"-B\", \"llama.cpp/build\"], check=True\n",
        "            )\n",
        "\n",
        "            # Build with CMake\n",
        "            subprocess.run(\n",
        "                [\"cmake\", \"--build\", \"llama.cpp/build\", \"--parallel\"], check=True\n",
        "            )\n",
        "\n",
        "            print(\"llama.cpp built successfully with CMake\")\n",
        "\n",
        "            # Use convert_hf_to_gguf.py with verbose output to see what's happening\n",
        "            convert_script = \"llama.cpp/convert_hf_to_gguf.py\"\n",
        "\n",
        "            if not os.path.exists(convert_script):\n",
        "                print(f\"ERROR: {convert_script} not found!\")\n",
        "                print(\"Please verify your llama.cpp installation.\")\n",
        "                raise RuntimeError(f\"Conversion script not found: {convert_script}\")\n",
        "\n",
        "            print(f\"\\nRunning conversion script with enhanced debugging...\")\n",
        "\n",
        "            # Try conversion with detailed error output - using q8_0 instead of q4_0\n",
        "            try:\n",
        "                result = subprocess.run(\n",
        "                    [\n",
        "                        \"python3\",\n",
        "                        convert_script,\n",
        "                        merged_model_path,\n",
        "                        \"--outfile\",\n",
        "                        self.quantized_model_path,\n",
        "                        \"--outtype\",\n",
        "                        \"q8_0\",  # Changed from q4_0 to q8_0\n",
        "                        \"--verbose\",\n",
        "                    ],\n",
        "                    check=True,\n",
        "                    capture_output=True,\n",
        "                    text=True,\n",
        "                )\n",
        "                print(result.stdout)\n",
        "\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(\"\\n===== Conversion Error Details =====\")\n",
        "                print(f\"Exit code: {e.returncode}\")\n",
        "                print(f\"STDOUT: {e.stdout}\")\n",
        "                print(f\"STDERR: {e.stderr}\")\n",
        "                print(\"===================================\\n\")\n",
        "\n",
        "                print(\n",
        "                    \"Trying alternate conversion approach with arch-specific parameters...\"\n",
        "                )\n",
        "                try:\n",
        "                    # Try with explicit model architecture parameters - using q8_0\n",
        "                    result = subprocess.run(\n",
        "                        [\n",
        "                            \"python3\",\n",
        "                            convert_script,\n",
        "                            merged_model_path,\n",
        "                            \"--outfile\",\n",
        "                            self.quantized_model_path,\n",
        "                            \"--outtype\",\n",
        "                            \"q8_0\",  # Changed from q4_0 to q8_0\n",
        "                            \"--model-name\",\n",
        "                            \"Qwen\",  # Added model name hint\n",
        "                        ],\n",
        "                        check=True,\n",
        "                        capture_output=True,\n",
        "                        text=True,\n",
        "                    )\n",
        "                    print(result.stdout)\n",
        "\n",
        "                except subprocess.CalledProcessError as e2:\n",
        "                    print(f\"Alternate approach also failed\")\n",
        "                    print(f\"STDOUT: {e2.stdout}\")\n",
        "                    print(f\"STDERR: {e2.stderr}\")\n",
        "                    raise RuntimeError(\"All conversion methods failed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during build or conversion process: {e}\")\n",
        "            raise RuntimeError(\"Failed to convert model to GGUF format\")\n",
        "\n",
        "        print(\n",
        "            f\"Model successfully converted to GGUF format: {self.quantized_model_path}\"\n",
        "        )\n",
        "\n",
        "        # Copy tokenizer files to output directory\n",
        "        tokenizer_files = [\"tokenizer_config.json\", \"tokenizer.json\"]\n",
        "        for file in tokenizer_files:\n",
        "            src_path = os.path.join(merged_model_path, file)\n",
        "            if os.path.exists(src_path):\n",
        "                dst_path = os.path.join(self.output_dir, file)\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "\n",
        "        return self.quantized_model_path\n",
        "\n",
        "    def quantize(self):\n",
        "        \"\"\"Perform the complete quantization process.\"\"\"\n",
        "        merged_model_path = self.load_and_merge_model()\n",
        "        gguf_path = self.convert_to_gguf(merged_model_path)\n",
        "        return gguf_path\n"
      ],
      "metadata": {
        "id": "u48XEcKdG4z7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZlzmyiC0fGrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ef4404-4272-4f18-8ca0-59c2ed4e1a67"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def main():\n",
        "    # Generate synthetic dataset with default directories\n",
        "    print(\"Generating synthetic dataset...\")\n",
        "    create_synthetic_data(\n",
        "        documents_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/dataset/q3_dataset\",\n",
        "        output_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\"\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model using default parameters\n",
        "    print(\"Fine-tuning Qwen/Qwen2.5-3B-Instruct...\")\n",
        "    fine_tuner = QAFineTuner(\n",
        "        \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        \"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\",\n",
        "        \"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/fine_tuned_model\"\n",
        "    )\n",
        "    fine_tuner.load_data()\n",
        "    fine_tuner.prepare_model()\n",
        "    fine_tuner.prepare_datasets()\n",
        "    fine_tuner.train()\n",
        "\n",
        "    # Quantize the model to GGUF format with default settings\n",
        "    print(\"Quantizing the model...\")\n",
        "    quantizer = ModelQuantizer(\n",
        "        model_path=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/fine_tuned_model/final\",\n",
        "        base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "        output_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model\"\n",
        "    )\n",
        "    quantizer.quantize()\n",
        "\n",
        "    # Build the RAG index with default directory\n",
        "    print(\"Building RAG index...\")\n",
        "    rag = RAGSystem(data_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\")\n",
        "    rag.build_index(force_rebuild=True)\n",
        "\n",
        "    # Evaluate the model using default settings (using all available samples)\n",
        "    print(\"Evaluating the model...\")\n",
        "    evaluator = Evaluator(\n",
        "        model_path=os.path.join(\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model\", \"model.gguf\"),\n",
        "        data_dir=\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/data\"\n",
        "    )\n",
        "    evaluator.evaluate(sample_size=None)\n",
        "\n",
        "    # Run inference using default parameters and a sample query\n",
        "    print(\"Running inference...\")\n",
        "    inference = ModelInference(\n",
        "        model_path=os.path.join(\"/content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model\", \"model.gguf\"),\n",
        "        use_rag=True\n",
        "    )\n",
        "    default_query = \"What is the latest research in AI?\"\n",
        "    answer = inference.generate_answer(default_query)\n",
        "    print(f\"\\nQuery: {default_query}\\n\")\n",
        "    print(f\"Answer:\\n{answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "E_dg7i4IG6as",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "678872e552c34725a9a907c9624869d8",
            "3efa823b01c347d3b6d960ce443de94d",
            "b556f1fdba364632934ff1e3ab578547",
            "cb2cf43a95cb42e9a98f68578d4e63b7",
            "21057437fcd24899baacc80e56cb2fc0",
            "e3b14c663e0b4f1c8634a9fcbd9ae85d",
            "7c2c8682896b4932b5cc5397354e190e",
            "a6ed243f128c45d1ae88e489a42bc8ec",
            "0040bece314d4eb59f5b330c65906a82",
            "ca73c47260fb457980d45f48b96fac80",
            "b1e2e1251f2b446bb3b732cd63b3df4f",
            "0df93d1dfa7c45eba3e1d75fd3a3c8ca",
            "ef31d749be7743518d6bfaaef91058a7",
            "8c47876410254de5ab13c05e9df5c472",
            "599fa7eea4a94625ab94afddb170d66f",
            "656aaf03f81e47eb9d0ac9f698e1d170",
            "c8340881399d4f959d34ff02a5ef3bdc",
            "cece402e7eb1478085b54e805e32232e",
            "e3570eaa4fcf40d7919bf13b165541cf",
            "8249a98efa3d4a07af5accc968e65d0d",
            "44df9d65b07b43458c5ec7b9918750de",
            "f80260627ad9403eb83050a73ddb4b46",
            "8d2d29cd7c49490883e4ef03c20ba6fc",
            "b609b0df17b14d929736cce942e3e253",
            "b1173f22eed04627b5c48dab7e99a306",
            "86b4f8a2653d4f2ebf1a1f53141a9006",
            "a1e9d5bae0364910a356f43c88f5fb43",
            "a37ece513d634c658add2608b545ab23",
            "e9aa71453fcf4f7281a8f22a45e739e0",
            "ffee3ec15bd84dd68ea80fc8f9d710d9",
            "04d9f68ad4434febafed493cfc9dc87e",
            "dd2fb1f16254414aa9b28866d41f90b1",
            "e70e9860ee56466188e2fa6dc572a1a6",
            "1847e804ef604423b752065435028b63",
            "ea9c5bbdc5c24618a4a3129dd98413a6",
            "702085fb7af143beba4740be2791a865",
            "7eb7465711254ddb966c7fc3791943c0",
            "5ec8b608d664457a9648a48c1dc29e7a",
            "85e02a8e9e394ac4805511bac25d8d1f",
            "ee36070d15614388ba35e6fe6bd17cde",
            "97868945923440c0b2813ab5bbb90f49",
            "a968b0f9f1f54137aa579ddfca212da9",
            "0dd6f7a38f67420c93d18178cc4a4449",
            "3edf7e821bc04f11a4e50bd7e21c9409",
            "57f4e0cd651041bd9a202bc55ffe6c13",
            "a922fb8c90774f4eb65fcc3310751e81",
            "07ebf603efcd4cd6b82fc3418d140f9f",
            "da47bd67a80941bc9a1acb9f2dea12cd",
            "29e12f867d264fc28e1353114debd3b4",
            "5757162d9d2e4ec09a39946d10994dd5",
            "adb78ee77ccf4b96afaecdf1c6892ee4",
            "3880bf427ac3401999c2fc64d75fe319",
            "5bc202bcfa4d4c0aa135eebb453e35c4",
            "c253c8fe7b424976981b1ac74b44ac3c",
            "589d53df87a24c8ca293e896f646d7ad",
            "70377359d69e481da01170212bf021c3",
            "2b2ecf41eb3a49d8995f0929c2e2f718",
            "66c1eba5238844eaa144f9979348793b",
            "dfeb0458e7214a48b045cd357ff22453",
            "1853883e6b3540679ec39e5e94f1324a",
            "efaedeed2a6e492e8ac64aa0fed89f9d",
            "feb2971e003b4398b4e1a63fe25a5914",
            "39b377489e7649cca582ea7533d95678",
            "ef439ef9f0d4407b9fead5036ff65f70",
            "dd6bafe15ed14e83adb0584fd6d31cd6",
            "d3f7fcffff584f37b50c3c34e6e14d3d",
            "ef6f332aa7584fba869d3d125de8579c",
            "851839054ed14d0e918a649ea32453f3",
            "5516ae75adcf4e3b8b979e544aa1f33f",
            "162281bfffea48f08e4680be2bcf2709",
            "872a257fbcee49398cd3b9c032ccaf8a",
            "574d0296086049698c0a3d61b65cbe72",
            "0c1b90c0af8e448596aad503195cc513",
            "5bc2f64b1ea24b9daf33834088330eaf",
            "06a2adc7755c4a37a74d10d797251401",
            "9da503f55405427d87d181f7ecbd2e81",
            "6e550baf83b845e0aaa6b853543aa4e0",
            "7e732b7b8ed744cfa214824ca2807fc1",
            "85453a7eb8f549f28dcc94ca8729ee8c",
            "5bb141b572004083862de5cbda456093",
            "ef3ebe9e4d074e98850c2f6ccdeb1930",
            "5cf6366ce8634280bbf226133d5ac842",
            "6c094ba2a2c04d7f89f99c7969455370",
            "77e8be90908248da9be66a44a032b879",
            "19b9a70e82334c48aff2db32f5112bbb",
            "0291855d486e4286b4e541253739140c",
            "527bad67f3ab427189dafe99b8d2fea0",
            "a180c8764d4b467aa304b0da545c3019",
            "968f660799784483acf00e9e6a9a6334",
            "4b661bad389541e38f9cd8fc78c415fa",
            "94432ece8c3b48bc8984a609b5f14fa0",
            "3644107f63d04938b00f0590e60346bb",
            "ea29502dae19445c858b6df182916ad2",
            "540feca764824764bcf80555ee2b5258",
            "28ecef50860e45e89e448e39026aefe9",
            "0e377b3927c445779a5344d4a983ab43",
            "5b67a7b482404aa69e083bc5e707da28",
            "8a8616989bb14a4eb24c6554f9529043",
            "7b6a399c510d409489c970bb5884b1d8",
            "836074441c104f5c8c808e2c05232e2b",
            "a534ac40f7ee4aaabd40e7a285160a32",
            "aade4cdbcf8242b3a8c06df51d70b506",
            "5067606ef6154958a4ef02f408c697a9",
            "d588b13a41824b2797d2a923367cdd08",
            "cdf28cb8d945449395694444237c8654",
            "70781fa1342343ef8712929d052876c4",
            "c15623d67b0c4a9182b0912692b39ed5",
            "2e2f4c4701b24f04a3a0de89133f9322",
            "51378e3b93c14f4aa54debe5cabcb14b",
            "6975f6a60fd649839885638d64b049de",
            "5534a5f8b1ac45b39edbac5c6a164521",
            "1e78ca0d8bb4470d8225796a7c1d7c59",
            "db1fc4e220734ecfbd86d6d7edd0ecdc",
            "495832ceb7bb4759a9b40286200b4bd9",
            "826e765975454e209c89ae5e395a98e3",
            "c60a06edc316476893948b4a4277bf8e",
            "749ad764db4c404e9aedecdf225daf8b",
            "30d081531ac4408295275595b4ec0530",
            "dd57d86bf8234461856e7d26bbcc4147",
            "0a3ba5dbb8794d5aaad0bcec5fa62c66",
            "36ba926d1e614ced9cdb55c05cadf5de"
          ]
        },
        "outputId": "c66082c9-03f4-48a1-c385-98279297654b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic dataset...\n",
            "Loaded 5 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "678872e552c34725a9a907c9624869d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing chunk 1/9\n",
            "\n",
            "Model response for chunk 1:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    202502 Open-Source Week\n",
            "\n",
            "We're a tiny team @deepseek-ai pushing our limits in AGI exploration. Starting this week , Feb 24, 2025 we'll open-source 5 repos – one daily drop – not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency. These are humble building blocks of our online service: documented, deployed and ba\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 1\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    202502 Open-Source Week\n",
            "\n",
            "We're a tiny team @deepseek-ai pushing our limits in AGI exploration. Starting this week , Feb 24, 2025 we'll open-source 5 repos – one daily drop – not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency. These are humble building blocks of our online service: documented, deployed and battle-tested in\n",
            "...\n",
            " V3/R1 online services:\n",
            "⚡ 73.7k/14.8k input/output tokens per second per H800 node\n",
            "🚀 Cost profit margin 545%\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 2/9\n",
            "\n",
            "Model response for chunk 2:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    Design Notes\n",
            "\n",
            "Design and implementation\n",
            "\n",
            "The 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE). Metadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and cli\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 2\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    Design Notes\n",
            "\n",
            "Design and implementation\n",
            "\n",
            "The 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE). Metadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and clients. Multiple\n",
            "...\n",
            " path string. Directory entry keys are composed of a \"DENT\" prefix, the parent inode ID, and the entry name.\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 3/9\n",
            "\n",
            "Model response for chunk 3:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    Design Notes\n",
            "\n",
            "Design and implementation\n",
            "\n",
            "The 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE). Metadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and cli\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 3\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    Design Notes\n",
            "\n",
            "Design and implementation\n",
            "\n",
            "The 3FS system has four components: cluster manager, metadata service, storage service and client. All components are connected in an RDMA network (InfiniBand or RoCE). Metadata and storage services send heartbeats to cluster manager. Cluster manager handles membership changes and distributes cluster configuration to other services and clients. Multiple\n",
            "...\n",
            "ted version of the chunk into memory, applies the update, and stores the updated chunk as a pending version.\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 4/9\n",
            "\n",
            "Model response for chunk 4:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    Without it, applications have to traverse each directory and remove files one by one. -   *Symbolic and hard links* Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files. -   *Familiar interface* The file interface is well known and used everywhere. There is no need to learn\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 4\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    Without it, applications have to traverse each directory and remove files one by one. -   *Symbolic and hard links* Our applications utilize symbolic and hard links to create lightweight snapshots of dynamically updated datasets, where new data is appended as individual files. -   *Familiar interface* The file interface is well known and used everywhere. There is no need to learn a new storage\n",
            "...\n",
            "data of all chunks on the successor target are received, it collects the chunk metadata on its local target.\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 5/9\n",
            "\n",
            "Model response for chunk 5:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    For write transactions, FoundationDB tracks the read/write key sets to form conflict detection sets. When concurrent transaction conflicts are detected, the meta service automatically retries the transaction. This design enables multiple meta services to process requests in parallel while maintaining file system metadata consistency. Dynamic file attributes\n",
            "\n",
            "On most local file sy\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 5\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    For write transactions, FoundationDB tracks the read/write key sets to form conflict detection sets. When concurrent transaction conflicts are detected, the meta service automatically retries the transaction. This design enables multiple meta services to process requests in parallel while maintaining file system metadata consistency. Dynamic file attributes\n",
            "\n",
            "On most local file systems, deletin\n",
            "...\n",
            "e atomically updated in RocksDB. [^1]: https://elixir.bootlin.com/linux/v5.4.284/source/fs/fuse/file.c#L1573\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 6/9\n",
            "\n",
            "Model response for chunk 6:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    <source name=\"https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c\"/>\n",
            "author - Ataka jeong\n",
            "\n",
            "1. Introduction\n",
            "How could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 6\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    <source name=\"https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c\"/>\n",
            "author - Ataka jeong\n",
            "\n",
            "1. Introduction\n",
            "How could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the model may see\n",
            "...\n",
            "e them as a chunk and continuously copy them together on other devices to reduce communication between GPUs.\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 7/9\n",
            "\n",
            "Model response for chunk 7:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    <source name=\"https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c\"/>\n",
            "author - Ataka jeong\n",
            "\n",
            "1. Introduction\n",
            "How could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 7\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    <source name=\"https://medium.com/@jjjy213/deepseek-v3-explained-fdac83ba280c\"/>\n",
            "author - Ataka jeong\n",
            "\n",
            "1. Introduction\n",
            "How could the DeepSeek-V3 model achieve incredible performance and economical training as an open source model? In this paper review, we will explore the various features that were invented and applied to build the DeepSeek-V3 model. The way the paper presents the model may see\n",
            "...\n",
            " a good AI model does not have to be undermined by the censorship and suppression of the Chinese government.\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 8/9\n",
            "\n",
            "Model response for chunk 8:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    DualPipe\n",
            "DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data. Pipeline Bubbles and Memory Usage Comparis\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 8\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    DualPipe\n",
            "DualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data. Pipeline Bubbles and Memory Usage Comparison\n",
            "\n",
            "| Method  \n",
            "...\n",
            "he bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Processing chunk 9/9\n",
            "\n",
            "Model response for chunk 9:\n",
            "mat for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "Given the text below from an AI research paper, generate 3 detailed question-answer pairs.\n",
            "            \n",
            "    TEXT:\n",
            "    <source name=\"https://medium.com/@visithkumarapperuma/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07\">\n",
            "\n",
            "author - Visith Kumarapperuma\n",
            "\n",
            "Deepseek V3: A Game-Changer in A.I. Here’s Why It Matters\n",
            "\n",
            "Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal p\n",
            "...\n",
            "bstantive, specific questions about key concepts in the text\n",
            "    - Write comprehensive answers using information directly from the text\n",
            "    - DO NOT generate generic or placeholder questions\n",
            "    - DO NOT use phrases like \"write a question here\" or \"comprehensive answer here\"\n",
            "    - Use this exact format for each pair:\n",
            "    \n",
            "    Q1: What is [specific concept from text]?\n",
            "    A1: [Detailed answer explaining the concept based on the text]\n",
            "    \n",
            "    Here are 3 question-answer pairs about this text:\n",
            "    \n",
            "Extracted 0 QA pairs from chunk 9\n",
            "Total extracted: 0 valid pairs\n",
            "DEBUG - Model response excerpt:\n",
            "I need exactly 3 question-answer pairs about this AI research text. \n",
            "                    \n",
            "    TEXT:\n",
            "    <source name=\"https://medium.com/@visithkumarapperuma/deepseek-v3-a-game-changer-in-a-i-heres-why-it-matters-75591957ca07\">\n",
            "\n",
            "author - Visith Kumarapperuma\n",
            "\n",
            "Deepseek V3: A Game-Changer in A.I. Here’s Why It Matters\n",
            "\n",
            "Currently, the AI models from the Chinese startup Deepseek are causing quite a stir in the AI space. Their latest reasoning model, Deepseek r1, shows better or equal performance to \n",
            "...\n",
            "K GPU hours\n",
            "Post‑training: 5K GPU hours\n",
            "Total: ~2.788 M GPU hours\n",
            "Cost (at $2 per GPU hour): ~$5.576 million\n",
            "    \n",
            "    FORMAT YOUR RESPONSE LIKE THIS - with real content, not placeholders:\n",
            "    Q1: [Real specific question about the content]\n",
            "    A1: [Real detailed answer from the content]\n",
            "    \n",
            "    Q2: [Real specific question about the content]\n",
            "    A2: [Real detailed answer from the content]\n",
            "    \n",
            "    Q3: [Real specific question about the content] \n",
            "    A3: [Real detailed answer from the content]\n",
            "    \n",
            "Fallback attempt extracted 0 QA pairs\n",
            "Trying with a different approach for at least some data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0df93d1dfa7c45eba3e1d75fd3a3c8ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d2d29cd7c49490883e4ef03c20ba6fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1847e804ef604423b752065435028b63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 3 training, 1 validation, and 1 test examples.\n",
            "Fine-tuning Qwen/Qwen2.5-3B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57f4e0cd651041bd9a202bc55ffe6c13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70377359d69e481da01170212bf021c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3 training examples and 1 validation examples.\n",
            "Using bitsandbytes version: 0.45.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef6f332aa7584fba869d3d125de8579c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded model with 4-bit quantization and LoRA adapters\n",
            "trainable params: 59,867,136 || all params: 3,145,805,824 || trainable%: 1.9031\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e732b7b8ed744cfa214824ca2807fc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "968f660799784483acf00e9e6a9a6334"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized datasets: Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 3\n",
            "}), Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 1\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranuga-d\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250309_161222-q0z5wz4r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa/runs/q0z5wz4r' target=\"_blank\">qwen-2.5-3b-qlora</a></strong> to <a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa' target=\"_blank\">https://wandb.ai/ranuga-d/qwen-ai-research-qa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa/runs/q0z5wz4r' target=\"_blank\">https://wandb.ai/ranuga-d/qwen-ai-research-qa/runs/q0z5wz4r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:15, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n",
            "Quantizing the model...\n",
            "Loading base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "836074441c104f5c8c808e2c05232e2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and merging LoRA weights...\n",
            "Saving merged model to /content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/merged...\n",
            "Model and tokenizer saved successfully.\n",
            "Converting to GGUF format with quantization...\n",
            "GGUF model already exists at /content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/model.gguf\n",
            "Do you want to rebuild it? (y/n): Y\n",
            "Building llama.cpp with CMake (this may take a few minutes)...\n",
            "llama.cpp built successfully with CMake\n",
            "\n",
            "Running conversion script with enhanced debugging...\n",
            "\n",
            "Model successfully converted to GGUF format: /content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/model.gguf\n",
            "Building RAG index...\n",
            "Building vector index...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5534a5f8b1ac45b39edbac5c6a164521"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 unique contexts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 434 tensors from /content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/model.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
            "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  181 tensors\n",
            "llama_model_loader: - type q8_0:  253 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 3.05 GiB (8.50 BPW) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built and saved index with 5 documents.\n",
            "Evaluating the model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 2048\n",
            "print_info: n_layer          = 36\n",
            "print_info: n_head           = 16\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.09 B\n",
            "print_info: general.name     = Qwen2.5 3B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 148848 'ÄĬ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  3127.59 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
            "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
            "llama_init_from_model: graph nodes  = 1266\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '2048', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 3B Instruct', 'qwen2.block_count': '36', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing index with 5 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 434 tensors from /content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/model.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
            "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  181 tensors\n",
            "llama_model_loader: - type q8_0:  253 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 3.05 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 2048\n",
            "print_info: n_layer          = 36\n",
            "print_info: n_head           = 16\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.09 B\n",
            "print_info: general.name     = Qwen2.5 3B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 148848 'ÄĬ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  3127.59 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
            "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
            "llama_init_from_model: graph nodes  = 1266\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '2048', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 3B Instruct', 'qwen2.block_count': '36', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on 1 test examples...\n",
            "Processing example 1/1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   81220.61 ms\n",
            "llama_perf_context_print: prompt eval time =   81219.93 ms /  3267 tokens (   24.86 ms per token,    40.22 tokens per second)\n",
            "llama_perf_context_print:        eval time =    9795.86 ms /   101 runs   (   96.99 ms per token,    10.31 tokens per second)\n",
            "llama_perf_context_print:       total time =   91176.32 ms /  3368 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation time: 91.19 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =    1632.98 ms\n",
            "llama_perf_context_print: prompt eval time =    1632.79 ms /    54 tokens (   30.24 ms per token,    33.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =   43740.16 ms /   553 runs   (   79.10 ms per token,    12.64 tokens per second)\n",
            "llama_perf_context_print:       total time =   46500.21 ms /   607 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation time: 46.51 seconds\n",
            "\n",
            "Evaluation Results:\n",
            "\n",
            "With RAG:\n",
            "bleu: 0.1621\n",
            "rouge1: 0.4615\n",
            "rouge2: 0.2957\n",
            "rougeL: 0.3932\n",
            "\n",
            "Without RAG:\n",
            "bleu: 0.0012\n",
            "rouge1: 0.0720\n",
            "rouge2: 0.0085\n",
            "rougeL: 0.0551\n",
            "\n",
            "Results saved to metrics.json\n",
            "Running inference...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 434 tensors from /content/drive/MyDrive/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA/quantized_model/model.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
            "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
            "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
            "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  181 tensors\n",
            "llama_model_loader: - type q8_0:  253 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q8_0\n",
            "print_info: file size   = 3.05 GiB (8.50 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 2048\n",
            "print_info: n_layer          = 36\n",
            "print_info: n_head           = 16\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 8\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.09 B\n",
            "print_info: general.name     = Qwen2.5 3B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 148848 'ÄĬ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: layer  33 assigned to device CPU\n",
            "load_tensors: layer  34 assigned to device CPU\n",
            "load_tensors: layer  35 assigned to device CPU\n",
            "load_tensors: layer  36 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q8_0) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  3127.59 MiB\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 4096\n",
            "llama_init_from_model: n_ctx_per_seq = 4096\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 1000000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
            "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
            "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
            "llama_init_from_model: graph nodes  = 1266\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '2048', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 3B Instruct', 'qwen2.block_count': '36', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded existing index with 5 documents.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   17334.24 ms\n",
            "llama_perf_context_print: prompt eval time =   17333.61 ms /   893 tokens (   19.41 ms per token,    51.52 tokens per second)\n",
            "llama_perf_context_print:        eval time =   53889.83 ms /   631 runs   (   85.40 ms per token,    11.71 tokens per second)\n",
            "llama_perf_context_print:       total time =   72552.72 ms /  1524 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation time: 72.56 seconds\n",
            "\n",
            "Query: What is the latest research in AI?\n",
            "\n",
            "Answer:\n",
            "Based on the context provided, the latest research in AI by the team at deepseek-ai can be summarized as follows:\n",
            "\n",
            "1. **FlashMLA**: An optimized kernel for efficient Multi-Head Attention (MLA) decoding specifically for Hopper GPUs. It supports variable-length sequences and has been battle-tested in production. Key features include BF16 support, paged key-value (KV) cache, and high performance (3000 GB/s memory-bound, 580 TFLOPS compute-bound on H800).\n",
            "\n",
            "2. **DeepEP**: The first open-source EP communication library for Massively Parallel (MoE) model training and inference. It includes efficient all-to-all communication support for intranode and internode scenarios, with kernels optimized for training and inference prefilling, as well as low-latency kernels for inference decoding. Native FP8 dispatch support is also provided.\n",
            "\n",
            "3. **DeepGEMM**: An FP8 General Matrix Multiply (GEMM) library that supports both dense and MoE GEMMs, essential for V3/R1 training and inference. It achieves up to 1350+ FP8 TFLOPS on Hopper GPUs and is fully Just-In-Time compiled. The core logic is relatively small (about 300 lines) but outperforms expert-tuned kernels across various matrix sizes.\n",
            "\n",
            "4. **3FS (Fire-Flyer File System)**: A parallel file system designed for efficient data access in deep learning pipelines. It utilizes modern SSDs and RDMA networks to achieve high aggregate read throughput of 6.6 TiB/s in a 180-node cluster and 3.66 TiB/min throughput on the GraySort benchmark in a 25-node cluster. The system is disaggregated and designed with strong consistency semantics, supporting training data preprocessing, dataset loading, checkpoint saving/reloading, and KVCache lookups for inference.\n",
            "\n",
            "5. **Optimized Parallelism Strategies**:\n",
            "   - **DualPipe**: A bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training.\n",
            "   - **EPLB (Expert-Parallel Load Balancer)**: An expert-parallel load balancer for V3/R1.\n",
            "   - **Analyzing Computation-Communication Overlap**: Tools and analysis to understand and optimize overlap in V3/R1.\n",
            "\n",
            "6. **DeepSeek-V3/R1 Inference System Overview**: An optimized system for achieving high throughput and latency via cross-node EP-powered batch scaling, computation-communication overlap, and load balancing. The system has been deployed in production, delivering 73.7k/14.8k input/output tokens per second per H800 node.\n",
            "\n",
            "These pieces of research collectively represent the latest advancements in various aspects of AI infrastructure, from optimization of specific components like MLP decoding, communication libraries for parallel models, high-performance GEMM libraries, efficient data access systems, and optimized inference systems. The focus is on improving both the performance and efficiency of deep learning systems, which is crucial for advancing AI research and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/content.zip /content -x \"/content/drive/*\" \"/content/wandb/*\""
      ],
      "metadata": {
        "id": "uefbwdZwG7FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6009943a-d539-4c7f-b70a-489a3fa6516f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/ (stored 0%)\n",
            "updating: content/.config/ (stored 0%)\n",
            "updating: content/.config/default_configs.db (deflated 98%)\n",
            "updating: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "updating: content/.config/active_config (stored 0%)\n",
            "updating: content/.config/.last_update_check.json (deflated 22%)\n",
            "updating: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "updating: content/.config/logs/ (stored 0%)\n",
            "updating: content/.config/logs/2025.03.06/ (stored 0%)\n",
            "updating: content/.config/logs/2025.03.06/14.28.23.979271.log (deflated 92%)\n",
            "updating: content/.config/logs/2025.03.06/14.28.44.811499.log (deflated 58%)\n",
            "updating: content/.config/logs/2025.03.06/14.29.03.284363.log (deflated 56%)\n",
            "updating: content/.config/logs/2025.03.06/14.28.53.350004.log (deflated 86%)\n",
            "updating: content/.config/logs/2025.03.06/14.29.02.658299.log (deflated 57%)\n",
            "updating: content/.config/logs/2025.03.06/14.28.54.467455.log (deflated 57%)\n",
            "updating: content/.config/gce (stored 0%)\n",
            "updating: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "updating: content/.config/configurations/ (stored 0%)\n",
            "updating: content/.config/configurations/config_default (deflated 15%)\n",
            "updating: content/.config/config_sentinel (stored 0%)\n",
            "updating: content/llama.cpp/ (stored 0%)\n",
            "updating: content/llama.cpp/AUTHORS (deflated 61%)\n",
            "updating: content/llama.cpp/prompts/ (stored 0%)\n",
            "updating: content/llama.cpp/prompts/chat.txt (deflated 53%)\n",
            "updating: content/llama.cpp/prompts/dan.txt (deflated 53%)\n",
            "updating: content/llama.cpp/prompts/chat-with-vicuna-v0.txt (deflated 44%)\n",
            "updating: content/llama.cpp/prompts/alpaca.txt (deflated 18%)\n",
            "updating: content/llama.cpp/prompts/dan-modified.txt (deflated 53%)\n",
            "updating: content/llama.cpp/prompts/assistant.txt (deflated 60%)\n",
            "updating: content/llama.cpp/prompts/mnemonics.txt (deflated 52%)\n",
            "updating: content/llama.cpp/prompts/LLM-questions.txt (deflated 76%)\n",
            "updating: content/llama.cpp/prompts/chat-with-qwen.txt (stored 0%)\n",
            "updating: content/llama.cpp/prompts/parallel-questions.txt (deflated 50%)\n",
            "updating: content/llama.cpp/prompts/reason-act.txt (deflated 53%)\n",
            "updating: content/llama.cpp/prompts/chat-with-bob.txt (deflated 38%)\n",
            "updating: content/llama.cpp/prompts/chat-with-baichuan.txt (deflated 3%)\n",
            "updating: content/llama.cpp/prompts/chat-with-vicuna-v1.txt (deflated 43%)\n",
            "updating: content/llama.cpp/grammars/ (stored 0%)\n",
            "updating: content/llama.cpp/grammars/japanese.gbnf (deflated 33%)\n",
            "updating: content/llama.cpp/grammars/README.md (deflated 67%)\n",
            "updating: content/llama.cpp/grammars/json_arr.gbnf (deflated 46%)\n",
            "updating: content/llama.cpp/grammars/arithmetic.gbnf (deflated 33%)\n",
            "updating: content/llama.cpp/grammars/json.gbnf (deflated 44%)\n",
            "updating: content/llama.cpp/grammars/chess.gbnf (deflated 44%)\n",
            "updating: content/llama.cpp/grammars/c.gbnf (deflated 64%)\n",
            "updating: content/llama.cpp/grammars/list.gbnf (deflated 8%)\n",
            "updating: content/llama.cpp/grammars/english.gbnf (deflated 31%)\n",
            "updating: content/llama.cpp/flake.nix (deflated 63%)\n",
            "updating: content/llama.cpp/gguf-py/ (stored 0%)\n",
            "updating: content/llama.cpp/gguf-py/README.md (deflated 60%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/ (stored 0%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/utility.py (deflated 70%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/gguf_reader.py (deflated 72%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/constants.py (deflated 86%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/lazy.py (deflated 69%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/metadata.py (deflated 80%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/vocab.py (deflated 76%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/gguf_writer.py (deflated 79%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/py.typed (stored 0%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/gguf.py (deflated 42%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/quants.py (deflated 75%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/ (stored 0%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/gguf_reader.cpython-311.pyc (deflated 54%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/constants.cpython-311.pyc (deflated 73%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/vocab.cpython-311.pyc (deflated 58%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/quants.cpython-311.pyc (deflated 69%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/gguf_writer.cpython-311.pyc (deflated 72%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/__init__.cpython-311.pyc (deflated 38%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/lazy.cpython-311.pyc (deflated 53%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/tensor_mapping.cpython-311.pyc (deflated 71%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/metadata.cpython-311.pyc (deflated 61%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__pycache__/utility.cpython-311.pyc (deflated 49%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/scripts/ (stored 0%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/scripts/gguf_convert_endian.py (deflated 71%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/scripts/gguf_hash.py (deflated 64%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/scripts/gguf_dump.py (deflated 75%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/scripts/gguf_set_metadata.py (deflated 60%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/scripts/gguf_new_metadata.py (deflated 71%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/scripts/__init__.py (deflated 60%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/__init__.py (deflated 57%)\n",
            "updating: content/llama.cpp/gguf-py/gguf/tensor_mapping.py (deflated 87%)\n",
            "updating: content/llama.cpp/gguf-py/pyproject.toml (deflated 50%)\n",
            "updating: content/llama.cpp/gguf-py/examples/ (stored 0%)\n",
            "updating: content/llama.cpp/gguf-py/examples/writer.py (deflated 57%)\n",
            "updating: content/llama.cpp/gguf-py/examples/reader.py (deflated 56%)\n",
            "updating: content/llama.cpp/gguf-py/LICENSE (deflated 41%)\n",
            "updating: content/llama.cpp/gguf-py/tests/ (stored 0%)\n",
            "updating: content/llama.cpp/gguf-py/tests/test_metadata.py (deflated 78%)\n",
            "updating: content/llama.cpp/gguf-py/tests/test_quants.py (deflated 73%)\n",
            "updating: content/llama.cpp/gguf-py/tests/__init__.py (stored 0%)\n",
            "updating: content/llama.cpp/.git/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/branches/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/refs/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/refs/tags/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/refs/heads/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/refs/heads/master (stored 0%)\n",
            "updating: content/llama.cpp/.git/refs/remotes/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/refs/remotes/origin/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "updating: content/llama.cpp/.git/config (deflated 34%)\n",
            "updating: content/llama.cpp/.git/index (deflated 62%)\n",
            "updating: content/llama.cpp/.git/logs/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/logs/refs/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/logs/refs/heads/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/logs/refs/heads/master (deflated 30%)\n",
            "updating: content/llama.cpp/.git/logs/refs/remotes/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/logs/refs/remotes/origin/HEAD (deflated 30%)\n",
            "updating: content/llama.cpp/.git/logs/HEAD (deflated 30%)\n",
            "updating: content/llama.cpp/.git/description (deflated 14%)\n",
            "updating: content/llama.cpp/.git/hooks/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "updating: content/llama.cpp/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "updating: content/llama.cpp/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "updating: content/llama.cpp/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "updating: content/llama.cpp/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "updating: content/llama.cpp/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "updating: content/llama.cpp/.git/hooks/update.sample (deflated 68%)\n",
            "updating: content/llama.cpp/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "updating: content/llama.cpp/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "updating: content/llama.cpp/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "updating: content/llama.cpp/.git/hooks/post-update.sample (deflated 27%)\n",
            "updating: content/llama.cpp/.git/hooks/pre-push.sample (deflated 49%)\n",
            "updating: content/llama.cpp/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "updating: content/llama.cpp/.git/objects/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/objects/pack/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/objects/pack/pack-36dfcd86d38589f86867b88abf03c5de12335373.idx (deflated 1%)\n",
            "updating: content/llama.cpp/.git/objects/pack/pack-36dfcd86d38589f86867b88abf03c5de12335373.pack (deflated 1%)\n",
            "updating: content/llama.cpp/.git/objects/info/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/info/ (stored 0%)\n",
            "updating: content/llama.cpp/.git/info/exclude (deflated 28%)\n",
            "updating: content/llama.cpp/.git/HEAD (stored 0%)\n",
            "updating: content/llama.cpp/.git/packed-refs (deflated 54%)\n",
            "updating: content/llama.cpp/include/ (stored 0%)\n",
            "updating: content/llama.cpp/include/llama-cpp.h (deflated 68%)\n",
            "updating: content/llama.cpp/include/llama.h (deflated 78%)\n",
            "updating: content/llama.cpp/convert_hf_to_gguf.py (deflated 82%)\n",
            "updating: content/llama.cpp/models/ (stored 0%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-qwen2.gguf.out (deflated 67%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-mpt.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-command-r.gguf (deflated 67%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-qwen2.gguf (deflated 71%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-falcon.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/templates/ (stored 0%)\n",
            "updating: content/llama.cpp/models/templates/NousResearch-Hermes-2-Pro-Llama-3-8B-tool_use.jinja (deflated 73%)\n",
            "updating: content/llama.cpp/models/templates/mistralai-Mistral-Nemo-Instruct-2407.jinja (deflated 74%)\n",
            "updating: content/llama.cpp/models/templates/microsoft-Phi-3.5-mini-instruct.jinja (deflated 61%)\n",
            "updating: content/llama.cpp/models/templates/fireworks-ai-llama-3-firefunction-v2.jinja (deflated 64%)\n",
            "updating: content/llama.cpp/models/templates/google-gemma-2-2b-it.jinja (deflated 52%)\n",
            "updating: content/llama.cpp/models/templates/llama-cpp-deepseek-r1.jinja (deflated 74%)\n",
            "updating: content/llama.cpp/models/templates/README.md (deflated 80%)\n",
            "updating: content/llama.cpp/models/templates/Qwen-Qwen2.5-7B-Instruct.jinja (deflated 71%)\n",
            "updating: content/llama.cpp/models/templates/meetkai-functionary-medium-v3.1.jinja (deflated 67%)\n",
            "updating: content/llama.cpp/models/templates/deepseek-ai-DeepSeek-R1-Distill-Llama-8B.jinja (deflated 75%)\n",
            "updating: content/llama.cpp/models/templates/meta-llama-Llama-3.2-3B-Instruct.jinja (deflated 71%)\n",
            "updating: content/llama.cpp/models/templates/meetkai-functionary-medium-v3.2.jinja (deflated 79%)\n",
            "updating: content/llama.cpp/models/templates/deepseek-ai-DeepSeek-R1-Distill-Qwen-32B.jinja (deflated 75%)\n",
            "updating: content/llama.cpp/models/templates/NousResearch-Hermes-3-Llama-3.1-8B-tool_use.jinja (deflated 73%)\n",
            "updating: content/llama.cpp/models/templates/CohereForAI-c4ai-command-r-plus-tool_use.jinja (deflated 71%)\n",
            "updating: content/llama.cpp/models/templates/CohereForAI-c4ai-command-r7b-12-2024-tool_use.jinja (deflated 64%)\n",
            "updating: content/llama.cpp/models/templates/meta-llama-Llama-3.3-70B-Instruct.jinja (deflated 72%)\n",
            "updating: content/llama.cpp/models/templates/meta-llama-Llama-3.1-8B-Instruct.jinja (deflated 72%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-gpt-4o.gguf.out (deflated 63%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-gpt-4o.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-starcoder.gguf (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-refact.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-bert-bge.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-refact.gguf (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-r1-qwen.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-llama-spm.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.out (deflated 72%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-llama-bpe.gguf (deflated 72%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-command-r.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.out (deflated 70%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-phi-3.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-aquila.gguf (deflated 72%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-llm.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.out (deflated 67%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-phi-3.gguf (deflated 74%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-gpt-2.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-starcoder.gguf.out (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-chameleon.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-gpt-2.gguf.out (deflated 71%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-phi-3.gguf.out (deflated 76%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.out (deflated 71%)\n",
            "updating: content/llama.cpp/models/.editorconfig (stored 0%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-roberta-bpe.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-mpt.gguf (deflated 70%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-falcon.gguf (deflated 70%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-refact.gguf.out (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-bert-bge.gguf.out (deflated 74%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-mpt.gguf.out (deflated 68%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-gpt-2.gguf (deflated 71%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-gpt-neox.gguf (deflated 70%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-baichuan.gguf (deflated 74%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-bert-bge.gguf (deflated 77%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-llama-spm.gguf (deflated 74%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-r1-qwen.gguf.out (deflated 67%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-falcon.gguf.out (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-chameleon.gguf.out (deflated 78%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-llama-bpe.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf (deflated 70%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-llama-spm.gguf.out (deflated 76%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-qwen2.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-deepseek-coder.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-starcoder.gguf.inp (deflated 69%)\n",
            "updating: content/llama.cpp/models/ggml-vocab-command-r.gguf.out (deflated 69%)\n",
            "updating: content/llama.cpp/requirements/ (stored 0%)\n",
            "updating: content/llama.cpp/requirements/requirements-convert_legacy_llama.txt (deflated 13%)\n",
            "updating: content/llama.cpp/requirements/requirements-pydantic.txt (stored 0%)\n",
            "updating: content/llama.cpp/requirements/requirements-tool_bench.txt (deflated 27%)\n",
            "updating: content/llama.cpp/requirements/requirements-convert_lora_to_gguf.txt (deflated 13%)\n",
            "updating: content/llama.cpp/requirements/requirements-convert_hf_to_gguf_update.txt (deflated 10%)\n",
            "updating: content/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt (deflated 10%)\n",
            "updating: content/llama.cpp/requirements/requirements-all.txt (deflated 67%)\n",
            "updating: content/llama.cpp/requirements/requirements-compare-llama-bench.txt (stored 0%)\n",
            "updating: content/llama.cpp/requirements/requirements-test-tokenizer-random.txt (stored 0%)\n",
            "updating: content/llama.cpp/requirements/requirements-convert_llama_ggml_to_gguf.txt (stored 0%)\n",
            "updating: content/llama.cpp/.gitmodules (deflated 21%)\n",
            "updating: content/llama.cpp/.pre-commit-config.yaml (deflated 47%)\n",
            "updating: content/llama.cpp/.dockerignore (deflated 27%)\n",
            "updating: content/llama.cpp/.github/ (stored 0%)\n",
            "updating: content/llama.cpp/.github/workflows/ (stored 0%)\n",
            "updating: content/llama.cpp/.github/workflows/server.yml (deflated 74%)\n",
            "updating: content/llama.cpp/.github/workflows/editorconfig.yml (deflated 49%)\n",
            "updating: content/llama.cpp/.github/workflows/python-type-check.yml (deflated 58%)\n",
            "updating: content/llama.cpp/.github/workflows/build.yml (deflated 85%)\n",
            "updating: content/llama.cpp/.github/workflows/bench.yml.disabled (deflated 71%)\n",
            "updating: content/llama.cpp/.github/workflows/python-lint.yml (deflated 50%)\n",
            "updating: content/llama.cpp/.github/workflows/gguf-publish.yml (deflated 47%)\n",
            "updating: content/llama.cpp/.github/workflows/labeler.yml (deflated 41%)\n",
            "updating: content/llama.cpp/.github/workflows/close-issue.yml (deflated 51%)\n",
            "updating: content/llama.cpp/.github/workflows/python-check-requirements.yml (deflated 61%)\n",
            "updating: content/llama.cpp/.github/workflows/docker.yml (deflated 71%)\n",
            "updating: content/llama.cpp/.github/pull_request_template.md (deflated 11%)\n",
            "updating: content/llama.cpp/.github/labeler.yml (deflated 79%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/ (stored 0%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/019-bug-misc.yml (deflated 62%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/config.yml (deflated 53%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/010-bug-compilation.yml (deflated 62%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/030-research.yml (deflated 59%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/011-bug-results.yml (deflated 60%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/040-refactor.yml (deflated 56%)\n",
            "updating: content/llama.cpp/.github/ISSUE_TEMPLATE/020-enhancement.yml (deflated 63%)\n",
            "updating: content/llama.cpp/README.md (deflated 65%)\n",
            "updating: content/llama.cpp/.devops/ (stored 0%)\n",
            "updating: content/llama.cpp/.devops/nix/ (stored 0%)\n",
            "updating: content/llama.cpp/.devops/nix/jetson-support.nix (deflated 62%)\n",
            "updating: content/llama.cpp/.devops/nix/python-scripts.nix (deflated 51%)\n",
            "updating: content/llama.cpp/.devops/nix/package.nix (deflated 60%)\n",
            "updating: content/llama.cpp/.devops/nix/apps.nix (deflated 51%)\n",
            "updating: content/llama.cpp/.devops/nix/sif.nix (deflated 40%)\n",
            "updating: content/llama.cpp/.devops/nix/docker.nix (deflated 46%)\n",
            "updating: content/llama.cpp/.devops/nix/package-gguf-py.nix (deflated 48%)\n",
            "updating: content/llama.cpp/.devops/nix/scope.nix (deflated 61%)\n",
            "updating: content/llama.cpp/.devops/nix/devshells.nix (deflated 65%)\n",
            "updating: content/llama.cpp/.devops/nix/nixpkgs-instances.nix (deflated 54%)\n",
            "updating: content/llama.cpp/.devops/musa.Dockerfile (deflated 62%)\n",
            "updating: content/llama.cpp/.devops/llama-cpp-cuda.srpm.spec (deflated 51%)\n",
            "updating: content/llama.cpp/.devops/tools.sh (deflated 64%)\n",
            "updating: content/llama.cpp/.devops/rocm.Dockerfile (deflated 59%)\n",
            "updating: content/llama.cpp/.devops/vulkan.Dockerfile (deflated 62%)\n",
            "updating: content/llama.cpp/.devops/llama-cpp.srpm.spec (deflated 51%)\n",
            "updating: content/llama.cpp/.devops/cuda.Dockerfile (deflated 60%)\n",
            "updating: content/llama.cpp/.devops/cpu.Dockerfile (deflated 63%)\n",
            "updating: content/llama.cpp/.devops/intel.Dockerfile (deflated 62%)\n",
            "updating: content/llama.cpp/.devops/llama-cli-cann.Dockerfile (deflated 71%)\n",
            "updating: content/llama.cpp/.devops/cloud-v-pipeline (deflated 53%)\n",
            "updating: content/llama.cpp/Makefile (deflated 78%)\n",
            "updating: content/llama.cpp/pocs/ (stored 0%)\n",
            "updating: content/llama.cpp/pocs/vdot/ (stored 0%)\n",
            "updating: content/llama.cpp/pocs/vdot/vdot.cpp (deflated 73%)\n",
            "updating: content/llama.cpp/pocs/vdot/q8dot.cpp (deflated 70%)\n",
            "updating: content/llama.cpp/pocs/vdot/CMakeLists.txt (deflated 59%)\n",
            "updating: content/llama.cpp/pocs/CMakeLists.txt (deflated 21%)\n",
            "updating: content/llama.cpp/CONTRIBUTING.md (deflated 56%)\n",
            "updating: content/llama.cpp/media/ (stored 0%)\n",
            "updating: content/llama.cpp/media/matmul.svg (deflated 93%)\n",
            "updating: content/llama.cpp/media/llama0-logo.png (deflated 2%)\n",
            "updating: content/llama.cpp/media/llama0-banner.png (deflated 8%)\n",
            "updating: content/llama.cpp/media/matmul.png (deflated 15%)\n",
            "updating: content/llama.cpp/media/llama1-logo.png (deflated 18%)\n",
            "updating: content/llama.cpp/media/llama1-banner.png (deflated 21%)\n",
            "updating: content/llama.cpp/pyproject.toml (deflated 48%)\n",
            "updating: content/llama.cpp/mypy.ini (deflated 37%)\n",
            "updating: content/llama.cpp/.clang-tidy (deflated 54%)\n",
            "updating: content/llama.cpp/SECURITY.md (deflated 55%)\n",
            "updating: content/llama.cpp/examples/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/run/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/run/README.md (deflated 53%)\n",
            "updating: content/llama.cpp/examples/run/linenoise.cpp/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/run/linenoise.cpp/linenoise.h (deflated 64%)\n",
            "updating: content/llama.cpp/examples/run/linenoise.cpp/linenoise.cpp (deflated 72%)\n",
            "updating: content/llama.cpp/examples/run/linenoise.cpp/LICENSE (deflated 49%)\n",
            "updating: content/llama.cpp/examples/run/run.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/examples/run/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/llm.vim (deflated 45%)\n",
            "updating: content/llama.cpp/examples/Miku.sh (deflated 54%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/mean.hpp (deflated 63%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/pca.hpp (deflated 70%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/README.md (deflated 55%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/cvector-generator.cpp (deflated 71%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/positive.txt (deflated 74%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/negative.txt (deflated 74%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/completions.txt (deflated 65%)\n",
            "updating: content/llama.cpp/examples/cvector-generator/CMakeLists.txt (deflated 32%)\n",
            "updating: content/llama.cpp/examples/speculative/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/speculative/README.md (deflated 50%)\n",
            "updating: content/llama.cpp/examples/speculative/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/speculative/speculative.cpp (deflated 78%)\n",
            "updating: content/llama.cpp/examples/llama-bench/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama-bench/README.md (deflated 77%)\n",
            "updating: content/llama.cpp/examples/llama-bench/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/llama-bench/llama-bench.cpp (deflated 82%)\n",
            "updating: content/llama.cpp/examples/chat.sh (deflated 34%)\n",
            "updating: content/llama.cpp/examples/chat-vicuna.sh (deflated 44%)\n",
            "updating: content/llama.cpp/examples/pydantic_models_to_grammar_examples.py (deflated 71%)\n",
            "updating: content/llama.cpp/examples/simple-chat/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/simple-chat/simple-chat.cpp (deflated 71%)\n",
            "updating: content/llama.cpp/examples/simple-chat/README.md (deflated 32%)\n",
            "updating: content/llama.cpp/examples/simple-chat/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/rpc/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/rpc/rpc-server.cpp (deflated 73%)\n",
            "updating: content/llama.cpp/examples/rpc/README.md (deflated 55%)\n",
            "updating: content/llama.cpp/examples/rpc/CMakeLists.txt (deflated 19%)\n",
            "updating: content/llama.cpp/examples/eval-callback/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/eval-callback/eval-callback.cpp (deflated 67%)\n",
            "updating: content/llama.cpp/examples/eval-callback/README.md (deflated 82%)\n",
            "updating: content/llama.cpp/examples/eval-callback/CMakeLists.txt (deflated 42%)\n",
            "updating: content/llama.cpp/examples/llama.android/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/settings.gradle.kts (deflated 48%)\n",
            "updating: content/llama.cpp/examples/llama.android/gradlew (deflated 60%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/proguard-rules.pro (deflated 44%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/build.gradle.kts (deflated 66%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher.xml (deflated 53%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-anydpi/ic_launcher_round.xml (deflated 53%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxxhdpi/ic_launcher.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher_round.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-hdpi/ic_launcher.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/values/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/values/themes.xml (deflated 17%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/values/colors.xml (deflated 58%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/values/strings.xml (deflated 20%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/xml/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/xml/backup_rules.xml (deflated 43%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/xml/data_extraction_rules.xml (deflated 49%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher_round.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xhdpi/ic_launcher.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-xxhdpi/ic_launcher.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_foreground.xml (deflated 63%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/drawable/ic_launcher_background.xml (deflated 93%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher_round.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/res/mipmap-mdpi/ic_launcher.webp (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/AndroidManifest.xml (deflated 61%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/Downloadable.kt (deflated 73%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainViewModel.kt (deflated 70%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/MainActivity.kt (deflated 71%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Color.kt (deflated 45%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Type.kt (deflated 68%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/src/main/java/com/example/llama/ui/theme/Theme.kt (deflated 65%)\n",
            "updating: content/llama.cpp/examples/llama.android/app/.gitignore (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/README.md (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/gradle/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/gradle/wrapper/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.jar (deflated 10%)\n",
            "updating: content/llama.cpp/examples/llama.android/gradle/wrapper/gradle-wrapper.properties (deflated 35%)\n",
            "updating: content/llama.cpp/examples/llama.android/gradle.properties (deflated 48%)\n",
            "updating: content/llama.cpp/examples/llama.android/build.gradle.kts (deflated 42%)\n",
            "updating: content/llama.cpp/examples/llama.android/.gitignore (deflated 34%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/proguard-rules.pro (deflated 44%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/build.gradle.kts (deflated 60%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/consumer-rules.pro (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/androidTest/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/androidTest/java/android/llama/cpp/ExampleInstrumentedTest.kt (deflated 49%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/AndroidManifest.xml (deflated 17%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/cpp/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/cpp/llama-android.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/cpp/CMakeLists.txt (deflated 53%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/java/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/java/android/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/main/java/android/llama/cpp/LLamaAndroid.kt (deflated 73%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/test/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/test/java/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/test/java/android/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/src/test/java/android/llama/cpp/ExampleUnitTest.kt (deflated 33%)\n",
            "updating: content/llama.cpp/examples/llama.android/llama/.gitignore (stored 0%)\n",
            "updating: content/llama.cpp/examples/deprecation-warning/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/deprecation-warning/deprecation-warning.cpp (deflated 53%)\n",
            "updating: content/llama.cpp/examples/deprecation-warning/README.md (deflated 60%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/llama_swiftuiApp.swift (deflated 24%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Resources/models/.gitignore (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/LoadCustomButton.swift (deflated 60%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/ContentView.swift (deflated 73%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/InputButton.swift (deflated 73%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/UI/DownloadButton.swift (deflated 72%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Models/LlamaState.swift (deflated 73%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/Contents.json (deflated 16%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui/Assets.xcassets/AppIcon.appiconset/Contents.json (deflated 34%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.cpp.swift/LibLlama.swift (deflated 74%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/README.md (deflated 46%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/contents.xcworkspacedata (deflated 27%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist (deflated 21%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/llama.swiftui.xcodeproj/project.pbxproj (deflated 81%)\n",
            "updating: content/llama.cpp/examples/llama.swiftui/.gitignore (deflated 8%)\n",
            "updating: content/llama.cpp/examples/main/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/main/README.md (deflated 67%)\n",
            "updating: content/llama.cpp/examples/main/CMakeLists.txt (deflated 28%)\n",
            "updating: content/llama.cpp/examples/main/main.cpp (deflated 76%)\n",
            "updating: content/llama.cpp/examples/chat-persistent.sh (deflated 64%)\n",
            "updating: content/llama.cpp/examples/speculative-simple/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/speculative-simple/speculative-simple.cpp (deflated 69%)\n",
            "updating: content/llama.cpp/examples/speculative-simple/README.md (deflated 45%)\n",
            "updating: content/llama.cpp/examples/speculative-simple/CMakeLists.txt (deflated 33%)\n",
            "updating: content/llama.cpp/examples/quantize-stats/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/quantize-stats/quantize-stats.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/examples/quantize-stats/CMakeLists.txt (deflated 36%)\n",
            "updating: content/llama.cpp/examples/json_schema_pydantic_example.py (deflated 58%)\n",
            "updating: content/llama.cpp/examples/infill/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/infill/infill.cpp (deflated 76%)\n",
            "updating: content/llama.cpp/examples/infill/README.md (deflated 56%)\n",
            "updating: content/llama.cpp/examples/infill/CMakeLists.txt (deflated 29%)\n",
            "updating: content/llama.cpp/examples/batched-bench/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/batched-bench/batched-bench.cpp (deflated 73%)\n",
            "updating: content/llama.cpp/examples/batched-bench/README.md (deflated 62%)\n",
            "updating: content/llama.cpp/examples/batched-bench/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/reason-act.sh (deflated 32%)\n",
            "updating: content/llama.cpp/examples/retrieval/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/retrieval/README.md (deflated 50%)\n",
            "updating: content/llama.cpp/examples/retrieval/retrieval.cpp (deflated 70%)\n",
            "updating: content/llama.cpp/examples/retrieval/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/convert_legacy_llama.py (deflated 75%)\n",
            "updating: content/llama.cpp/examples/simple/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/simple/README.md (deflated 51%)\n",
            "updating: content/llama.cpp/examples/simple/simple.cpp (deflated 73%)\n",
            "updating: content/llama.cpp/examples/simple/CMakeLists.txt (deflated 29%)\n",
            "updating: content/llama.cpp/examples/gguf/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf/gguf.cpp (deflated 76%)\n",
            "updating: content/llama.cpp/examples/gguf/CMakeLists.txt (deflated 27%)\n",
            "updating: content/llama.cpp/examples/imatrix/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/imatrix/README.md (deflated 53%)\n",
            "updating: content/llama.cpp/examples/imatrix/imatrix.cpp (deflated 72%)\n",
            "updating: content/llama.cpp/examples/imatrix/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/sycl/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/sycl/win-run-llama2.bat (deflated 22%)\n",
            "updating: content/llama.cpp/examples/sycl/build.sh (deflated 49%)\n",
            "updating: content/llama.cpp/examples/sycl/README.md (deflated 59%)\n",
            "updating: content/llama.cpp/examples/sycl/run-llama2.sh (deflated 42%)\n",
            "updating: content/llama.cpp/examples/sycl/ls-sycl-device.cpp (deflated 19%)\n",
            "updating: content/llama.cpp/examples/sycl/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/sycl/win-build-sycl.bat (deflated 50%)\n",
            "updating: content/llama.cpp/examples/jeopardy/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/jeopardy/README.md (deflated 45%)\n",
            "updating: content/llama.cpp/examples/jeopardy/jeopardy.sh (deflated 44%)\n",
            "updating: content/llama.cpp/examples/jeopardy/questions.txt (deflated 50%)\n",
            "updating: content/llama.cpp/examples/jeopardy/graph.py (deflated 60%)\n",
            "updating: content/llama.cpp/examples/jeopardy/qasheet.csv (deflated 51%)\n",
            "updating: content/llama.cpp/examples/simple-cmake-pkg/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/simple-cmake-pkg/README.md (deflated 50%)\n",
            "updating: content/llama.cpp/examples/simple-cmake-pkg/.gitignore (deflated 41%)\n",
            "updating: content/llama.cpp/examples/simple-cmake-pkg/CMakeLists.txt (deflated 33%)\n",
            "updating: content/llama.cpp/examples/convert-llama2c-to-ggml/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/convert-llama2c-to-ggml/README.md (deflated 55%)\n",
            "updating: content/llama.cpp/examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/examples/convert-llama2c-to-ggml/CMakeLists.txt (deflated 33%)\n",
            "updating: content/llama.cpp/examples/passkey/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/passkey/README.md (deflated 38%)\n",
            "updating: content/llama.cpp/examples/passkey/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/passkey/passkey.cpp (deflated 71%)\n",
            "updating: content/llama.cpp/examples/embedding/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/embedding/README.md (deflated 63%)\n",
            "updating: content/llama.cpp/examples/embedding/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/embedding/embedding.cpp (deflated 73%)\n",
            "updating: content/llama.cpp/examples/gritlm/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gritlm/gritlm.cpp (deflated 67%)\n",
            "updating: content/llama.cpp/examples/gritlm/README.md (deflated 57%)\n",
            "updating: content/llama.cpp/examples/gritlm/CMakeLists.txt (deflated 29%)\n",
            "updating: content/llama.cpp/examples/tts/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/tts/README.md (deflated 64%)\n",
            "updating: content/llama.cpp/examples/tts/tts-outetts.py (deflated 68%)\n",
            "updating: content/llama.cpp/examples/tts/tts.cpp (deflated 72%)\n",
            "updating: content/llama.cpp/examples/tts/CMakeLists.txt (deflated 28%)\n",
            "updating: content/llama.cpp/examples/tts/convert_pt_to_hf.py (deflated 63%)\n",
            "updating: content/llama.cpp/examples/llava/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llava/glmedge-convert-image-encoder-to-gguf.py (deflated 69%)\n",
            "updating: content/llama.cpp/examples/llava/glmedge-surgery.py (deflated 53%)\n",
            "updating: content/llama.cpp/examples/llava/llava-cli.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/examples/llava/android/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/llava/android/build_64.sh (deflated 24%)\n",
            "updating: content/llama.cpp/examples/llava/android/adb_run.sh (deflated 73%)\n",
            "updating: content/llama.cpp/examples/llava/README.md (deflated 62%)\n",
            "updating: content/llama.cpp/examples/llava/qwen2vl-cli.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/examples/llava/README-minicpmo2.6.md (deflated 64%)\n",
            "updating: content/llama.cpp/examples/llava/minicpmv-convert-image-encoder-to-gguf.py (deflated 71%)\n",
            "updating: content/llama.cpp/examples/llava/README-granitevision.md (deflated 62%)\n",
            "updating: content/llama.cpp/examples/llava/clip-quantize-cli.cpp (deflated 66%)\n",
            "updating: content/llama.cpp/examples/llava/convert_image_encoder_to_gguf.py (deflated 72%)\n",
            "updating: content/llama.cpp/examples/llava/clip.h (deflated 72%)\n",
            "updating: content/llama.cpp/examples/llava/clip.cpp (deflated 80%)\n",
            "updating: content/llama.cpp/examples/llava/llava.h (deflated 64%)\n",
            "updating: content/llama.cpp/examples/llava/minicpmv-cli.cpp (deflated 76%)\n",
            "updating: content/llama.cpp/examples/llava/llava.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/examples/llava/llava_surgery.py (deflated 53%)\n",
            "updating: content/llama.cpp/examples/llava/README-glmedge.md (deflated 59%)\n",
            "updating: content/llama.cpp/examples/llava/requirements.txt (deflated 21%)\n",
            "updating: content/llama.cpp/examples/llava/README-minicpmv2.6.md (deflated 64%)\n",
            "updating: content/llama.cpp/examples/llava/README-minicpmv2.5.md (deflated 65%)\n",
            "updating: content/llama.cpp/examples/llava/MobileVLM-README.md (deflated 76%)\n",
            "updating: content/llama.cpp/examples/llava/llava_surgery_v2.py (deflated 72%)\n",
            "updating: content/llama.cpp/examples/llava/qwen2_vl_surgery.py (deflated 67%)\n",
            "updating: content/llama.cpp/examples/llava/minicpmv-surgery.py (deflated 59%)\n",
            "updating: content/llama.cpp/examples/llava/CMakeLists.txt (deflated 76%)\n",
            "updating: content/llama.cpp/examples/llava/README-quantize.md (deflated 60%)\n",
            "updating: content/llama.cpp/examples/regex_to_grammar.py (deflated 44%)\n",
            "updating: content/llama.cpp/examples/ts-type-to-grammar.sh (deflated 49%)\n",
            "updating: content/llama.cpp/examples/save-load-state/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/save-load-state/save-load-state.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/examples/save-load-state/CMakeLists.txt (deflated 32%)\n",
            "updating: content/llama.cpp/examples/tokenize/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/tokenize/tokenize.cpp (deflated 70%)\n",
            "updating: content/llama.cpp/examples/tokenize/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/server-llama2-13B.sh (deflated 38%)\n",
            "updating: content/llama.cpp/examples/gbnf-validator/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gbnf-validator/gbnf-validator.cpp (deflated 71%)\n",
            "updating: content/llama.cpp/examples/gbnf-validator/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/llama.vim (deflated 73%)\n",
            "updating: content/llama.cpp/examples/export-lora/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/export-lora/README.md (deflated 59%)\n",
            "updating: content/llama.cpp/examples/export-lora/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/export-lora/export-lora.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/examples/gen-docs/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gen-docs/gen-docs.cpp (deflated 67%)\n",
            "updating: content/llama.cpp/examples/gen-docs/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/CMakeLists.txt (deflated 68%)\n",
            "updating: content/llama.cpp/examples/chat-13B.bat (deflated 52%)\n",
            "updating: content/llama.cpp/examples/parallel/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/parallel/README.md (deflated 15%)\n",
            "updating: content/llama.cpp/examples/parallel/parallel.cpp (deflated 70%)\n",
            "updating: content/llama.cpp/examples/parallel/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/lookahead/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/lookahead/README.md (deflated 28%)\n",
            "updating: content/llama.cpp/examples/lookahead/lookahead.cpp (deflated 76%)\n",
            "updating: content/llama.cpp/examples/lookahead/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/server/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/server.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/examples/server/chat.sh (deflated 55%)\n",
            "updating: content/llama.cpp/examples/server/httplib.h (deflated 81%)\n",
            "updating: content/llama.cpp/examples/server/README.md (deflated 68%)\n",
            "updating: content/llama.cpp/examples/server/utils.hpp (deflated 74%)\n",
            "updating: content/llama.cpp/examples/server/chat-llama2.sh (deflated 54%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/simplechat_screens.webp (deflated 1%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/ui.mjs (deflated 75%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/datautils.mjs (deflated 68%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/readme.md (deflated 61%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/simplechat.css (deflated 59%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/simplechat.js (deflated 76%)\n",
            "updating: content/llama.cpp/examples/server/public_simplechat/index.html (deflated 62%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/style.css (deflated 82%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/theme-ketivah.css (deflated 84%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/system-prompts.js (deflated 60%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/theme-mangotango.css (deflated 82%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/theme-snowstorm.css (deflated 84%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/theme-playground.css (deflated 83%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/colorthemes.css (deflated 82%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/json-schema-to-grammar.mjs (deflated 75%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/theme-polarnight.css (deflated 84%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/theme-beeninorder.css (deflated 83%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/favicon.ico (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/index.js (deflated 62%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/loading.html (deflated 39%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/index-new.html (deflated 73%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/prompt-formats.js (deflated 83%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/completion.js (deflated 67%)\n",
            "updating: content/llama.cpp/examples/server/public_legacy/index.html (deflated 75%)\n",
            "updating: content/llama.cpp/examples/server/themes/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/themes/wild/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/themes/wild/llamapattern.png (deflated 0%)\n",
            "updating: content/llama.cpp/examples/server/themes/wild/README.md (deflated 9%)\n",
            "updating: content/llama.cpp/examples/server/themes/wild/wild.png (deflated 1%)\n",
            "updating: content/llama.cpp/examples/server/themes/wild/llama_cpp.png (deflated 0%)\n",
            "updating: content/llama.cpp/examples/server/themes/wild/favicon.ico (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/themes/wild/index.html (deflated 74%)\n",
            "updating: content/llama.cpp/examples/server/themes/README.md (deflated 21%)\n",
            "updating: content/llama.cpp/examples/server/themes/buttons-top/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/themes/buttons-top/README.md (deflated 31%)\n",
            "updating: content/llama.cpp/examples/server/themes/buttons-top/favicon.ico (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/themes/buttons-top/buttons_top.png (deflated 2%)\n",
            "updating: content/llama.cpp/examples/server/themes/buttons-top/index.html (deflated 74%)\n",
            "updating: content/llama.cpp/examples/server/chat.mjs (deflated 62%)\n",
            "updating: content/llama.cpp/examples/server/CMakeLists.txt (deflated 50%)\n",
            "updating: content/llama.cpp/examples/server/public/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/public/loading.html (deflated 39%)\n",
            "updating: content/llama.cpp/examples/server/public/index.html.gz (deflated 0%)\n",
            "updating: content/llama.cpp/examples/server/webui/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/webui/.prettierignore (deflated 25%)\n",
            "updating: content/llama.cpp/examples/server/webui/tailwind.config.js (deflated 42%)\n",
            "updating: content/llama.cpp/examples/server/webui/tsconfig.node.json (deflated 47%)\n",
            "updating: content/llama.cpp/examples/server/webui/vite.config.ts (deflated 54%)\n",
            "updating: content/llama.cpp/examples/server/webui/postcss.config.js (deflated 18%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/utils/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/utils/types.ts (deflated 60%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/utils/common.tsx (deflated 46%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/utils/storage.ts (deflated 68%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/utils/app.context.tsx (deflated 72%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/utils/llama-vscode.ts (deflated 58%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/utils/misc.ts (deflated 58%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/SettingDialog.tsx (deflated 72%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/CanvasPyInterpreter.tsx (deflated 64%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/ChatScreen.tsx (deflated 67%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/Header.tsx (deflated 68%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/Sidebar.tsx (deflated 65%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/ChatMessage.tsx (deflated 76%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/components/MarkdownDisplay.tsx (deflated 65%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/App.tsx (deflated 61%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/main.tsx (deflated 37%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/index.scss (deflated 59%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/vite-env.d.ts (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/webui/src/Config.ts (deflated 60%)\n",
            "updating: content/llama.cpp/examples/server/webui/.gitignore (deflated 38%)\n",
            "updating: content/llama.cpp/examples/server/webui/package-lock.json (deflated 78%)\n",
            "updating: content/llama.cpp/examples/server/webui/package.json (deflated 60%)\n",
            "updating: content/llama.cpp/examples/server/webui/tsconfig.json (deflated 33%)\n",
            "updating: content/llama.cpp/examples/server/webui/public/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/webui/public/demo-conversation.json (deflated 56%)\n",
            "updating: content/llama.cpp/examples/server/webui/eslint.config.js (deflated 56%)\n",
            "updating: content/llama.cpp/examples/server/webui/tsconfig.app.json (deflated 48%)\n",
            "updating: content/llama.cpp/examples/server/webui/index.html (deflated 39%)\n",
            "updating: content/llama.cpp/examples/server/tests/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/tests/tests.sh (deflated 39%)\n",
            "updating: content/llama.cpp/examples/server/tests/README.md (deflated 55%)\n",
            "updating: content/llama.cpp/examples/server/tests/utils.py (deflated 74%)\n",
            "updating: content/llama.cpp/examples/server/tests/conftest.py (deflated 39%)\n",
            "updating: content/llama.cpp/examples/server/tests/pytest.ini (deflated 15%)\n",
            "updating: content/llama.cpp/examples/server/tests/.gitignore (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/tests/requirements.txt (deflated 16%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_chat_completion.py (deflated 80%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_ctx_shift.py (deflated 61%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_rerank.py (deflated 67%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_tokenize.py (deflated 69%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_security.py (deflated 67%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_lora.py (deflated 68%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_embedding.py (deflated 78%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_completion.py (deflated 80%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_basic.py (deflated 71%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_slot_save.py (deflated 79%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_tool_call.py (deflated 87%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_speculative.py (deflated 72%)\n",
            "updating: content/llama.cpp/examples/server/tests/unit/test_infill.py (deflated 72%)\n",
            "updating: content/llama.cpp/examples/server/bench/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/server/bench/README.md (deflated 60%)\n",
            "updating: content/llama.cpp/examples/server/bench/script.js (deflated 69%)\n",
            "updating: content/llama.cpp/examples/server/bench/prometheus.yml (deflated 31%)\n",
            "updating: content/llama.cpp/examples/server/bench/bench.py (deflated 71%)\n",
            "updating: content/llama.cpp/examples/server/bench/requirements.txt (stored 0%)\n",
            "updating: content/llama.cpp/examples/perplexity/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/perplexity/perplexity.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/examples/perplexity/README.md (deflated 70%)\n",
            "updating: content/llama.cpp/examples/perplexity/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/pydantic_models_to_grammar.py (deflated 82%)\n",
            "updating: content/llama.cpp/examples/batched/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/batched/batched.cpp (deflated 70%)\n",
            "updating: content/llama.cpp/examples/batched/README.md (deflated 55%)\n",
            "updating: content/llama.cpp/examples/batched/CMakeLists.txt (deflated 30%)\n",
            "updating: content/llama.cpp/examples/gguf-split/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf-split/tests.sh (deflated 70%)\n",
            "updating: content/llama.cpp/examples/gguf-split/gguf-split.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/examples/gguf-split/README.md (deflated 41%)\n",
            "updating: content/llama.cpp/examples/gguf-split/CMakeLists.txt (deflated 31%)\n",
            "updating: content/llama.cpp/examples/chat-13B.sh (deflated 45%)\n",
            "updating: content/llama.cpp/examples/json_schema_to_grammar.py (deflated 75%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/xxhash/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/xxhash/clib.json (deflated 33%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.h (deflated 77%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/xxhash/xxhash.c (deflated 48%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha1/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.h (deflated 54%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha1/package.json (deflated 35%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha1/sha1.c (deflated 66%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha256/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.h (deflated 47%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha256/sha256.c (deflated 63%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/sha256/package.json (deflated 41%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/rotate-bits/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/rotate-bits/package.json (deflated 38%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/deps/rotate-bits/rotate-bits.h (deflated 67%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/README.md (deflated 71%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/gguf-hash.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/examples/gguf-hash/CMakeLists.txt (deflated 54%)\n",
            "updating: content/llama.cpp/examples/quantize/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/quantize/tests.sh (deflated 62%)\n",
            "updating: content/llama.cpp/examples/quantize/README.md (deflated 67%)\n",
            "updating: content/llama.cpp/examples/quantize/quantize.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/examples/quantize/CMakeLists.txt (deflated 35%)\n",
            "updating: content/llama.cpp/examples/server_embd.py (deflated 55%)\n",
            "updating: content/llama.cpp/examples/batched.swift/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/batched.swift/Package.swift (deflated 51%)\n",
            "updating: content/llama.cpp/examples/batched.swift/Sources/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/batched.swift/Sources/main.swift (deflated 67%)\n",
            "updating: content/llama.cpp/examples/batched.swift/README.md (deflated 11%)\n",
            "updating: content/llama.cpp/examples/batched.swift/Makefile (deflated 38%)\n",
            "updating: content/llama.cpp/examples/batched.swift/.gitignore (deflated 29%)\n",
            "updating: content/llama.cpp/examples/lookup/ (stored 0%)\n",
            "updating: content/llama.cpp/examples/lookup/lookup-create.cpp (deflated 55%)\n",
            "updating: content/llama.cpp/examples/lookup/lookup.cpp (deflated 72%)\n",
            "updating: content/llama.cpp/examples/lookup/README.md (deflated 42%)\n",
            "updating: content/llama.cpp/examples/lookup/lookup-merge.cpp (deflated 64%)\n",
            "updating: content/llama.cpp/examples/lookup/CMakeLists.txt (deflated 79%)\n",
            "updating: content/llama.cpp/examples/lookup/lookup-stats.cpp (deflated 71%)\n",
            "updating: content/llama.cpp/src/ (stored 0%)\n",
            "updating: content/llama.cpp/src/llama-sampling.h (deflated 59%)\n",
            "updating: content/llama.cpp/src/unicode-data.h (deflated 60%)\n",
            "updating: content/llama.cpp/src/llama-kv-cache.h (deflated 72%)\n",
            "updating: content/llama.cpp/src/llama-impl.h (deflated 63%)\n",
            "updating: content/llama.cpp/src/llama-hparams.cpp (deflated 71%)\n",
            "updating: content/llama.cpp/src/llama-mmap.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/src/unicode-data.cpp (deflated 78%)\n",
            "updating: content/llama.cpp/src/llama-impl.cpp (deflated 73%)\n",
            "updating: content/llama.cpp/src/llama-chat.h (deflated 72%)\n",
            "updating: content/llama.cpp/src/llama-arch.cpp (deflated 92%)\n",
            "updating: content/llama.cpp/src/llama-kv-cache.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/src/llama-quant.h (stored 0%)\n",
            "updating: content/llama.cpp/src/llama-sampling.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/src/llama-grammar.h (deflated 70%)\n",
            "updating: content/llama.cpp/src/llama-model.cpp (deflated 89%)\n",
            "updating: content/llama.cpp/src/llama-context.cpp (deflated 82%)\n",
            "updating: content/llama.cpp/src/llama-context.h (deflated 68%)\n",
            "updating: content/llama.cpp/src/llama-adapter.h (deflated 68%)\n",
            "updating: content/llama.cpp/src/llama-model.h (deflated 84%)\n",
            "updating: content/llama.cpp/src/llama-model-loader.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/src/llama-arch.h (deflated 77%)\n",
            "updating: content/llama.cpp/src/llama-batch.h (deflated 66%)\n",
            "updating: content/llama.cpp/src/llama-adapter.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/src/llama-cparams.h (deflated 54%)\n",
            "updating: content/llama.cpp/src/llama-cparams.cpp (stored 0%)\n",
            "updating: content/llama.cpp/src/llama-vocab.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/src/llama-model-loader.h (deflated 71%)\n",
            "updating: content/llama.cpp/src/llama-quant.cpp (deflated 78%)\n",
            "updating: content/llama.cpp/src/llama-batch.cpp (deflated 78%)\n",
            "updating: content/llama.cpp/src/llama-chat.cpp (deflated 82%)\n",
            "updating: content/llama.cpp/src/unicode.h (deflated 68%)\n",
            "updating: content/llama.cpp/src/CMakeLists.txt (deflated 63%)\n",
            "updating: content/llama.cpp/src/llama-hparams.h (deflated 67%)\n",
            "updating: content/llama.cpp/src/llama-mmap.h (deflated 66%)\n",
            "updating: content/llama.cpp/src/llama.cpp (deflated 89%)\n",
            "updating: content/llama.cpp/src/llama-grammar.cpp (deflated 80%)\n",
            "updating: content/llama.cpp/src/llama-vocab.h (deflated 78%)\n",
            "updating: content/llama.cpp/src/unicode.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/LICENSE (deflated 41%)\n",
            "updating: content/llama.cpp/cmake/ (stored 0%)\n",
            "updating: content/llama.cpp/cmake/x64-windows-llvm.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/cmake/arm64-windows-msvc.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/cmake/arm64-apple-clang.cmake (deflated 51%)\n",
            "updating: content/llama.cpp/cmake/arm64-windows-llvm.cmake (deflated 51%)\n",
            "updating: content/llama.cpp/cmake/llama.pc.in (deflated 35%)\n",
            "updating: content/llama.cpp/cmake/git-vars.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/cmake/llama-config.cmake.in (deflated 56%)\n",
            "updating: content/llama.cpp/cmake/common.cmake (deflated 64%)\n",
            "updating: content/llama.cpp/cmake/build-info.cmake (deflated 65%)\n",
            "updating: content/llama.cpp/.editorconfig (deflated 65%)\n",
            "updating: content/llama.cpp/ggml/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/include/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-cpu.h (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-opt.h (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-alloc.h (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-cann.h (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-cpp.h (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-kompute.h (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-sycl.h (deflated 66%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-backend.h (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-metal.h (deflated 58%)\n",
            "updating: content/llama.cpp/ggml/include/gguf.h (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-cuda.h (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/include/ggml.h (deflated 83%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-rpc.h (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-blas.h (deflated 55%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-vulkan.h (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/include/ggml-opencl.h (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/Doxyfile (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.cpp (deflated 86%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/aclnn_ops.h (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f32_q8_0.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q4_0.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f16.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_f16_q8_0.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_f32.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/get_row_q8_0.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/quantize_float_to_q4_0.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/ascendc_kernels.h (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/CMakeLists.txt (deflated 58%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/kernels/dup.cpp (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/acl_tensor.h (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/common.h (deflated 70%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/acl_tensor.cpp (deflated 71%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/CMakeLists.txt (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/.clang-format (deflated 66%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cann/ggml-cann.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-common.h (deflated 83%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-quants.c (deflated 84%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-metal/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-metal/ggml-metal.metal (deflated 87%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-metal/ggml-metal.m (deflated 88%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-metal/ggml-metal-impl.h (deflated 88%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-metal/CMakeLists.txt (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.cpp (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/kleidiai/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kernels.cpp (deflated 93%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kernels.h (deflated 70%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kleidiai.h (deflated 31%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/kleidiai/kleidiai.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.h (deflated 92%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp (deflated 88%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/llamafile/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp (deflated 86%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.h (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-quants.c (deflated 87%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/cmake/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/cmake/FindSIMD.cmake (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.cpp (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/cpu-feats-x86.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-traits.h (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-hbm.h (deflated 27%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/amx/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.h (deflated 56%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp (deflated 86%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/amx/amx.h (deflated 28%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/amx/common.h (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/CMakeLists.txt (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c (deflated 88%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-impl.h (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cpu/ggml-cpu-aarch64.h (deflated 26%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/unary.cu (deflated 83%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/cpy.cuh (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/sum.cu (deflated 56%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/norm.cu (deflated 84%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_k.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-f16.cu (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/generate_cu_files.py (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_0.cu (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q2_k.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_s.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_0.cu (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_0.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_s.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_1.cu (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_nl.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q5_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q8_0.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q5_0.cu (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q4_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q5_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-f16.cu (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq3_xxs.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q5_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q6_k.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q3_k.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_k.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_1-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq4_xs.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q4_1.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q8_0.cu (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_1-q4_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q5_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q5_0-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_0-q8_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_0.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-q5_1.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-f16.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_1-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq1_s.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xs.cu (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_1.cu (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q4_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q5_1-q4_0.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-q5_1.cu (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cuh (deflated 40%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/pool2d.cu (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu (deflated 81%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/arange.cuh (deflated 7%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/mmv.cu (deflated 81%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/argsort.cuh (deflated 16%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/pad.cu (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f32.cuh (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu (deflated 78%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/scale.cu (deflated 56%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/clamp.cu (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-mma-f16.cuh (deflated 81%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/common.cuh (deflated 78%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/acc.cu (deflated 65%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cuh (deflated 12%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu (deflated 81%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/gla.cu (deflated 69%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/getrows.cu (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/cpy.cu (deflated 88%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-vec-f16.cuh (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/softmax.cu (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/tsembd.cuh (deflated 10%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/count-equal.cuh (deflated 9%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn.cu (deflated 87%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cuh (deflated 9%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/clamp.cuh (deflated 7%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/sumrows.cu (deflated 57%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/diagmask.cu (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/quantize.cuh (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu (deflated 78%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/vendors/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/vendors/cuda.h (deflated 51%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/vendors/musa.h (deflated 76%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/vendors/hip.h (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/binbcast.cuh (deflated 76%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/argmax.cu (deflated 69%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/mmvq.cuh (deflated 45%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/convert.cu (deflated 85%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/gla.cuh (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/cp-async.cuh (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/norm.cuh (deflated 70%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/concat.cuh (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/acc.cuh (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/mmv.cuh (deflated 55%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/sum.cuh (deflated 32%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/mmq.cuh (deflated 88%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/pool2d.cuh (deflated 7%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/tsembd.cu (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/im2col.cu (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/rope.cu (deflated 86%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/upscale.cu (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/pad.cuh (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/dequantize.cuh (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cuh (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/binbcast.cu (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn.cuh (deflated 15%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/convert.cuh (deflated 42%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cuh (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/softmax.cuh (deflated 39%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/quantize.cu (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/CMakeLists.txt (deflated 70%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/mmvq.cu (deflated 87%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/im2col.cuh (deflated 7%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/diagmask.cuh (deflated 9%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/argsort.cu (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/arange.cu (deflated 57%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/unary.cuh (deflated 86%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/rope.cuh (deflated 40%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/mmq.cu (deflated 76%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/mma.cuh (deflated 86%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/concat.cu (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/getrows.cuh (deflated 46%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/sumrows.cuh (deflated 33%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/fattn-common.cuh (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/scale.cuh (deflated 7%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/out-prod.cu (deflated 66%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/wkv6.cuh (deflated 8%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cuh (deflated 10%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/vecdotq.cuh (deflated 84%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/wkv6.cu (deflated 69%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/argmax.cuh (deflated 13%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/out-prod.cuh (deflated 14%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/upscale.cuh (deflated 7%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-cuda/count-equal.cu (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-musa/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-musa/CMakeLists.txt (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml.c (deflated 85%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-rpc/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-rpc/ggml-rpc.cpp (deflated 83%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-rpc/CMakeLists.txt (deflated 37%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/ggml-opencl.cpp (deflated 86%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_cvt.cl (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle_general.cl (deflated 89%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_16.cl (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/embed_kernel.py (deflated 42%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mul_mat_Ab_Bi_8x4.cl (deflated 76%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_mm.cl (deflated 87%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle.cl (deflated 89%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32_16.cl (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl.cl (deflated 89%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32.cl (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opencl/CMakeLists.txt (deflated 85%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-blas/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-blas/ggml-blas.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-blas/CMakeLists.txt (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-opt.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-threading.h (deflated 36%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-backend-impl.h (deflated 78%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f16.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale.comp (deflated 41%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_0.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_k.comp (deflated 73%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q6_k.comp (deflated 57%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/rope_common.comp (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_add.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f32.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_silu.comp (deflated 44%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_softmax.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_scale_8.comp (deflated 44%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_0.comp (deflated 55%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f16.comp (deflated 49%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_f16.comp (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_f32.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f16.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rmsnorm.comp (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_norm_f32.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f16.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q6_k.comp (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows_q4_1.comp (deflated 51%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n_pre.comp (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mv_q_n.comp (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q4_1.comp (deflated 57%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f16_f16.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_diagmask.comp (deflated 53%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_relu.comp (deflated 43%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_gelu.comp (deflated 42%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_cpy_f32_f32.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_norm.comp (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/common.comp (deflated 70%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_rope_neox_f32.comp (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_q8_0.comp (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_getrows.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul_mat_mat_f32.comp (deflated 54%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_mul.comp (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/kompute-shaders/op_addrow.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/ggml-kompute.cpp (deflated 85%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-kompute/CMakeLists.txt (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/div.comp (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq4_nl.comp (deflated 51%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q6_k.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q5_k.comp (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q2_k.comp (deflated 66%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/opt_step_adamw.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_1.comp (deflated 54%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/repeat_back.comp (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/add.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sum_rows.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_1.comp (deflated 53%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rms_norm.comp (deflated 55%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_binary_head.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs_cm2.comp (deflated 84%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q3_k.comp (deflated 73%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_unary_head.comp (deflated 76%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_vision.comp (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/repeat.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/silu.comp (deflated 41%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/wkv6.comp (deflated 69%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat2_support.comp (deflated 3%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/diag_mask_inf.comp (deflated 48%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_head.comp (deflated 58%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_head.comp (deflated 35%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/im2col.comp (deflated 65%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq2_xs.comp (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_xs.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq2_xxs.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/types.comp (deflated 82%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q2_k.comp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/argmax.comp (deflated 57%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sigmoid.comp (deflated 40%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp (deflated 73%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sin.comp (deflated 37%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq1_s.comp (deflated 49%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat_support.comp (deflated 3%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_split_k_reduce.comp (deflated 58%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/acc.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq3_s.comp (deflated 69%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/square.comp (deflated 38%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/norm.comp (deflated 55%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy_from_quant.comp (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy_to_quant.comp (deflated 79%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/tanh.comp (deflated 40%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/argsort.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/flash_attn_cm2.comp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/timestep_embedding.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq3_xxs.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_s.comp (deflated 65%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.comp (deflated 70%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q6_k.comp (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_p021.comp (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/soft_max.comp (deflated 69%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_f32.comp (deflated 38%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/relu.comp (deflated 40%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/leaky_relu.comp (deflated 42%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/copy.comp (deflated 43%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs.comp (deflated 88%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq3_xxs.comp (deflated 65%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_nc.comp (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/group_norm.comp (deflated 65%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq4_xs.comp (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq1_m.comp (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq2_xxs.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pool2d.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/contig_copy.comp (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/sub.comp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/scale.comp (deflated 45%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul.comp (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq1_m.comp (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/concat.comp (deflated 61%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq2_s.comp (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/count_equal.comp (deflated 48%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_cm2.comp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/generic_head.comp (deflated 24%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rope_multi.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt (deflated 34%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu.comp (deflated 43%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/pad.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q4_k.comp (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows_quant.comp (deflated 55%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/cos.comp (deflated 38%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/rms_norm_back.comp (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec.comp (deflated 68%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/silu_back.comp (deflated 46%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/gelu_quick.comp (deflated 43%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/clamp.comp (deflated 41%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_iq3_s.comp (deflated 56%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_0.comp (deflated 54%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q3_k.comp (deflated 62%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/soft_max_back.comp (deflated 59%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_0.comp (deflated 51%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/get_rows.comp (deflated 56%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mm.comp (deflated 85%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_iq1_s.comp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/vulkan-shaders/dequant_q8_0.comp (deflated 53%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/ggml-vulkan.cpp (deflated 87%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/cmake/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/cmake/host-toolchain.cmake.in (deflated 57%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-vulkan/CMakeLists.txt (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/gguf.cpp (deflated 83%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-alloc.c (deflated 81%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/softmax.hpp (deflated 38%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/gla.hpp (deflated 27%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/tsembd.hpp (deflated 38%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/concat.hpp (deflated 38%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/dequantize.hpp (deflated 84%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/convert.cpp (deflated 91%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/wkv6.hpp (deflated 26%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/mmq.cpp (deflated 91%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/norm.cpp (deflated 85%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/tsembd.cpp (deflated 60%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/getrows.hpp (deflated 44%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/im2col.hpp (deflated 45%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/rope.hpp (deflated 43%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/cpy.cpp (deflated 90%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/common.hpp (deflated 75%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/cpy.hpp (deflated 47%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/mmq.hpp (deflated 49%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/ggml-sycl.cpp (deflated 82%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/dpct/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/dpct/helper.hpp (deflated 84%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/presets.hpp (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/mmvq.hpp (deflated 47%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/backend.hpp (deflated 55%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/outprod.hpp (deflated 29%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/element_wise.cpp (deflated 92%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/gemm.hpp (deflated 74%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/mmvq.cpp (deflated 94%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/gla.cpp (deflated 72%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/sycl_hw.cpp (deflated 50%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/im2col.cpp (deflated 69%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/getrows.cpp (deflated 83%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/vecdotq.hpp (deflated 84%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/dmmv.hpp (deflated 47%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/concat.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/element_wise.hpp (deflated 85%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/norm.hpp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/convert.hpp (deflated 48%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/wkv6.cpp (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/CMakeLists.txt (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/dmmv.cpp (deflated 87%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/rope.cpp (deflated 80%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/sycl_hw.hpp (deflated 41%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/common.cpp (deflated 64%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/outprod.cpp (deflated 63%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/conv.cpp (deflated 65%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/conv.hpp (deflated 38%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-sycl/softmax.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-backend.cpp (deflated 83%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-threading.cpp (deflated 52%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-backend-reg.cpp (deflated 77%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-quants.h (deflated 90%)\n",
            "updating: content/llama.cpp/ggml/src/CMakeLists.txt (deflated 70%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-impl.h (deflated 73%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-hip/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/src/ggml-hip/CMakeLists.txt (deflated 67%)\n",
            "updating: content/llama.cpp/ggml/cmake/ (stored 0%)\n",
            "updating: content/llama.cpp/ggml/cmake/ggml-config.cmake.in (deflated 76%)\n",
            "updating: content/llama.cpp/ggml/.gitignore (deflated 36%)\n",
            "updating: content/llama.cpp/ggml/CMakeLists.txt (deflated 71%)\n",
            "updating: content/llama.cpp/pyrightconfig.json (deflated 47%)\n",
            "updating: content/llama.cpp/CMakePresets.json (deflated 84%)\n",
            "updating: content/llama.cpp/.gitignore (deflated 52%)\n",
            "updating: content/llama.cpp/scripts/ (stored 0%)\n",
            "updating: content/llama.cpp/scripts/tool_bench.py (deflated 71%)\n",
            "updating: content/llama.cpp/scripts/qnt-all.sh (deflated 44%)\n",
            "updating: content/llama.cpp/scripts/fetch_server_test_models.py (deflated 63%)\n",
            "updating: content/llama.cpp/scripts/debug-test.sh (deflated 60%)\n",
            "updating: content/llama.cpp/scripts/tool_bench.sh (deflated 78%)\n",
            "updating: content/llama.cpp/scripts/sync-ggml.sh (deflated 80%)\n",
            "updating: content/llama.cpp/scripts/run-all-ppl.sh (deflated 40%)\n",
            "updating: content/llama.cpp/scripts/run-all-perf.sh (deflated 41%)\n",
            "updating: content/llama.cpp/scripts/apple/ (stored 0%)\n",
            "updating: content/llama.cpp/scripts/apple/validate-visionos.sh (deflated 78%)\n",
            "updating: content/llama.cpp/scripts/apple/validate-ios.sh (deflated 78%)\n",
            "updating: content/llama.cpp/scripts/apple/validate-apps.sh (deflated 58%)\n",
            "updating: content/llama.cpp/scripts/apple/validate-macos.sh (deflated 78%)\n",
            "updating: content/llama.cpp/scripts/apple/validate-tvos.sh (deflated 78%)\n",
            "updating: content/llama.cpp/scripts/verify-checksum-models.py (deflated 59%)\n",
            "updating: content/llama.cpp/scripts/get-wikitext-2.sh (deflated 34%)\n",
            "updating: content/llama.cpp/scripts/sync-ggml.last (stored 0%)\n",
            "updating: content/llama.cpp/scripts/get_chat_template.py (deflated 58%)\n",
            "updating: content/llama.cpp/scripts/check-requirements.sh (deflated 57%)\n",
            "updating: content/llama.cpp/scripts/compare-commits.sh (deflated 51%)\n",
            "updating: content/llama.cpp/scripts/hf.sh (deflated 61%)\n",
            "updating: content/llama.cpp/scripts/sync-ggml-am.sh (deflated 76%)\n",
            "updating: content/llama.cpp/scripts/xxd.cmake (deflated 43%)\n",
            "updating: content/llama.cpp/scripts/build-info.sh (deflated 53%)\n",
            "updating: content/llama.cpp/scripts/get-flags.mk (deflated 61%)\n",
            "updating: content/llama.cpp/scripts/get-wikitext-103.sh (deflated 27%)\n",
            "updating: content/llama.cpp/scripts/get-pg.sh (deflated 50%)\n",
            "updating: content/llama.cpp/scripts/install-oneapi.bat (deflated 56%)\n",
            "updating: content/llama.cpp/scripts/gen-authors.sh (deflated 30%)\n",
            "updating: content/llama.cpp/scripts/get-hellaswag.sh (deflated 36%)\n",
            "updating: content/llama.cpp/scripts/get-winogrande.sh (deflated 38%)\n",
            "updating: content/llama.cpp/scripts/gen-unicode-data.py (deflated 69%)\n",
            "updating: content/llama.cpp/scripts/ci-run.sh (deflated 51%)\n",
            "updating: content/llama.cpp/scripts/compare-llama-bench.py (deflated 68%)\n",
            "updating: content/llama.cpp/requirements.txt (deflated 62%)\n",
            "updating: content/llama.cpp/common/ (stored 0%)\n",
            "updating: content/llama.cpp/common/sampling.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/common/ngram-cache.h (deflated 66%)\n",
            "updating: content/llama.cpp/common/ngram-cache.cpp (deflated 78%)\n",
            "updating: content/llama.cpp/common/console.h (deflated 44%)\n",
            "updating: content/llama.cpp/common/console.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/common/build-info.cpp.in (deflated 53%)\n",
            "updating: content/llama.cpp/common/log.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/common/arg.cpp (deflated 82%)\n",
            "updating: content/llama.cpp/common/arg.h (deflated 76%)\n",
            "updating: content/llama.cpp/common/json-schema-to-grammar.cpp (deflated 79%)\n",
            "updating: content/llama.cpp/common/cmake/ (stored 0%)\n",
            "updating: content/llama.cpp/common/cmake/build-info-gen-cpp.cmake (deflated 63%)\n",
            "updating: content/llama.cpp/common/common.h (deflated 72%)\n",
            "updating: content/llama.cpp/common/log.h (deflated 70%)\n",
            "updating: content/llama.cpp/common/json-schema-to-grammar.h (deflated 59%)\n",
            "updating: content/llama.cpp/common/sampling.h (deflated 68%)\n",
            "updating: content/llama.cpp/common/base64.hpp (deflated 76%)\n",
            "updating: content/llama.cpp/common/build-info.cpp (deflated 32%)\n",
            "updating: content/llama.cpp/common/chat.h (deflated 73%)\n",
            "updating: content/llama.cpp/common/CMakeLists.txt (deflated 64%)\n",
            "updating: content/llama.cpp/common/llguidance.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/common/speculative.cpp (deflated 73%)\n",
            "updating: content/llama.cpp/common/chat.cpp (deflated 81%)\n",
            "updating: content/llama.cpp/common/json.hpp (deflated 85%)\n",
            "updating: content/llama.cpp/common/common.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/common/speculative.h (deflated 62%)\n",
            "updating: content/llama.cpp/common/stb_image.h (deflated 75%)\n",
            "updating: content/llama.cpp/common/minja/ (stored 0%)\n",
            "updating: content/llama.cpp/common/minja/minja.hpp (deflated 82%)\n",
            "updating: content/llama.cpp/common/minja/chat-template.hpp (deflated 79%)\n",
            "updating: content/llama.cpp/docs/ (stored 0%)\n",
            "updating: content/llama.cpp/docs/build.md (deflated 64%)\n",
            "updating: content/llama.cpp/docs/function-calling.md (deflated 74%)\n",
            "updating: content/llama.cpp/docs/android.md (deflated 51%)\n",
            "updating: content/llama.cpp/docs/cuda-fedora.md (deflated 63%)\n",
            "updating: content/llama.cpp/docs/backend/ (stored 0%)\n",
            "updating: content/llama.cpp/docs/backend/CANN.md (deflated 68%)\n",
            "updating: content/llama.cpp/docs/backend/OPENCL.md (deflated 65%)\n",
            "updating: content/llama.cpp/docs/backend/BLIS.md (deflated 44%)\n",
            "updating: content/llama.cpp/docs/backend/SYCL.md (deflated 70%)\n",
            "updating: content/llama.cpp/docs/development/ (stored 0%)\n",
            "updating: content/llama.cpp/docs/development/debugging-tests.md (deflated 56%)\n",
            "updating: content/llama.cpp/docs/development/HOWTO-add-model.md (deflated 60%)\n",
            "updating: content/llama.cpp/docs/development/token_generation_performance_tips.md (deflated 51%)\n",
            "updating: content/llama.cpp/docs/development/llama-star/ (stored 0%)\n",
            "updating: content/llama.cpp/docs/development/llama-star/idea-arch.pdf (deflated 27%)\n",
            "updating: content/llama.cpp/docs/development/llama-star/idea-arch.key (deflated 24%)\n",
            "updating: content/llama.cpp/docs/docker.md (deflated 76%)\n",
            "updating: content/llama.cpp/docs/llguidance.md (deflated 55%)\n",
            "updating: content/llama.cpp/docs/install.md (deflated 52%)\n",
            "updating: content/llama.cpp/poetry.lock (deflated 71%)\n",
            "updating: content/llama.cpp/.flake8 (deflated 43%)\n",
            "updating: content/llama.cpp/ci/ (stored 0%)\n",
            "updating: content/llama.cpp/ci/README.md (deflated 48%)\n",
            "updating: content/llama.cpp/ci/run.sh (deflated 89%)\n",
            "updating: content/llama.cpp/CMakeLists.txt (deflated 69%)\n",
            "updating: content/llama.cpp/CODEOWNERS (deflated 55%)\n",
            "updating: content/llama.cpp/convert_llama_ggml_to_gguf.py (deflated 72%)\n",
            "updating: content/llama.cpp/convert_lora_to_gguf.py (deflated 72%)\n",
            "updating: content/llama.cpp/flake.lock (deflated 66%)\n",
            "updating: content/llama.cpp/build/ (stored 0%)\n",
            "updating: content/llama.cpp/build/bin/ (stored 0%)\n",
            "updating: content/llama.cpp/build/bin/test-quantize-perf (deflated 69%)\n",
            "updating: content/llama.cpp/build/bin/llama-server (deflated 43%)\n",
            "updating: content/llama.cpp/build/bin/llama-passkey (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-imatrix (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-perplexity (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-speculative-simple (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-tokenize (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-bench (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/test-tokenizer-1-bpe (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/libggml.so (deflated 68%)\n",
            "updating: content/llama.cpp/build/bin/llama-gguf-hash (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/libggml-base.so (deflated 62%)\n",
            "updating: content/llama.cpp/build/bin/test-tokenizer-0 (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/llama-cli (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-parallel (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/test-grammar-integration (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/test-chat-template (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-simple (deflated 74%)\n",
            "updating: content/llama.cpp/build/bin/llama-gguf-split (deflated 67%)\n",
            "updating: content/llama.cpp/build/bin/llama-quantize (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/llama-infill (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/libllama.so (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/libllava_shared.so (deflated 61%)\n",
            "updating: content/llama.cpp/build/bin/test-json-schema-to-grammar (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-minicpmv-cli (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/llama-llava-clip-quantize-cli (deflated 60%)\n",
            "updating: content/llama.cpp/build/bin/test-sampling (deflated 66%)\n",
            "updating: content/llama.cpp/build/bin/test-barrier (deflated 77%)\n",
            "updating: content/llama.cpp/build/bin/test-log (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-cvector-generator (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-embedding (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-save-load-state (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/test-quantize-fns (deflated 67%)\n",
            "updating: content/llama.cpp/build/bin/libggml-cpu.so (deflated 62%)\n",
            "updating: content/llama.cpp/build/bin/llama-speculative (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-retrieval (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-lookup-create (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/test-c (deflated 87%)\n",
            "updating: content/llama.cpp/build/bin/llama-gbnf-validator (deflated 77%)\n",
            "updating: content/llama.cpp/build/bin/test-tokenizer-1-spm (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/test-arg-parser (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-q8dot (deflated 71%)\n",
            "updating: content/llama.cpp/build/bin/llama-lookup (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-qwen2vl-cli (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/test-llama-grammar (deflated 67%)\n",
            "updating: content/llama.cpp/build/bin/llama-simple-chat (deflated 70%)\n",
            "updating: content/llama.cpp/build/bin/llama-convert-llama2c-to-ggml (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-export-lora (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-lookup-merge (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-llava-cli (deflated 63%)\n",
            "updating: content/llama.cpp/build/bin/llama-run (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-batched-bench (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-vdot (deflated 68%)\n",
            "updating: content/llama.cpp/build/bin/llama-batched (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/test-gguf (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/llama-lookahead (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/test-grammar-parser (deflated 66%)\n",
            "updating: content/llama.cpp/build/bin/llama-eval-callback (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-lookup-stats (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/llama-gguf (deflated 70%)\n",
            "updating: content/llama.cpp/build/bin/llama-gen-docs (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/test-chat (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/test-rope (deflated 73%)\n",
            "updating: content/llama.cpp/build/bin/llama-gritlm (deflated 65%)\n",
            "updating: content/llama.cpp/build/bin/test-model-load-cancel (deflated 81%)\n",
            "updating: content/llama.cpp/build/bin/llama-quantize-stats (deflated 66%)\n",
            "updating: content/llama.cpp/build/bin/test-backend-ops (deflated 72%)\n",
            "updating: content/llama.cpp/build/bin/llama-tts (deflated 64%)\n",
            "updating: content/llama.cpp/build/bin/test-autorelease (deflated 76%)\n",
            "updating: content/llama.cpp/build/DartConfiguration.tcl (deflated 53%)\n",
            "updating: content/llama.cpp/build/Makefile (deflated 90%)\n",
            "updating: content/llama.cpp/build/pocs/ (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/ (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/Makefile (deflated 80%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/cmake_clean.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.internal (deflated 83%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/flags.make (deflated 46%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o (deflated 61%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/build.make (deflated 75%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/link.txt (deflated 42%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-vdot.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/cmake_clean.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.internal (deflated 83%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/flags.make (deflated 46%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/build.make (deflated 75%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o (deflated 63%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/link.txt (deflated 42%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/llama-q8dot.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/pocs/vdot/cmake_install.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/pocs/Makefile (deflated 77%)\n",
            "updating: content/llama.cpp/build/pocs/CTestTestfile.cmake (deflated 40%)\n",
            "updating: content/llama.cpp/build/pocs/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/pocs/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/pocs/cmake_install.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/compile_commands.json (deflated 96%)\n",
            "updating: content/llama.cpp/build/examples/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/run/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/run/Makefile (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/run/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/DependInfo.cmake (deflated 63%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/cmake_clean.cmake (deflated 54%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.internal (deflated 89%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/progress.make (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o.d (deflated 85%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/compiler_depend.make (deflated 89%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/llama-run.dir/run.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/examples/run/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/run/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/Makefile (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CTestTestfile.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/DependInfo.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cmake_clean.cmake (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.ts (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/link.txt (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/cvector-generator/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/speculative/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/cmake_clean.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/llama-speculative.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/speculative/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/cmake_clean.cmake (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o (deflated 75%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/llama-bench.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/llama-bench/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/cmake_clean.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o (deflated 66%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/llama-simple-chat.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/simple-chat/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CTestTestfile.cmake (deflated 53%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/DependInfo.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/cmake_clean.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/eval-callback/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/main/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/main/Makefile (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/main/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/DependInfo.cmake (deflated 56%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o.d (deflated 85%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/main.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/cmake_clean.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.internal (deflated 85%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/progress.make (deflated 29%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/build.make (deflated 75%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/link.txt (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/llama-cli.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/main/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/main/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/Makefile (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CTestTestfile.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/DependInfo.cmake (deflated 60%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/cmake_clean.cmake (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.ts (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/link.txt (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/speculative-simple/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/DependInfo.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/cmake_clean.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o (deflated 75%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/quantize-stats/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/infill/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/infill/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/infill/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/cmake_clean.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/build.make (deflated 76%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/llama-infill.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/infill/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/infill/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/DependInfo.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/cmake_clean.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/llama-batched-bench.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/batched-bench/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/cmake_clean.cmake (deflated 46%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/llama-retrieval.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/retrieval/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/Makefile (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/simple/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/simple/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/cmake_clean.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/build.make (deflated 76%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/link.txt (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/llama-simple.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/simple/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/simple/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/gguf/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf/Makefile (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/DependInfo.cmake (deflated 56%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/cmake_clean.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/flags.make (deflated 33%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/build.make (deflated 75%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/link.txt (deflated 35%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/llama-gguf.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/gguf/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/cmake_clean.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/llama-imatrix.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/imatrix/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/CTestTestfile.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/Makefile (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CTestTestfile.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/cmake_clean.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.ts (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/link.txt (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/convert-llama2c-to-ggml/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/passkey/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/passkey/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/cmake_clean.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/link.txt (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/llama-passkey.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/passkey/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/passkey/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/embedding/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/embedding/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/cmake_clean.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/llama-embedding.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/embedding/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/embedding/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/cmake_clean.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/build.make (deflated 76%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/llama-gritlm.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/gritlm/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/tts/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tts/Makefile (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/tts/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/DependInfo.cmake (deflated 56%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/cmake_clean.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o (deflated 75%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/build.make (deflated 75%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/link.txt (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/llama-tts.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/tts/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/tts/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/llava/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/Makefile (deflated 86%)\n",
            "updating: content/llama.cpp/build/examples/llava/libllava_static.a (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/llava/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/cmake_clean.cmake (deflated 54%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/flags.make (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/link.txt (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/cmake_clean.cmake (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.internal (deflated 88%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/flags.make (deflated 51%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/clip.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/compiler_depend.make (deflated 89%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava.dir/llava.cpp.o (deflated 66%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/cmake_clean.cmake (deflated 46%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/flags.make (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/link.txt (deflated 51%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-llava-cli.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/cmake_clean.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/flags.make (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/link.txt (deflated 51%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/flags.make (deflated 28%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/build.make (deflated 72%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/link.txt (deflated 40%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_static.dir/cmake_clean_target.cmake (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/cmake_clean.cmake (deflated 48%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/flags.make (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/link.txt (deflated 51%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llama-minicpmv-cli.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/cmake_clean.cmake (deflated 28%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/flags.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/build.make (deflated 71%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/llava_shared.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/examples/llava/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/llava/cmake_install.cmake (deflated 85%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/Makefile (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CTestTestfile.cmake (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/DependInfo.cmake (deflated 60%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/cmake_clean.cmake (deflated 50%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/link.txt (deflated 46%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/save-load-state/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/cmake_clean.cmake (deflated 46%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/llama-tokenize.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/tokenize/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/DependInfo.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/cmake_clean.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/gbnf-validator/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/cmake_clean.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/llama-export-lora.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/export-lora/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/cmake_clean.cmake (deflated 46%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/gen-docs/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/parallel/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/parallel/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/cmake_clean.cmake (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/llama-parallel.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/parallel/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/parallel/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/cmake_clean.cmake (deflated 46%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/llama-lookahead.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/lookahead/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/server/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/server/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/server/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/server/loading.html.hpp (deflated 76%)\n",
            "updating: content/llama.cpp/build/examples/server/index.html.gz.hpp (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o.d (deflated 85%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/server.cpp.o (deflated 48%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/cmake_clean.cmake (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.internal (deflated 85%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/flags.make (deflated 51%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/progress.make (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/llama-server.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/server/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/server/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/cmake_clean.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/perplexity/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/batched/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/batched/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/cmake_clean.cmake (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/llama-batched.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/batched/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/batched/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/cmake_clean.cmake (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/link.txt (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/gguf-split/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/cmake_install.cmake (deflated 88%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/Makefile (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/cmake_clean.cmake (deflated 38%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.internal (deflated 80%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/flags.make (deflated 36%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o (deflated 52%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/build.make (deflated 76%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/depend.make (deflated 13%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha256.dir/compiler_depend.make (deflated 86%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/cmake_clean.cmake (deflated 39%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.internal (deflated 83%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/flags.make (deflated 36%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/build.make (deflated 76%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/xxhash.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/cmake_clean.cmake (deflated 46%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/flags.make (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/link.txt (deflated 50%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/DependInfo.cmake (deflated 56%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/cmake_clean.cmake (deflated 36%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.internal (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/flags.make (deflated 36%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o.d (deflated 81%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/build.make (deflated 75%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/depend.make (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/sha1.dir/compiler_depend.make (deflated 85%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/gguf-hash/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/quantize/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize/Makefile (deflated 79%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CTestTestfile.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/cmake_clean.cmake (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/flags.make (deflated 47%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/llama-quantize.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/quantize/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/quantize/cmake_install.cmake (deflated 67%)\n",
            "updating: content/llama.cpp/build/examples/lookup/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookup/Makefile (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CTestTestfile.cmake (deflated 42%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/cmake_clean.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-merge.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/cmake_clean.cmake (deflated 48%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-stats.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/cmake_clean.cmake (deflated 44%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/build.make (deflated 76%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/cmake_clean.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/flags.make (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/build.make (deflated 78%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/examples/lookup/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/examples/lookup/cmake_install.cmake (deflated 83%)\n",
            "updating: content/llama.cpp/build/CTestTestfile.cmake (deflated 46%)\n",
            "updating: content/llama.cpp/build/CMakeCache.txt (deflated 79%)\n",
            "updating: content/llama.cpp/build/src/ (stored 0%)\n",
            "updating: content/llama.cpp/build/src/Makefile (deflated 87%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/DependInfo.cmake (deflated 83%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o (deflated 74%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/cmake_clean.cmake (deflated 81%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.internal (deflated 96%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o (deflated 78%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/flags.make (deflated 41%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o (deflated 77%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/progress.make (deflated 74%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-quant.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d (deflated 80%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-grammar.cpp.o (deflated 74%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o (deflated 63%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/build.make (deflated 91%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o (deflated 79%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/link.txt (deflated 73%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o (deflated 61%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d (deflated 81%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/unicode-data.cpp.o (deflated 58%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama.cpp.o (deflated 73%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-mmap.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-adapter.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-chat.cpp.o (deflated 74%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/compiler_depend.make (deflated 96%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-vocab.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-impl.cpp.o (deflated 63%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-context.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/llama.dir/llama-sampling.cpp.o (deflated 74%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/src/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/src/cmake_install.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/ggml/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/Makefile (deflated 77%)\n",
            "updating: content/llama.cpp/build/ggml/src/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/ggml-cpu/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/ggml-cpu/Makefile (deflated 77%)\n",
            "updating: content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/ggml-cpu/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/ggml/src/ggml-cpu/cmake_install.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/ggml/src/Makefile (deflated 88%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/DependInfo.cmake (deflated 80%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o (deflated 72%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o (deflated 63%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o.d (deflated 35%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d (deflated 88%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d (deflated 87%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d (deflated 87%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o (deflated 76%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o (deflated 72%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o.d (deflated 87%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o.d (deflated 89%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d (deflated 86%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d (deflated 86%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o (deflated 66%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o (deflated 71%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o.d (deflated 87%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/cmake_clean.cmake (deflated 78%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.internal (deflated 96%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/flags.make (deflated 63%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/progress.make (deflated 68%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/build.make (deflated 90%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/link.txt (deflated 67%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-cpu.dir/compiler_depend.make (deflated 95%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/DependInfo.cmake (deflated 56%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/cmake_clean.cmake (deflated 40%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/flags.make (deflated 31%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/build.make (deflated 74%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o (deflated 72%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/link.txt (deflated 32%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/DependInfo.cmake (deflated 75%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o (deflated 81%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/cmake_clean.cmake (deflated 71%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.internal (deflated 94%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/flags.make (deflated 56%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o (deflated 60%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d (deflated 89%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/progress.make (deflated 65%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/build.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o (deflated 73%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d (deflated 85%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/link.txt (deflated 61%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d (deflated 86%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o (deflated 65%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o (deflated 65%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/compiler_depend.make (deflated 93%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o (deflated 63%)\n",
            "updating: content/llama.cpp/build/ggml/src/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/ggml/src/cmake_install.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/ggml/ggml-version.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/ggml/ggml-config.cmake (deflated 75%)\n",
            "updating: content/llama.cpp/build/ggml/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/ggml/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/ggml/cmake_install.cmake (deflated 78%)\n",
            "updating: content/llama.cpp/build/Testing/ (stored 0%)\n",
            "updating: content/llama.cpp/build/Testing/Temporary/ (stored 0%)\n",
            "updating: content/llama.cpp/build/common/ (stored 0%)\n",
            "updating: content/llama.cpp/build/common/Makefile (deflated 85%)\n",
            "updating: content/llama.cpp/build/common/libcommon.a (deflated 78%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/DependInfo.cmake (deflated 77%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o (deflated 79%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o.d (deflated 85%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean.cmake (deflated 73%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.internal (deflated 96%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o (deflated 62%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/progress.make (deflated 68%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/arg.cpp.o (deflated 77%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/speculative.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/build.make (deflated 89%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.ts (deflated 18%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/ngram-cache.cpp.o (deflated 73%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/link.txt (deflated 66%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/chat.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/common.cpp.o (deflated 74%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o (deflated 77%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/sampling.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o (deflated 65%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/log.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/compiler_depend.make (deflated 95%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/console.cpp.o.d (deflated 85%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/cmake_clean_target.cmake (stored 0%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/common.dir/llguidance.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/DependInfo.cmake (deflated 56%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/cmake_clean.cmake (deflated 41%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.internal (deflated 27%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/flags.make (deflated 28%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/progress.make (deflated 29%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/build.make (deflated 75%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o.d (deflated 20%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/build-info.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/build_info.dir/compiler_depend.make (deflated 41%)\n",
            "updating: content/llama.cpp/build/common/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/common/cmake_install.cmake (deflated 59%)\n",
            "updating: content/llama.cpp/build/llama-version.cmake (deflated 68%)\n",
            "updating: content/llama.cpp/build/llama.pc (deflated 31%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousConfigure.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/cmake_clean.cmake (deflated 27%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalCoverage.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/CMakeConfigureLog.yaml (deflated 93%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalBuild.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/cmake_clean.cmake (deflated 25%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.ts (deflated 10%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyCoverage.dir/compiler_depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/cmake_clean.cmake (deflated 25%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.ts (deflated 11%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyConfigure.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/cmake.check_cache (deflated 8%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/cmake_clean.cmake (deflated 23%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/build.make (deflated 65%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.ts (deflated 10%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyTest.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/cmake_clean.cmake (deflated 24%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.ts (deflated 9%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyUpdate.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/cmake_clean.cmake (deflated 27%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousCoverage.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Nightly.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Nightly.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Nightly.dir/cmake_clean.cmake (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Nightly.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Nightly.dir/build.make (deflated 64%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.ts (deflated 9%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Nightly.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/cmake_clean.cmake (deflated 27%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/build.make (deflated 68%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalConfigure.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalSubmit.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/cmake_clean.cmake (deflated 25%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousTest.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeCCompiler.cmake (deflated 70%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeCXXCompiler.cmake (deflated 73%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeDetermineCompilerABI_C.bin (deflated 86%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeSystem.cmake (deflated 63%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/a.out (deflated 85%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/CMakeCXXCompilerId.cpp (deflated 82%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdCXX/tmp/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/a.out (deflated 85%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/CMakeCCompilerId.c (deflated 81%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CompilerIdC/tmp/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/3.31.6/CMakeDetermineCompilerABI_CXX.bin (deflated 86%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/cmake_clean.cmake (deflated 25%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousStart.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousMemCheck.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/pkgRedirects/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/cmake_clean.cmake (deflated 25%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.ts (deflated 11%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemCheck.dir/compiler_depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/cmake_clean.cmake (deflated 27%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.ts (deflated 14%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalMemCheck.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/TargetDirectories.txt (deflated 93%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/cmake_clean.cmake (deflated 24%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.ts (deflated 9%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlySubmit.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/cmake_clean.cmake (deflated 25%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousBuild.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/cmake_clean.cmake (deflated 24%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.ts (deflated 9%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyBuild.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Makefile2 (deflated 94%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousSubmit.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/cmake_clean.cmake (deflated 27%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.ts (deflated 11%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalUpdate.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/CMakeRuleHashes.txt (deflated 54%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Experimental.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Experimental.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Experimental.dir/cmake_clean.cmake (deflated 24%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Experimental.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Experimental.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Experimental.dir/compiler_depend.make (deflated 22%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Continuous.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Continuous.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Continuous.dir/cmake_clean.cmake (deflated 23%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Continuous.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Continuous.dir/build.make (deflated 65%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.ts (deflated 11%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Continuous.dir/compiler_depend.make (deflated 21%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.ts (deflated 12%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalTest.dir/compiler_depend.make (deflated 23%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/FindOpenMP/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_C.bin (deflated 86%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/FindOpenMP/ompver_CXX.bin (deflated 86%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ContinuousUpdate.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.ts (deflated 11%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyMemoryCheck.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/cmake_clean.cmake (deflated 24%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/build.make (deflated 66%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.ts (deflated 9%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/NightlyStart.dir/compiler_depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/Makefile.cmake (deflated 88%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/cmake_clean.cmake (deflated 26%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/progress.make (stored 0%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/build.make (deflated 67%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.ts (deflated 13%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/ExperimentalStart.dir/compiler_depend.make (deflated 23%)\n",
            "updating: content/llama.cpp/build/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/cmake_install.cmake (deflated 77%)\n",
            "updating: content/llama.cpp/build/llama-config.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/tests/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/Makefile (deflated 91%)\n",
            "updating: content/llama.cpp/build/tests/CTestTestfile.cmake (deflated 92%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/DependInfo.cmake (deflated 63%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/cmake_clean.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/link.txt (deflated 49%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o (deflated 76%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/cmake_clean.cmake (deflated 51%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/cmake_clean.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o (deflated 70%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/DependInfo.cmake (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/cmake_clean.cmake (deflated 63%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.internal (deflated 86%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/flags.make (deflated 44%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/progress.make (deflated 44%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o (deflated 77%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/build.make (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.ts (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/link.txt (deflated 53%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/DependInfo.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/cmake_clean.cmake (deflated 51%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/build.make (deflated 77%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/link.txt (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/DependInfo.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/cmake_clean.cmake (deflated 50%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/progress.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o (deflated 67%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/test-log.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/compiler_depend.make (deflated 89%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-log.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/DependInfo.cmake (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/cmake_clean.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.internal (deflated 90%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/build.make (deflated 82%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.ts (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/link.txt (deflated 52%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o (deflated 65%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/depend.make (deflated 20%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/compiler_depend.make (deflated 90%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/DependInfo.cmake (deflated 63%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/cmake_clean.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/progress.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.ts (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/link.txt (deflated 50%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o (deflated 72%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/progress.marks (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/cmake_clean.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.internal (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/compiler_depend.make (deflated 89%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o.d (deflated 86%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/DependInfo.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/cmake_clean.cmake (deflated 51%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.internal (deflated 86%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/progress.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/DependInfo.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/cmake_clean.cmake (deflated 54%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/link.txt (deflated 47%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o (deflated 66%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/compiler_depend.make (deflated 89%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-barrier.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/DependInfo.cmake (deflated 63%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/cmake_clean.cmake (deflated 58%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.ts (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/link.txt (deflated 50%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/DependInfo.cmake (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/cmake_clean.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.internal (deflated 86%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o (deflated 77%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/build.make (deflated 82%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.ts (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/link.txt (deflated 52%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/depend.make (deflated 19%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/DependInfo.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/cmake_clean.cmake (deflated 39%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.internal (deflated 79%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/flags.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o (deflated 69%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/build.make (deflated 73%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.ts (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/test-c.c.o.d (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/link.txt (deflated 37%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-c.dir/compiler_depend.make (deflated 85%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/cmake_clean.cmake (deflated 55%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.internal (deflated 86%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/progress.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/link.txt (deflated 47%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o (deflated 78%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-sampling.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/cmake_clean.cmake (deflated 56%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/progress.make (deflated 44%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/build.make (deflated 80%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.ts (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o (deflated 72%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/DependInfo.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/cmake_clean.cmake (deflated 51%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.internal (deflated 86%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/progress.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/test-chat.cpp.o (deflated 76%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-chat.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/cmake_clean.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.internal (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/progress.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o (deflated 65%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/DependInfo.cmake (deflated 61%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/cmake_clean.cmake (deflated 51%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.internal (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/progress.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/build.make (deflated 79%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.ts (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/link.txt (deflated 45%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/depend.make (deflated 17%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/compiler_depend.make (deflated 89%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-rope.dir/test-rope.cpp.o (deflated 63%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/DependInfo.cmake (deflated 62%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o (deflated 82%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/cmake_clean.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.internal (deflated 86%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/progress.make (deflated 42%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o.d (deflated 83%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/build.make (deflated 81%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/link.txt (deflated 48%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/depend.make (deflated 18%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/compiler_depend.make (deflated 88%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o (deflated 64%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/ (stored 0%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/DependInfo.cmake (deflated 57%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/cmake_clean.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.internal (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/flags.make (deflated 43%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/progress.make (deflated 26%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/build.make (deflated 76%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.ts (deflated 15%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o.d (deflated 84%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/link.txt (deflated 41%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/depend.make (deflated 16%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o (deflated 68%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/test-tokenizer-0.dir/compiler_depend.make (deflated 87%)\n",
            "updating: content/llama.cpp/build/tests/CMakeFiles/CMakeDirectoryInformation.cmake (deflated 49%)\n",
            "updating: content/llama.cpp/build/tests/cmake_install.cmake (deflated 93%)\n",
            "updating: content/llama.cpp/build-xcframework.sh (deflated 79%)\n",
            "updating: content/llama.cpp/.clang-format (deflated 62%)\n",
            "updating: content/llama.cpp/tests/ (stored 0%)\n",
            "updating: content/llama.cpp/tests/test-log.cpp (deflated 67%)\n",
            "updating: content/llama.cpp/tests/test-grammar-integration.cpp (deflated 84%)\n",
            "updating: content/llama.cpp/tests/test-rope.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/tests/test-quantize-fns.cpp (deflated 75%)\n",
            "updating: content/llama.cpp/tests/test-tokenizer-0.py (deflated 68%)\n",
            "updating: content/llama.cpp/tests/test-autorelease.cpp (deflated 50%)\n",
            "updating: content/llama.cpp/tests/test-json-schema-to-grammar.cpp (deflated 88%)\n",
            "updating: content/llama.cpp/tests/test-tokenizer-1-spm.cpp (deflated 68%)\n",
            "updating: content/llama.cpp/tests/get-model.cpp (deflated 45%)\n",
            "updating: content/llama.cpp/tests/test-chat-template.cpp (deflated 85%)\n",
            "updating: content/llama.cpp/tests/test-tokenizer-0.cpp (deflated 74%)\n",
            "updating: content/llama.cpp/tests/test-sampling.cpp (deflated 82%)\n",
            "updating: content/llama.cpp/tests/test-model-load-cancel.cpp (deflated 51%)\n",
            "updating: content/llama.cpp/tests/run-json-schema-to-grammar.mjs (deflated 39%)\n",
            "updating: content/llama.cpp/tests/test-grammar-llguidance.cpp (deflated 84%)\n",
            "updating: content/llama.cpp/tests/test-tokenizer-0.sh (deflated 58%)\n",
            "updating: content/llama.cpp/tests/test-tokenizer-1-bpe.cpp (deflated 70%)\n",
            "updating: content/llama.cpp/tests/test-grammar-parser.cpp (deflated 86%)\n",
            "updating: content/llama.cpp/tests/test-gguf.cpp (deflated 84%)\n",
            "updating: content/llama.cpp/tests/test-opt.cpp (deflated 84%)\n",
            "updating: content/llama.cpp/tests/test-arg-parser.cpp (deflated 76%)\n",
            "updating: content/llama.cpp/tests/test-c.c (deflated 7%)\n",
            "updating: content/llama.cpp/tests/test-chat.cpp (deflated 84%)\n",
            "updating: content/llama.cpp/tests/test-lora-conversion-inference.sh (deflated 73%)\n",
            "updating: content/llama.cpp/tests/.gitignore (stored 0%)\n",
            "updating: content/llama.cpp/tests/test-double-float.cpp (deflated 53%)\n",
            "updating: content/llama.cpp/tests/test-tokenizer-random.py (deflated 73%)\n",
            "updating: content/llama.cpp/tests/test-backend-ops.cpp (deflated 84%)\n",
            "updating: content/llama.cpp/tests/get-model.h (stored 0%)\n",
            "updating: content/llama.cpp/tests/CMakeLists.txt (deflated 80%)\n",
            "updating: content/llama.cpp/tests/test-barrier.cpp (deflated 65%)\n",
            "updating: content/llama.cpp/tests/test-quantize-perf.cpp (deflated 80%)\n",
            "updating: content/llama.cpp/tests/test-llama-grammar.cpp (deflated 86%)\n",
            "updating: content/llama.cpp/convert_hf_to_gguf_update.py (deflated 72%)\n",
            "updating: content/llama.cpp/.ecrc (deflated 9%)\n",
            "updating: content/sample_data/ (stored 0%)\n",
            "updating: content/sample_data/README.md (deflated 39%)\n",
            "updating: content/sample_data/anscombe.json (deflated 83%)\n",
            "updating: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "updating: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "updating: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "updating: content/sample_data/california_housing_test.csv (deflated 76%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/content.zip')"
      ],
      "metadata": {
        "id": "9VEsnLtAmIla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6081d668-a440-4603-acda-b6cd9116f2a9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7a641733-34be-4f26-b973-e67f30755836\", \"content.zip\", 167313885)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuwA81J1ueXO",
        "outputId": "acda949e-f47e-4a81-a468-63f5861755ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.10.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.13)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Set up the model and tokenizer\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,  # or \"auto\" if you prefer\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Define your test data as a list of dictionaries with 'input' and 'reference' keys.\n",
        "# Replace these with your own examples or load them from a dataset.\n",
        "test_data = [\n",
        "    {\n",
        "        \"input\": \"What does the text say about write transactions, FoundationDB?\",\n",
        "        \"reference\": \"For write transactions, FoundationDB tracks the read/write key sets to form conflict detection sets. When concurrent transaction conflicts are detected, the meta service automatically retries the transaction\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Load evaluation metrics\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "# Loop through test examples\n",
        "for example in test_data:\n",
        "    input_text = example[\"input\"]\n",
        "    reference_text = example[\"reference\"]\n",
        "\n",
        "    # Encode the input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate the model output (tweak max_new_tokens as needed)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(prediction)\n",
        "    references.append(reference_text)\n",
        "\n",
        "# Note: BLEU expects a list of lists for references.\n",
        "bleu_result = bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "print(\"BLEU score:\", bleu_result)\n",
        "print(\"ROUGE scores:\", rouge_result)"
      ],
      "metadata": {
        "id": "q-MzBn9tIHub",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "2eca93c6e26e441fa5bc6aee92d51fd3",
            "a694757687504b32b83eb6d67ef73e2d",
            "c5686ab94f4542c7901b727a6b302fa7",
            "2c4cdb148ea041bd9193b576021f600d",
            "7472e5b662b6472487c5d7c56ada62de",
            "436964aa562d42368dd8c640cc538e90",
            "9983e63c8b664b6aa6c4c1565438037c",
            "26bafdb9e3c74da08ad5072ace3248a4",
            "4a545abf008e45dcbd40f803e84254da",
            "e9b6e53aedf2458595d615ed8f61d652",
            "032efa8da42843028a35a28197f2176b"
          ]
        },
        "outputId": "b9da1945-2c99-404c-86c3-ec79bd87a9c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eca93c6e26e441fa5bc6aee92d51fd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: {'bleu': 0.05318827859531979, 'precisions': [0.1568627450980392, 0.06, 0.04081632653061224, 0.020833333333333332], 'brevity_penalty': 1.0, 'length_ratio': 1.59375, 'translation_length': 51, 'reference_length': 32}\n",
            "ROUGE scores: {'rouge1': 0.19444444444444445, 'rouge2': 0.05714285714285714, 'rougeL': 0.1388888888888889, 'rougeLsum': 0.1388888888888889}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVlyiDsAuct0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}