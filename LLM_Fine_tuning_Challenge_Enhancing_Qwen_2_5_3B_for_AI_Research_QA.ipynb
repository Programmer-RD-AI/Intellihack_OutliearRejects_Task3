{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opdLejGbD7J6",
    "outputId": "8ba05d2c-5eee-4b01-c599-a42cb733801f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDrive\n"
     ]
    }
   ],
   "source": [
    "!cd drive/ && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414,
     "referenced_widgets": [
      "25e23a0485074053a64f0d9317ddb91a",
      "304ab46b292e44d8a5035089db59cc4a",
      "293fbc77370a48ff914d0c840aaf317b",
      "d1b6ca54c65b47e5bddb511e059b4403",
      "6d3c6403b2694c28b63c93025f3de3b9",
      "833206dbcdc441bca391157d09b86232",
      "e7249ce079da4cd5bb28d3cfebc232e0",
      "8750ce4493a542908a75112341b6c3bc",
      "d16371a97c07429683d3f51dfc4dbae8",
      "3fd283f12f1444ddbe04f30bb0329c6b",
      "3c509f7b3cd5436483a52f36a9edb4b3",
      "6a3609938b6b456aa7c6920da78f2861",
      "c211bc574ad5440da5d4d96ec2864781",
      "c5c1ee7fe1d74f08a50deabb7a47a7b1",
      "07d170f3d5ed46f38b3fd89db3c6b571",
      "fe469e8567224a1c9d2b5a6f7aed4bd9",
      "1a71efde72764ae688f95a8dfc3c9c08"
     ]
    },
    "id": "wad34zPjLNq0",
    "outputId": "208ab317-41a1-4f0e-d2c7-9df4e6f472eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'notebook_login': pass new_session='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' as keyword args. From version 1.0 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e23a0485074053a64f0d9317ddb91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Fcdqt04JELEr"
   },
   "outputs": [],
   "source": [
    "# !unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcRs8428LSge",
    "outputId": "a5033a76-f1b8-4dfc-ee2d-b05d4f03977c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
      "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpi4py in /usr/local/lib/python3.11/dist-packages (4.0.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement deepseed (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for deepseed\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets llama-cpp-python faiss-cpu rouge_score bitsandbytes\n",
    "!pip install -U bitsandbytes\n",
    "!pip install --upgrade transformers\n",
    "!pip install mpi4py\n",
    "!pip install -U deepseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFVejZVgLOw9",
    "outputId": "3e248caf-c525-4c77-83e8-811d5de31c6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    AutoModel,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import wandb\n",
    "import subprocess\n",
    "import shutil\n",
    "from typing import List, Optional, Dict, Any\n",
    "import time\n",
    "from llama_cpp import Llama\n",
    "import faiss\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import gc\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "# Set this environment variable to avoid memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Clear cache at startup\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "plKpXvlhGkPp"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # or \"Qwen/Qwen2.5-3B\"\n",
    "DATA_DIR = \"./data\"\n",
    "OUTPUT_DIR = \"./fine_tuned_model\"\n",
    "MAX_LENGTH = 2048\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_CONFIG = LoraConfig(\n",
    "    r=32,  # rank\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TZ-OKGRYGo2P"
   },
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, documents_dir):\n",
    "        self.documents_dir = Path(documents_dir)\n",
    "        self.documents = {}\n",
    "        self.load_documents()\n",
    "\n",
    "    def load_documents(self):\n",
    "        \"\"\"Load all markdown documents from the specified directory.\"\"\"\n",
    "        for file_path in self.documents_dir.glob(\"*.md\"):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                # Extract title from filename or first heading in the document\n",
    "                title = file_path.stem\n",
    "                self.documents[title] = content\n",
    "        print(f\"Loaded {len(self.documents)} documents.\")\n",
    "\n",
    "    def chunk_documents(self, chunk_size=1500, overlap=150):\n",
    "        \"\"\"Chunk documents into smaller pieces with overlap.\"\"\"\n",
    "        chunked_docs = []\n",
    "\n",
    "        for title, content in self.documents.items():\n",
    "            # Remove markdown formatting for cleaner text\n",
    "            text = re.sub(r\"```.*?```\", \"\", content, flags=re.DOTALL)\n",
    "            text = re.sub(r\"#+ \", \"\", text)\n",
    "            text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)\n",
    "\n",
    "            # Split into sentences (rough approximation)\n",
    "            sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "            for sentence in sentences:\n",
    "                sentence_length = len(sentence.split())\n",
    "                if current_length + sentence_length > chunk_size:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "                    # Start new chunk with overlap\n",
    "                    overlap_tokens = (\n",
    "                        current_chunk[-overlap:]\n",
    "                        if overlap < len(current_chunk)\n",
    "                        else current_chunk\n",
    "                    )\n",
    "                    current_chunk = overlap_tokens + [sentence]\n",
    "                    current_length = len(current_chunk)\n",
    "                else:\n",
    "                    current_chunk.append(sentence)\n",
    "                    current_length += sentence_length\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_docs.append(\n",
    "                    {\n",
    "                        \"title\": title,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"text\": chunk.strip(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return chunked_docs\n",
    "\n",
    "\n",
    "class QAGenerator:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen1.5-7B-Chat\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def generate_qa_pairs(self, chunks, num_questions_per_chunk=3):\n",
    "        qa_pairs = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            # Create a prompt for the model to generate questions and answers\n",
    "            prompt = f\"\"\"Given the following text from an AI research paper, generate {num_questions_per_chunk} relevant question-answer pairs.\n",
    "The questions should be detailed and test deep understanding of the concepts.\n",
    "Make sure the answer is comprehensive and accurate based on the text.\n",
    "\n",
    "TEXT:\n",
    "{chunk[\"text\"]}\n",
    "\n",
    "FORMAT:\n",
    "Q1: [Question 1]\n",
    "A1: [Answer 1]\n",
    "Q2: [Question 2]\n",
    "A2: [Answer 2]\n",
    "Q3: [Question 3]\n",
    "A3: [Answer 3]\n",
    "\"\"\"\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,  # Generate up to 1024 new tokens\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Extract questions and answers\n",
    "            pairs = self.extract_qa_pairs(response)\n",
    "\n",
    "            for q, a in pairs:\n",
    "                qa_pairs.append(\n",
    "                    {\n",
    "                        \"title\": chunk[\"title\"],\n",
    "                        \"chunk_id\": chunk[\"chunk_id\"],\n",
    "                        \"context\": chunk[\"text\"],\n",
    "                        \"question\": q,\n",
    "                        \"answer\": a,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    def extract_qa_pairs(self, text):\n",
    "        # Extract Q/A pairs using regex\n",
    "        pattern = r\"Q\\d+:\\s*(.*?)\\s*\\nA\\d+:\\s*(.*?)(?=\\nQ\\d+:|$)\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "        # Clean up the extracted pairs\n",
    "        pairs = []\n",
    "        for question, answer in matches:\n",
    "            question = question.strip()\n",
    "            answer = answer.strip()\n",
    "            if question and answer:  # Ensure both question and answer are non-empty\n",
    "                pairs.append((question, answer))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "\n",
    "def create_synthetic_data(documents_dir=\"./dataset/q3_dataset\", output_dir=\"./data\"):\n",
    "    # Process documents\n",
    "    processor = DocumentProcessor(documents_dir)\n",
    "    chunks = processor.chunk_documents()\n",
    "\n",
    "    # Generate QA pairs\n",
    "    generator = QAGenerator()\n",
    "    qa_pairs = generator.generate_qa_pairs(chunks)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_list(qa_pairs)\n",
    "\n",
    "    # Create train/validation/test splits\n",
    "    splits = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_valid = splits[\"train\"]\n",
    "    test = splits[\"test\"]\n",
    "\n",
    "    # Further split train into train and validation\n",
    "    splits = train_valid.train_test_split(\n",
    "        test_size=0.25, seed=42\n",
    "    )  # 0.25 * 0.8 = 0.2 of original data\n",
    "    train = splits[\"train\"]\n",
    "    validation = splits[\"test\"]\n",
    "\n",
    "    # Save datasets\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train.to_json(os.path.join(output_dir, \"train.json\"))\n",
    "    validation.to_json(os.path.join(output_dir, \"validation.json\"))\n",
    "    test.to_json(os.path.join(output_dir, \"test.json\"))\n",
    "\n",
    "    print(\n",
    "        f\"Dataset created with {len(train)} training, {len(validation)} validation, and {len(test)} test examples.\"\n",
    "    )\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DpckIntfGs6K"
   },
   "outputs": [],
   "source": [
    "class QAFineTuner:\n",
    "    def __init__(self, model_name, data_dir, output_dir):\n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.train_dataset = None\n",
    "        self.validation_dataset = None\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load and preprocess the datasets.\"\"\"\n",
    "        train_path = os.path.join(self.data_dir, \"train.json\")\n",
    "        validation_path = os.path.join(self.data_dir, \"validation.json\")\n",
    "\n",
    "        self.train_dataset = load_dataset(\"json\", data_files=train_path)[\"train\"]\n",
    "        self.validation_dataset = load_dataset(\"json\", data_files=validation_path)[\n",
    "            \"train\"\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            f\"Loaded {len(self.train_dataset)} training examples and {len(self.validation_dataset)} validation examples.\"\n",
    "        )\n",
    "\n",
    "    def prepare_model(self):\n",
    "        \"\"\"Load and prepare the model with LoRA.\"\"\"\n",
    "        # Clear memory before model loading\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        try:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            import bitsandbytes\n",
    "\n",
    "            print(f\"Using bitsandbytes version: {bitsandbytes.__version__}\")\n",
    "\n",
    "            # Configure quantization for memory efficiency\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "            )\n",
    "\n",
    "            # Load model with quantization\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                quantization_config=bnb_config,\n",
    "                use_cache=False,  # Important for training\n",
    "            )\n",
    "\n",
    "            # Apply LoRA adapter\n",
    "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
    "\n",
    "            print(\"Successfully loaded model with 4-bit quantization and LoRA adapters\")\n",
    "\n",
    "        except (ImportError, ModuleNotFoundError) as e:\n",
    "            print(f\"Warning: Could not use quantization: {e}\")\n",
    "            print(\"Falling back to CPU loading with offloading\")\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"cpu\",\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "\n",
    "            # Apply LoRA adapter\n",
    "            self.model = get_peft_model(self.model, LORA_CONFIG)\n",
    "\n",
    "            # Move to GPU selectively if possible\n",
    "            try:\n",
    "                self.model.to_bettertransformer()\n",
    "            except:\n",
    "                print(\"Could not convert to BetterTransformer\")\n",
    "\n",
    "        # Print trainable parameters info\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def format_instruction(self, example):\n",
    "        \"\"\"Format the input as an instruction.\"\"\"\n",
    "        context = example[\"context\"]\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answer\"]\n",
    "\n",
    "        instruction = f\"\"\"### System:\n",
    "You are an AI assistant that specializes in answering questions about AI research papers.\n",
    "Your responses should be comprehensive, accurate, and based on the provided context.\n",
    "\n",
    "### Human:\n",
    "I have a question about an AI research paper.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "### Assistant:\n",
    "{answer}\n",
    "\"\"\"\n",
    "        return instruction\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        \"\"\"Tokenize and format the examples.\"\"\"\n",
    "        instructions = []\n",
    "\n",
    "        for i in range(len(examples[\"context\"])):\n",
    "            example = {\n",
    "                \"context\": examples[\"context\"][i],\n",
    "                \"question\": examples[\"question\"][i],\n",
    "                \"answer\": examples[\"answer\"][i],\n",
    "            }\n",
    "            instructions.append(self.format_instruction(example))\n",
    "\n",
    "        tokenized = self.tokenizer(\n",
    "            instructions,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "        return tokenized\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"Prepare tokenized datasets for training.\"\"\"\n",
    "        tokenize_batch_size = 8\n",
    "\n",
    "        self.train_dataset = self.train_dataset.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=tokenize_batch_size,\n",
    "            remove_columns=self.train_dataset.column_names,\n",
    "        )\n",
    "\n",
    "        self.validation_dataset = self.validation_dataset.map(\n",
    "            self.tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=tokenize_batch_size,\n",
    "            remove_columns=self.validation_dataset.column_names,\n",
    "        )\n",
    "\n",
    "        print(f\"Tokenized datasets: {self.train_dataset}, {self.validation_dataset}\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        # Clear CUDA cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Initialize wandb for tracking\n",
    "        wandb.init(project=\"qwen-ai-research-qa\", name=\"qwen-2.5-3b-qlora\")\n",
    "\n",
    "        # Make sure no DeepSpeed configurations are active\n",
    "        for key in list(os.environ.keys()):\n",
    "            if \"DEEPSPEED\" in key or \"DS_\" in key:\n",
    "                del os.environ[key]\n",
    "\n",
    "        # Configure training arguments with NO DeepSpeed\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=16,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            weight_decay=0.01,\n",
    "            warmup_ratio=0.1,\n",
    "            logging_dir=os.path.join(self.output_dir, \"logs\"),\n",
    "            logging_steps=50,\n",
    "            eval_steps=1000,\n",
    "            save_steps=1000,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            report_to=\"wandb\",\n",
    "            # Switch to standard FP32 precision\n",
    "            bf16=False,\n",
    "            fp16=False,\n",
    "            # DeepSpeed settings - force disable\n",
    "            deepspeed=None,\n",
    "            local_rank=-1,\n",
    "            ddp_backend=None,  # Don't use any distributed backend\n",
    "        )\n",
    "\n",
    "        # Create trainer with standard optimizer\n",
    "        from transformers import AdamW\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # Create data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "\n",
    "        # Create trainer with explicit optimizer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.validation_dataset,\n",
    "            data_collator=data_collator,\n",
    "            optimizers=(optimizer, None),  # Use our optimizer, no scheduler\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the final model\n",
    "        self.model.save_pretrained(os.path.join(self.output_dir, \"final\"))\n",
    "        self.tokenizer.save_pretrained(os.path.join(self.output_dir, \"final\"))\n",
    "\n",
    "        print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iWliG5x-GwPY"
   },
   "outputs": [],
   "source": [
    "class ModelQuantizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path=\"./fine_tuned_model/final\",\n",
    "        base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        output_dir=\"./quantized_model\",\n",
    "    ):\n",
    "        self.model_path = model_path\n",
    "        self.base_model = base_model\n",
    "        self.output_dir = output_dir\n",
    "        self.quantized_model_path = os.path.join(output_dir, \"model.gguf\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_and_merge_model(self):\n",
    "        \"\"\"Load the LoRA model and merge with the base model.\"\"\"\n",
    "        print(\"Loading base model...\")\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        # Load LoRA weights\n",
    "        print(\"Loading and merging LoRA weights...\")\n",
    "        model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "\n",
    "        # Merge LoRA weights with base model\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        # Save merged model and tokenizer\n",
    "        merged_model_path = os.path.join(self.output_dir, \"merged\")\n",
    "        os.makedirs(merged_model_path, exist_ok=True)\n",
    "\n",
    "        print(f\"Saving merged model to {merged_model_path}...\")\n",
    "        model.save_pretrained(merged_model_path)\n",
    "\n",
    "        # Save tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.base_model)\n",
    "        tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "        print(\"Model and tokenizer saved successfully.\")\n",
    "        return merged_model_path\n",
    "\n",
    "    def convert_to_gguf(self, merged_model_path):\n",
    "        \"\"\"Convert the merged model to GGUF format with 4-bit quantization.\"\"\"\n",
    "        print(\"Converting to GGUF format with 4-bit quantization...\")\n",
    "\n",
    "        # Check for existing GGUF model\n",
    "        if os.path.exists(self.quantized_model_path):\n",
    "            print(f\"GGUF model already exists at {self.quantized_model_path}\")\n",
    "            user_input = input(\"Do you want to rebuild it? (y/n): \").lower()\n",
    "            if user_input != \"y\":\n",
    "                print(\"Using existing GGUF model.\")\n",
    "                return self.quantized_model_path\n",
    "\n",
    "        # Clone llama.cpp repository if needed\n",
    "        if not os.path.exists(\"llama.cpp\"):\n",
    "            try:\n",
    "                print(\"Cloning llama.cpp repository...\")\n",
    "                subprocess.run(\n",
    "                    [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\"],\n",
    "                    check=True,\n",
    "                )\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(\"Error cloning llama.cpp repository.\")\n",
    "                raise RuntimeError(\"Failed to clone llama.cpp repository\")\n",
    "\n",
    "        # Build llama.cpp with better error handling\n",
    "        try:\n",
    "            print(\"Building llama.cpp with CMake (this may take a few minutes)...\")\n",
    "            os.makedirs(\"llama.cpp/build\", exist_ok=True)\n",
    "\n",
    "            # Configure with CMake\n",
    "            subprocess.run(\n",
    "                [\"cmake\", \"-S\", \"llama.cpp\", \"-B\", \"llama.cpp/build\"], check=True\n",
    "            )\n",
    "\n",
    "            # Build with CMake\n",
    "            subprocess.run(\n",
    "                [\"cmake\", \"--build\", \"llama.cpp/build\", \"--parallel\"], check=True\n",
    "            )\n",
    "\n",
    "            print(\"llama.cpp built successfully with CMake\")\n",
    "\n",
    "            # Use convert_hf_to_gguf.py with verbose output to see what's happening\n",
    "            convert_script = \"llama.cpp/convert_hf_to_gguf.py\"\n",
    "\n",
    "            if not os.path.exists(convert_script):\n",
    "                print(f\"ERROR: {convert_script} not found!\")\n",
    "                print(\"Please verify your llama.cpp installation.\")\n",
    "                raise RuntimeError(f\"Conversion script not found: {convert_script}\")\n",
    "\n",
    "            print(\"\\nRunning conversion script with enhanced debugging...\")\n",
    "\n",
    "            # Try conversion with detailed error output\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\n",
    "                        \"python3\",\n",
    "                        convert_script,\n",
    "                        merged_model_path,\n",
    "                        \"--outfile\",\n",
    "                        self.quantized_model_path,\n",
    "                        \"--outtype\",\n",
    "                        \"q4_0\",\n",
    "                        \"--verbose\",  # Add verbose output\n",
    "                    ],\n",
    "                    check=True,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                )\n",
    "                print(result.stdout)\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(\"\\n===== Conversion Error Details =====\")\n",
    "                print(f\"Exit code: {e.returncode}\")\n",
    "                print(f\"STDOUT: {e.stdout}\")\n",
    "                print(f\"STDERR: {e.stderr}\")\n",
    "                print(\"===================================\\n\")\n",
    "\n",
    "                print(\n",
    "                    \"Trying alternate conversion approach with arch-specific parameters...\"\n",
    "                )\n",
    "                try:\n",
    "                    # Try with explicit model architecture parameters\n",
    "                    result = subprocess.run(\n",
    "                        [\n",
    "                            \"python3\",\n",
    "                            convert_script,\n",
    "                            merged_model_path,\n",
    "                            \"--outfile\",\n",
    "                            self.quantized_model_path,\n",
    "                            \"--outtype\",\n",
    "                            \"q4_0\",\n",
    "                            \"--model-type\",\n",
    "                            \"llama\",  # Try forcing llama architecture\n",
    "                            \"--ctx\",\n",
    "                            \"4096\",\n",
    "                        ],\n",
    "                        check=True,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                    )\n",
    "                    print(result.stdout)\n",
    "\n",
    "                except subprocess.CalledProcessError as e2:\n",
    "                    print(\"Alternate approach also failed\")\n",
    "                    print(f\"STDOUT: {e2.stdout}\")\n",
    "                    print(f\"STDERR: {e2.stderr}\")\n",
    "                    raise RuntimeError(\"All conversion methods failed\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during build or conversion process: {e}\")\n",
    "            raise RuntimeError(\"Failed to convert model to GGUF format\")\n",
    "\n",
    "        print(\n",
    "            f\"Model successfully converted to GGUF format: {self.quantized_model_path}\"\n",
    "        )\n",
    "\n",
    "        # Copy tokenizer files to output directory\n",
    "        tokenizer_files = [\"tokenizer_config.json\", \"tokenizer.json\"]\n",
    "        for file in tokenizer_files:\n",
    "            src_path = os.path.join(merged_model_path, file)\n",
    "            if os.path.exists(src_path):\n",
    "                dst_path = os.path.join(self.output_dir, file)\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "\n",
    "        return self.quantized_model_path\n",
    "\n",
    "    def quantize(self):\n",
    "        \"\"\"Perform the complete quantization process.\"\"\"\n",
    "        merged_model_path = self.load_and_merge_model()\n",
    "        gguf_path = self.convert_to_gguf(merged_model_path)\n",
    "        return gguf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SosmF2eRtCiI"
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model_path=\"./quantized_model/model.gguf\", data_dir=\"./data\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.inference = ModelInference(model_path=model_path, use_rag=True)\n",
    "        self.inference_no_rag = ModelInference(model_path=model_path, use_rag=False)\n",
    "\n",
    "        # Download necessary NLTK data\n",
    "        try:\n",
    "            nltk.data.find(\"punkt\")\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "\n",
    "        # Initialize ROUGE scorer\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True\n",
    "        )\n",
    "        self.smooth = SmoothingFunction().method1\n",
    "\n",
    "    def load_test_data(self):\n",
    "        \"\"\"Load the test dataset.\"\"\"\n",
    "        test_path = os.path.join(self.data_dir, \"test.json\")\n",
    "        return load_dataset(\"json\", data_files=test_path)[\"train\"]\n",
    "\n",
    "    def calculate_metrics(self, reference, candidate):\n",
    "        \"\"\"Calculate BLEU and ROUGE scores.\"\"\"\n",
    "        # ROUGE scores\n",
    "        rouge_scores = self.rouge_scorer.score(reference, candidate)\n",
    "\n",
    "        # BLEU score\n",
    "        reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "        candidate_tokens = nltk.word_tokenize(candidate.lower())\n",
    "        bleu_score = sentence_bleu(\n",
    "            [reference_tokens], candidate_tokens, smoothing_function=self.smooth\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"bleu\": bleu_score,\n",
    "            \"rouge1\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "            \"rouge2\": rouge_scores[\"rouge2\"].fmeasure,\n",
    "            \"rougeL\": rouge_scores[\"rougeL\"].fmeasure,\n",
    "        }\n",
    "\n",
    "    def evaluate(self, sample_size=None):\n",
    "        \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "        test_data = self.load_test_data()\n",
    "\n",
    "        # Limit evaluation to sample_size if specified\n",
    "        if sample_size is not None:\n",
    "            test_data = test_data.select(range(min(sample_size, len(test_data))))\n",
    "\n",
    "        results_with_rag = []\n",
    "        results_without_rag = []\n",
    "\n",
    "        print(f\"Evaluating on {len(test_data)} test examples...\")\n",
    "\n",
    "        for i, example in enumerate(test_data):\n",
    "            print(f\"Processing example {i + 1}/{len(test_data)}...\")\n",
    "\n",
    "            question = example[\"question\"]\n",
    "            reference_answer = example[\"answer\"]\n",
    "\n",
    "            # Generate answers with and without RAG\n",
    "            answer_with_rag = self.inference.generate_answer(question)\n",
    "            answer_without_rag = self.inference_no_rag.generate_answer(question)\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics_with_rag = self.calculate_metrics(reference_answer, answer_with_rag)\n",
    "            metrics_without_rag = self.calculate_metrics(\n",
    "                reference_answer, answer_without_rag\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            results_with_rag.append(\n",
    "                {\n",
    "                    \"question\": question,\n",
    "                    \"reference\": reference_answer,\n",
    "                    \"prediction\": answer_with_rag,\n",
    "                    **metrics_with_rag,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            results_without_rag.append(\n",
    "                {\n",
    "                    \"question\": question,\n",
    "                    \"reference\": reference_answer,\n",
    "                    \"prediction\": answer_without_rag,\n",
    "                    **metrics_without_rag,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_metrics_with_rag = {\n",
    "            \"bleu\": sum(r[\"bleu\"] for r in results_with_rag) / len(results_with_rag),\n",
    "            \"rouge1\": sum(r[\"rouge1\"] for r in results_with_rag)\n",
    "            / len(results_with_rag),\n",
    "            \"rouge2\": sum(r[\"rouge2\"] for r in results_with_rag)\n",
    "            / len(results_with_rag),\n",
    "            \"rougeL\": sum(r[\"rougeL\"] for r in results_with_rag)\n",
    "            / len(results_with_rag),\n",
    "        }\n",
    "\n",
    "        avg_metrics_without_rag = {\n",
    "            \"bleu\": sum(r[\"bleu\"] for r in results_without_rag)\n",
    "            / len(results_without_rag),\n",
    "            \"rouge1\": sum(r[\"rouge1\"] for r in results_without_rag)\n",
    "            / len(results_without_rag),\n",
    "            \"rouge2\": sum(r[\"rouge2\"] for r in results_without_rag)\n",
    "            / len(results_without_rag),\n",
    "            \"rougeL\": sum(r[\"rougeL\"] for r in results_without_rag)\n",
    "            / len(results_without_rag),\n",
    "        }\n",
    "\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        print(\"\\nWith RAG:\")\n",
    "        for metric, value in avg_metrics_with_rag.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        print(\"\\nWithout RAG:\")\n",
    "        for metric, value in avg_metrics_without_rag.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        return {\n",
    "            \"with_rag\": {\n",
    "                \"detailed_results\": results_with_rag,\n",
    "                \"average_metrics\": avg_metrics_with_rag,\n",
    "            },\n",
    "            \"without_rag\": {\n",
    "                \"detailed_results\": results_without_rag,\n",
    "                \"average_metrics\": avg_metrics_without_rag,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "b64r9EuDG0Cc"
   },
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name=\"BAAI/bge-small-en-v1.5\"):\n",
    "        self.model_name = model_name\n",
    "        # Use CPU for embeddings to save GPU memory\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        # Get embedding dimension from the model\n",
    "        self.embedding_dim = self.model.config.hidden_size\n",
    "\n",
    "    def get_embedding_dim(self):\n",
    "        \"\"\"Return the embedding dimension of the model.\"\"\"\n",
    "        return self.embedding_dim\n",
    "\n",
    "    def get_embeddings(self, texts: List[str], batch_size=16) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i : i + batch_size]\n",
    "\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_dim=768):\n",
    "        self.index = faiss.IndexFlatL2(\n",
    "            embedding_dim\n",
    "        )  # L2 distance for similarity search\n",
    "        self.texts = []\n",
    "\n",
    "    def add_texts(self, texts: List[str], embeddings: np.ndarray):\n",
    "        \"\"\"Add texts and their embeddings to the vector store.\"\"\"\n",
    "        # Add embeddings to index\n",
    "        self.index.add(embeddings)\n",
    "        # Store original texts\n",
    "        self.texts.extend(texts)\n",
    "\n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for most similar texts given a query embedding.\"\"\"\n",
    "        # Reshape query embedding\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "        # Search in the index\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "\n",
    "        # Build results\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.texts) and idx >= 0:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"text\": self.texts[idx],\n",
    "                        \"score\": float(distances[0][i]),\n",
    "                        \"id\": int(idx),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, data_dir=\"./data\"):\n",
    "        self.embedding_model = EmbeddingModel()\n",
    "        # Use the actual embedding dimension from the model\n",
    "        self.vector_store = VectorStore(\n",
    "            embedding_dim=self.embedding_model.get_embedding_dim()\n",
    "        )\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def build_index(self, force_rebuild=False):\n",
    "        \"\"\"Build the vector index from the dataset chunks.\"\"\"\n",
    "        index_file = os.path.join(self.data_dir, \"vector_index.faiss\")\n",
    "        texts_file = os.path.join(self.data_dir, \"vector_texts.npy\")\n",
    "\n",
    "        # Load from disk if exists and not forced to rebuild\n",
    "        if (\n",
    "            os.path.exists(index_file)\n",
    "            and os.path.exists(texts_file)\n",
    "            and not force_rebuild\n",
    "        ):\n",
    "            self.vector_store.index = faiss.read_index(index_file)\n",
    "            self.vector_store.texts = np.load(texts_file, allow_pickle=True).tolist()\n",
    "            print(\n",
    "                f\"Loaded existing index with {len(self.vector_store.texts)} documents.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Load datasets\n",
    "        print(\"Building vector index...\")\n",
    "\n",
    "        # Load train, validation, test datasets\n",
    "        train_path = os.path.join(self.data_dir, \"train.json\")\n",
    "        validation_path = os.path.join(self.data_dir, \"validation.json\")\n",
    "        test_path = os.path.join(self.data_dir, \"test.json\")\n",
    "\n",
    "        train_data = load_dataset(\"json\", data_files=train_path)[\"train\"]\n",
    "        validation_data = load_dataset(\"json\", data_files=validation_path)[\"train\"]\n",
    "        test_data = load_dataset(\"json\", data_files=test_path)[\"train\"]\n",
    "\n",
    "        # Combine all contexts\n",
    "        all_contexts = []\n",
    "        seen_contexts = set()\n",
    "\n",
    "        # Helper to add unique contexts\n",
    "        def add_unique_contexts(dataset):\n",
    "            for item in dataset:\n",
    "                context = item[\"context\"]\n",
    "                if context not in seen_contexts:\n",
    "                    all_contexts.append(context)\n",
    "                    seen_contexts.add(context)\n",
    "\n",
    "        add_unique_contexts(train_data)\n",
    "        add_unique_contexts(validation_data)\n",
    "        add_unique_contexts(test_data)\n",
    "\n",
    "        print(f\"Found {len(all_contexts)} unique contexts.\")\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.get_embeddings(all_contexts)\n",
    "\n",
    "        # Add to vector store\n",
    "        self.vector_store.add_texts(all_contexts, embeddings)\n",
    "\n",
    "        # Save to disk\n",
    "        faiss.write_index(self.vector_store.index, index_file)\n",
    "        np.save(texts_file, np.array(self.vector_store.texts, dtype=object))\n",
    "\n",
    "        print(f\"Built and saved index with {len(all_contexts)} documents.\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"Retrieve relevant contexts for a query.\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_model.get_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        results = self.vector_store.search(query_embedding, k=k)\n",
    "\n",
    "        # Return contexts\n",
    "        return [item[\"text\"] for item in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bc070G4AOhdw"
   },
   "outputs": [],
   "source": [
    "class ModelInference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str = \"./quantized_model/model.gguf\",\n",
    "        use_rag: bool = True,\n",
    "        context_length: int = 4096,\n",
    "        num_retrieved_docs: int = 3,\n",
    "    ):\n",
    "        self.model_path = model_path\n",
    "        self.use_rag = use_rag\n",
    "        self.num_retrieved_docs = num_retrieved_docs\n",
    "\n",
    "        # Initialize Llama model\n",
    "        self.llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=context_length,\n",
    "            n_batch=512,\n",
    "            n_gpu_layers=-1,  # Use all layers on GPU if available\n",
    "        )\n",
    "\n",
    "        # Initialize RAG system if needed\n",
    "        if use_rag:\n",
    "            self.rag = RAGSystem()\n",
    "            self.rag.build_index()\n",
    "\n",
    "    def retrieve_context(self, query: str) -> str:\n",
    "        \"\"\"Retrieve relevant context using RAG with token count limiting.\"\"\"\n",
    "        if not self.use_rag:\n",
    "            return \"\"\n",
    "\n",
    "        contexts = self.rag.retrieve(query, k=self.num_retrieved_docs)\n",
    "\n",
    "        # Calculate token budgets\n",
    "        system_prompt = \"You are an AI assistant that specializes in answering questions about AI research papers.\"\n",
    "        query_prompt = f\"Question: {query}\"\n",
    "        combined_prompt = system_prompt + query_prompt\n",
    "\n",
    "        # Fix: Use the more reliable approach with llama_cpp\n",
    "        # Reserve tokens for the system prompt, query, and generated response\n",
    "        try:\n",
    "            # Use the proper encoding with llama_cpp\n",
    "            reserved_tokens = (\n",
    "                len(self.llm.tokenize(bytes(combined_prompt, \"utf-8\"))) + 1024\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback method if bytes conversion doesn't work\n",
    "            reserved_tokens = len(combined_prompt.split()) * 2 + 1024  # Approximate\n",
    "\n",
    "        max_context_tokens = self.llm.n_ctx() - reserved_tokens\n",
    "\n",
    "        # Start with all contexts and trim as needed\n",
    "        selected_contexts = []\n",
    "        current_tokens = 0\n",
    "\n",
    "        for context in contexts:\n",
    "            try:\n",
    "                context_tokens = len(self.llm.tokenize(bytes(context, \"utf-8\")))\n",
    "            except TypeError:\n",
    "                # Fallback approximation\n",
    "                context_tokens = len(context.split()) * 2\n",
    "\n",
    "            if current_tokens + context_tokens <= max_context_tokens:\n",
    "                selected_contexts.append(context)\n",
    "                current_tokens += context_tokens\n",
    "            else:\n",
    "                # Try to add a truncated version if it's the first context\n",
    "                if len(selected_contexts) == 0:\n",
    "                    # Estimate truncation point (rough approximation)\n",
    "                    max_chars = int(max_context_tokens / context_tokens * len(context))\n",
    "                    truncated = context[:max_chars]\n",
    "                    selected_contexts.append(truncated)\n",
    "                break\n",
    "\n",
    "        return \"\\n\\n\".join(selected_contexts)\n",
    "\n",
    "    def format_prompt(self, query: str, context: Optional[str] = None) -> str:\n",
    "        \"\"\"Format the prompt for the model.\"\"\"\n",
    "        system_message = \"You are an AI assistant that specializes in answering questions about AI research papers. Provide comprehensive, accurate responses based on the information available to you.\"\n",
    "\n",
    "        if context:\n",
    "            prompt = f\"\"\"### System:\n",
    "{system_message}\n",
    "\n",
    "### Human:\n",
    "I have a question about an AI research paper.\n",
    "\n",
    "Here is some relevant context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "        else:\n",
    "            prompt = f\"\"\"### System:\n",
    "{system_message}\n",
    "\n",
    "### Human:\n",
    "Question about AI research: {query}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def generate_answer(self, query: str) -> str:\n",
    "        \"\"\"Generate an answer for a query.\"\"\"\n",
    "        # Retrieve context if using RAG\n",
    "        context = self.retrieve_context(query) if self.use_rag else None\n",
    "\n",
    "        # Format prompt\n",
    "        prompt = self.format_prompt(query, context)\n",
    "\n",
    "        # Generate response\n",
    "        start_time = time.time()\n",
    "        response = self.llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            stop=[\"### Human:\", \"### System:\"],\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Extract answer text\n",
    "        answer = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "        # Log performance\n",
    "        print(f\"Generation time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "u48XEcKdG4z7"
   },
   "outputs": [],
   "source": [
    "class ModelQuantizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path=\"./fine_tuned_model/final\",\n",
    "        base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        output_dir=\"./quantized_model\",\n",
    "    ):\n",
    "        self.model_path = model_path\n",
    "        self.base_model = base_model\n",
    "        self.output_dir = output_dir\n",
    "        self.quantized_model_path = os.path.join(output_dir, \"model.gguf\")\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def load_and_merge_model(self):\n",
    "        \"\"\"Load the LoRA model and merge with the base model.\"\"\"\n",
    "        print(\"Loading base model...\")\n",
    "\n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        # Load LoRA weights\n",
    "        print(\"Loading and merging LoRA weights...\")\n",
    "        model = PeftModel.from_pretrained(base_model, self.model_path)\n",
    "\n",
    "        # Merge LoRA weights with base model\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        # Save merged model and tokenizer\n",
    "        merged_model_path = os.path.join(self.output_dir, \"merged\")\n",
    "        os.makedirs(merged_model_path, exist_ok=True)\n",
    "\n",
    "        print(f\"Saving merged model to {merged_model_path}...\")\n",
    "        model.save_pretrained(merged_model_path)\n",
    "\n",
    "        # Save tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.base_model)\n",
    "        tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "        print(\"Model and tokenizer saved successfully.\")\n",
    "        return merged_model_path\n",
    "\n",
    "    def convert_to_gguf(self, merged_model_path):\n",
    "        \"\"\"Convert the merged model to GGUF format with quantization.\"\"\"\n",
    "        print(\"Converting to GGUF format with quantization...\")\n",
    "\n",
    "        # Check for existing GGUF model\n",
    "        if os.path.exists(self.quantized_model_path):\n",
    "            print(f\"GGUF model already exists at {self.quantized_model_path}\")\n",
    "            user_input = input(\"Do you want to rebuild it? (y/n): \").lower()\n",
    "            if user_input != \"y\":\n",
    "                print(\"Using existing GGUF model.\")\n",
    "                return self.quantized_model_path\n",
    "\n",
    "        # Clone llama.cpp repository if needed\n",
    "        if not os.path.exists(\"llama.cpp\"):\n",
    "            try:\n",
    "                print(\"Cloning llama.cpp repository...\")\n",
    "                subprocess.run(\n",
    "                    [\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\"],\n",
    "                    check=True,\n",
    "                )\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(\"Error cloning llama.cpp repository.\")\n",
    "                raise RuntimeError(\"Failed to clone llama.cpp repository\")\n",
    "\n",
    "        # Build llama.cpp with better error handling\n",
    "        try:\n",
    "            print(\"Building llama.cpp with CMake (this may take a few minutes)...\")\n",
    "            os.makedirs(\"llama.cpp/build\", exist_ok=True)\n",
    "\n",
    "            # Configure with CMake\n",
    "            subprocess.run(\n",
    "                [\"cmake\", \"-S\", \"llama.cpp\", \"-B\", \"llama.cpp/build\"], check=True\n",
    "            )\n",
    "\n",
    "            # Build with CMake\n",
    "            subprocess.run(\n",
    "                [\"cmake\", \"--build\", \"llama.cpp/build\", \"--parallel\"], check=True\n",
    "            )\n",
    "\n",
    "            print(\"llama.cpp built successfully with CMake\")\n",
    "\n",
    "            # Use convert_hf_to_gguf.py with verbose output to see what's happening\n",
    "            convert_script = \"llama.cpp/convert_hf_to_gguf.py\"\n",
    "\n",
    "            if not os.path.exists(convert_script):\n",
    "                print(f\"ERROR: {convert_script} not found!\")\n",
    "                print(\"Please verify your llama.cpp installation.\")\n",
    "                raise RuntimeError(f\"Conversion script not found: {convert_script}\")\n",
    "\n",
    "            print(\"\\nRunning conversion script with enhanced debugging...\")\n",
    "\n",
    "            # Try conversion with detailed error output - using q8_0 instead of q4_0\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\n",
    "                        \"python3\",\n",
    "                        convert_script,\n",
    "                        merged_model_path,\n",
    "                        \"--outfile\",\n",
    "                        self.quantized_model_path,\n",
    "                        \"--outtype\",\n",
    "                        \"q8_0\",  # Changed from q4_0 to q8_0\n",
    "                        \"--verbose\",\n",
    "                    ],\n",
    "                    check=True,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                )\n",
    "                print(result.stdout)\n",
    "\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(\"\\n===== Conversion Error Details =====\")\n",
    "                print(f\"Exit code: {e.returncode}\")\n",
    "                print(f\"STDOUT: {e.stdout}\")\n",
    "                print(f\"STDERR: {e.stderr}\")\n",
    "                print(\"===================================\\n\")\n",
    "\n",
    "                print(\n",
    "                    \"Trying alternate conversion approach with arch-specific parameters...\"\n",
    "                )\n",
    "                try:\n",
    "                    # Try with explicit model architecture parameters - using q8_0\n",
    "                    result = subprocess.run(\n",
    "                        [\n",
    "                            \"python3\",\n",
    "                            convert_script,\n",
    "                            merged_model_path,\n",
    "                            \"--outfile\",\n",
    "                            self.quantized_model_path,\n",
    "                            \"--outtype\",\n",
    "                            \"q8_0\",  # Changed from q4_0 to q8_0\n",
    "                            \"--model-name\",\n",
    "                            \"Qwen\",  # Added model name hint\n",
    "                        ],\n",
    "                        check=True,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                    )\n",
    "                    print(result.stdout)\n",
    "\n",
    "                except subprocess.CalledProcessError as e2:\n",
    "                    print(\"Alternate approach also failed\")\n",
    "                    print(f\"STDOUT: {e2.stdout}\")\n",
    "                    print(f\"STDERR: {e2.stderr}\")\n",
    "                    raise RuntimeError(\"All conversion methods failed\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during build or conversion process: {e}\")\n",
    "            raise RuntimeError(\"Failed to convert model to GGUF format\")\n",
    "\n",
    "        print(\n",
    "            f\"Model successfully converted to GGUF format: {self.quantized_model_path}\"\n",
    "        )\n",
    "\n",
    "        # Copy tokenizer files to output directory\n",
    "        tokenizer_files = [\"tokenizer_config.json\", \"tokenizer.json\"]\n",
    "        for file in tokenizer_files:\n",
    "            src_path = os.path.join(merged_model_path, file)\n",
    "            if os.path.exists(src_path):\n",
    "                dst_path = os.path.join(self.output_dir, file)\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "\n",
    "        return self.quantized_model_path\n",
    "\n",
    "    def quantize(self):\n",
    "        \"\"\"Perform the complete quantization process.\"\"\"\n",
    "        merged_model_path = self.load_and_merge_model()\n",
    "        gguf_path = self.convert_to_gguf(merged_model_path)\n",
    "        return gguf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3151f5730ccc4962a7baed78b56b1971",
      "fb86c293f0924187b05577bafcecdcee",
      "0ab8fdad5e184610a74fea3da0e36091",
      "83951e88e9734934b9ba8b906d42335c",
      "e7bc49081c394355ad44d83c567f6c39",
      "2bd06e530e1143e18c59fc12201dc957",
      "803d7d52699446d3a461937e1f012813",
      "8be3848c81d345a6a32f269244141b3a",
      "aeb22d02a2d44b0b88d5c435e5e6554e",
      "2714d10155324ce6a4c14503c7d79498",
      "df4c9a87f85c457dbf4727eb8c10c7c9",
      "71d5278ed1d1402f9e83870082734449",
      "aa0af8b1ed83453ab715a8301487b6e8",
      "fdc737fb2ae842afadb26fe9926d70c7",
      "c2b701181f3d4fb4a723c6bc34f720c2",
      "5117d6b06ff7498384f92c17263fd8c4",
      "06cdc025197d4e7081cccc7b0f393633",
      "33e1803ff2284dbc9040cfec3ad93592",
      "723b7083097f49179884786c4c145034",
      "3a4f6956c7244057bea194a4da13ed3d",
      "eea5e6322c33402eb75f6d0bc16173b9",
      "5f1caff87551428081c54ec44df1318e",
      "ab564f2a1c734326a03159c4c1b56791",
      "6d23e73833864b1f9eae4a788092f9f7",
      "4688175e18224050bb9bba25cfab583f",
      "cb01e4b0fbde4c6f94f7c78ab15f8f85",
      "0bf312de9de846a9b6fcbec8c133b0ea",
      "46c319439436435dbb0fee9a7516a4bd",
      "fd9a0bbef8254a74b9b39213147bf049",
      "4da832270a4146128240151b58955d64",
      "251c4924d2cf492cbbe495f73fcb6720",
      "67e3c4f773fb433caa3d5679920846b1",
      "6de8c451ca8e4fba9d74773bac463782",
      "6c36d99df80b4aec8f0982d519fb068c",
      "9e537c90972d4ef399d5c5b78d466d6e",
      "c70be734e55346cfb9496d0df49b1c6f",
      "7af43cdaf3604d59a81a2764f5053528",
      "27aeeb20d8d34e4d9613308ee9b22f88",
      "f1d89fc291c24e728da03d17a27ab785",
      "ddc800b4d20d4d03ae59a7b5c20b4680",
      "c1455b2e7b0f47dbae672d40ad3cb699",
      "0a695b2f502e4a7d8878c75aa36da831",
      "3f579bb984cf4953957ed1f6ca311441",
      "9ba3af643ec745e989d821ad4ad89910",
      "734ab7dd46db4959aa3769a7ab4dd4c0",
      "cbc8069c7bab49e99731fba704915906",
      "f52def45289341ffa82930300e341c0c",
      "baaf84171f714d23930dc50efa3bd835",
      "f2a01c85e40144119f2624b89502ce51",
      "78c0a13b68db47628ac93e9527bd3356",
      "d81db81fac854bc4bc17c9de580c7f74",
      "988c99222264457cab97ef8a9ca689cb",
      "0f80d25024c94e0cba2361976d6e429a",
      "d1ec16bd07de4008aff1156b5426ed75",
      "9bc414b798d549c4a21aced99dfb562c",
      "11c4faaaf87f4e95978d7736df70b98b",
      "69c58c3cd33f4d1ab65cdfc18a642a9e",
      "13d3874ed389447cb25aaf29f9af6dc0",
      "c7d835034e8c45c18e25e60a5e559d89",
      "0ae0d8efcc3841119661313f3164df8f",
      "3e34688137754dc890dff558cced54ba",
      "1d5e6247d84045b4942510644c8d7926",
      "da35c74cce134c9982d451ce94eaaf33",
      "7b9d963050a44babbf420c02697ed11d",
      "0095897d2219422a8ee49912cb2840a5",
      "d8a901f571b64416ab8479384d8a7028",
      "dda01da9cff141ee92cdd8186ebd48dc",
      "ea954d402c854417aa5cec3881c461d2",
      "b2d0e0bd72d04d9f938dfd832dce957b",
      "da30c7f6f52a48f5b9b1e49886241c9a",
      "e0502f1c70b24511adf779ca5c649ddf",
      "0cfe4d466ed446cd8b9860ffc2a3ffea",
      "9b6b8eb049a24d289acd2588e9f9fb09",
      "0c7abf7953044ee4a9a46b48c5115457",
      "5690e187cc884e1db8555f17d9e26ce2",
      "7dda3d27453e4a7896714b8369282b15",
      "d52305c00196472ebe8f72d5038d4329",
      "75b183a37d3b4bea8d02400d96d674a1",
      "be079351f7824516aeafdce08a3387fc",
      "1d2446a66ce44341bcf9d0e4d084152d",
      "284cdc77413c406f97395bdf4d57c6c0",
      "14f517d95ad34381a154f99791434629",
      "414ff5b4353646c3bf1d78bcb31639df",
      "5c0db21b600c4d17ac613dc83e44e4ed",
      "57a8997217384432a55489e28ef6cb2e",
      "65c7245f4bde4bca92903df02c448012",
      "a3fc7bd0dcef4dcfaf2c4be86e5d0b6d",
      "4ebca178bc4d45c289af2c3ec11304c6",
      "d7519f78d7e14a19ae273010ce2e9b3a",
      "4f4154b01bc940d6b6195c90766febe4",
      "f062efc9248c484e9de0956fea123753",
      "b362acc5bdc44f5fabffe15e1beea57c",
      "f3924e36572b46bbba4dfee0ba081112",
      "27baea7e2317407e83610df61ef847c0",
      "8f3399d422fe4ecd81c1868c2a8e2266",
      "27cfe6baa6ad46fba93f7b76d73e1a7e",
      "9da7fe111dfd42f1a499f77b5da3fa3d",
      "348055a6fd0749c382c8b08d51f61d1d",
      "184c18cf770e48969983a0fc0088c855",
      "66a77e1f57234990b22a0dd23a31d48b",
      "e4eb2ce65c5a4922969c617bd9bb413a",
      "ddd13c80003a4950837b1ff6c937be5c",
      "7042c4ceb2ad4c34a089d39bcd3a7c24",
      "0e528df8602b439f9553f4de99b3de97",
      "9c041d59aea5443688bb4cf0fcba4937",
      "275250ee3d2e4a93b040926ae8e3bb49",
      "ca9fbe3fea5a4d9ba70d0f69e35773ae",
      "bf1885a5bbbb44688079028132f3cc06",
      "09974d45d8a34b3191723395acd933a3",
      "b9a2c3e22e4a499997399d58fe46f488",
      "d1b7a3db9d984690a67d63ee4eed2740",
      "b0216412c402433e9f31cb15c5208c9e",
      "270bae39974a4b93be70b8e2f7909915",
      "2dba3c40dd9a47648ab0f9211fc61e7b",
      "370189690d554782bbd97cad0df9d1b1",
      "8b6ecfbcea944f0d933caf15a5f13875",
      "7484d0b763cf4204bb385d6befc33ff2",
      "18b8497d2fc6462ba6839c1d546f2333",
      "3c32ead0499d43f8ac873986c50d93b7",
      "ca29a12766b84416a647e00ab81e3c83",
      "35ccdeb4e91a4ad08a2cf622e3744840"
     ]
    },
    "id": "E_dg7i4IG6as",
    "outputId": "23bf9304-92c2-4fd1-e61a-667ee957690f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic dataset...\n",
      "Loaded 5 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3151f5730ccc4962a7baed78b56b1971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d5278ed1d1402f9e83870082734449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab564f2a1c734326a03159c4c1b56791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c36d99df80b4aec8f0982d519fb068c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 15 training, 6 validation, and 6 test examples.\n",
      "Fine-tuning Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734ab7dd46db4959aa3769a7ab4dd4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c4faaaf87f4e95978d7736df70b98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 training examples and 6 validation examples.\n",
      "Using bitsandbytes version: 0.45.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda01da9cff141ee92cdd8186ebd48dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model with 4-bit quantization and LoRA adapters\n",
      "trainable params: 59,867,136 || all params: 3,145,805,824 || trainable%: 1.9031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b183a37d3b4bea8d02400d96d674a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7519f78d7e14a19ae273010ce2e9b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 15\n",
      "}), Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 6\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mranuga-d\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250309_075101-ee4jyepw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa/runs/ee4jyepw' target=\"_blank\">qwen-2.5-3b-qlora</a></strong> to <a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa' target=\"_blank\">https://wandb.ai/ranuga-d/qwen-ai-research-qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ranuga-d/qwen-ai-research-qa/runs/ee4jyepw' target=\"_blank\">https://wandb.ai/ranuga-d/qwen-ai-research-qa/runs/ee4jyepw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-09 07:51:02,295] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Quantizing the model...\n",
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a77e1f57234990b22a0dd23a31d48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging LoRA weights...\n",
      "Saving merged model to ./quantized_model/merged...\n",
      "Model and tokenizer saved successfully.\n",
      "Converting to GGUF format with quantization...\n",
      "GGUF model already exists at ./quantized_model/model.gguf\n",
      "Do you want to rebuild it? (y/n): y\n",
      "Building llama.cpp with CMake (this may take a few minutes)...\n",
      "llama.cpp built successfully with CMake\n",
      "\n",
      "Running conversion script with enhanced debugging...\n",
      "\n",
      "Model successfully converted to GGUF format: ./quantized_model/model.gguf\n",
      "Building RAG index...\n",
      "Building vector index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b7a3db9d984690a67d63ee4eed2740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 27 key-value pairs and 434 tensors from ./quantized_model/model.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q8_0:  253 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 3.05 GiB (8.50 BPW) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built and saved index with 9 documents.\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'Ã„Ä¬'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  3127.59 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
      "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
      "llama_init_from_model: graph nodes  = 1266\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '2048', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 3B Instruct', 'qwen2.block_count': '36', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 434 tensors from ./quantized_model/model.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q8_0:  253 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 3.05 GiB (8.50 BPW) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing index with 9 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'Ã„Ä¬'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  3127.59 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
      "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
      "llama_init_from_model: graph nodes  = 1266\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '2048', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 3B Instruct', 'qwen2.block_count': '36', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 6 test examples...\n",
      "Processing example 1/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61333.42 ms\n",
      "llama_perf_context_print: prompt eval time =   61332.47 ms /  2677 tokens (   22.91 ms per token,    43.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17735.66 ms /   178 runs   (   99.64 ms per token,    10.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   79371.80 ms /  2855 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 79.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     827.35 ms\n",
      "llama_perf_context_print: prompt eval time =     827.16 ms /    47 tokens (   17.60 ms per token,    56.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3377.19 ms /    43 runs   (   78.54 ms per token,    12.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    4270.60 ms /    90 tokens\n",
      "Llama.generate: 2676 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 4.28 seconds\n",
      "Processing example 2/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61333.42 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   20539.93 ms /   205 runs   (  100.19 ms per token,     9.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   20896.29 ms /   206 tokens\n",
      "Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 20.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     827.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2956.41 ms /    37 runs   (   79.90 ms per token,    12.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    3012.81 ms /    38 tokens\n",
      "Llama.generate: 2676 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 3.02 seconds\n",
      "Processing example 3/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61333.42 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   39643.82 ms /   396 runs   (  100.11 ms per token,     9.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   40399.83 ms /   397 tokens\n",
      "Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 40.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     827.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3773.80 ms /    47 runs   (   80.29 ms per token,    12.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    3846.82 ms /    48 tokens\n",
      "Llama.generate: 2672 prefix-match hit, remaining 5 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 3.85 seconds\n",
      "Processing example 4/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61333.42 ms\n",
      "llama_perf_context_print: prompt eval time =     186.60 ms /     5 tokens (   37.32 ms per token,    26.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12027.23 ms /   120 runs   (  100.23 ms per token,     9.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   12416.56 ms /   125 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 5 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 12.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     827.35 ms\n",
      "llama_perf_context_print: prompt eval time =     412.93 ms /     5 tokens (   82.59 ms per token,    12.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4896.62 ms /    62 runs   (   78.98 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    5405.69 ms /    67 tokens\n",
      "Llama.generate: 50 prefix-match hit, remaining 1916 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 5.41 seconds\n",
      "Processing example 5/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61333.42 ms\n",
      "llama_perf_context_print: prompt eval time =   42641.20 ms /  1916 tokens (   22.26 ms per token,    44.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38200.33 ms /   400 runs   (   95.50 ms per token,    10.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   81608.41 ms /  2316 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 5 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 81.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     827.35 ms\n",
      "llama_perf_context_print: prompt eval time =     692.47 ms /     5 tokens (  138.49 ms per token,     7.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4726.69 ms /    57 runs   (   82.92 ms per token,    12.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    5508.80 ms /    62 tokens\n",
      "Llama.generate: 1965 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 5.51 seconds\n",
      "Processing example 6/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61333.42 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   48315.80 ms /   502 runs   (   96.25 ms per token,    10.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   49332.63 ms /   503 tokens\n",
      "Llama.generate: 46 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 49.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     827.35 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4636.32 ms /    58 runs   (   79.94 ms per token,    12.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    4725.47 ms /    59 tokens\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 434 tensors from ./quantized_model/model.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3B Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ä  Ä \", \"Ä Ä  Ä Ä \", \"i n\", \"Ä  t\",...\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q8_0:  253 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q8_0\n",
      "print_info: file size   = 3.05 GiB (8.50 BPW) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 4.73 seconds\n",
      "\n",
      "Evaluation Results:\n",
      "\n",
      "With RAG:\n",
      "bleu: 0.0009\n",
      "rouge1: 0.0209\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0177\n",
      "\n",
      "Without RAG:\n",
      "bleu: 0.0018\n",
      "rouge1: 0.0210\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0210\n",
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 148848 'Ã„Ä¬'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q8_0) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  3127.59 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
      "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
      "llama_init_from_model: graph nodes  = 1266\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '7', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '2048', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 3B Instruct', 'qwen2.block_count': '36', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing index with 9 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   62722.55 ms\n",
      "llama_perf_context_print: prompt eval time =   62721.94 ms /  2742 tokens (   22.87 ms per token,    43.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   95608.26 ms /   933 runs   (  102.47 ms per token,     9.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  160595.86 ms /  3675 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 160.61 seconds\n",
      "\n",
      "Query: What is the latest research in AI?\n",
      "\n",
      "Answer:\n",
      "The context provided does not directly address the latest research in AI. However, I can provide a summary of the key advancements mentioned in the Deepseek V3 model and the accompanying research papers:\n",
      "\n",
      "### Key Advancements in Deepseek V3:\n",
      "\n",
      "1. **Mixture-of-Experts (MoE) Architecture**:\n",
      "   - Employed a MoE architecture where only 37 billion parameters fire for each token out of the total 671 billion.\n",
      "   - This sparse activation significantly reduces compute requirements compared to dense models.\n",
      "\n",
      "2. **FP8 Mixed Precision Training**:\n",
      "   - Implemented an FP8 mixed precision training framework.\n",
      "   - Reduced memory usage and accelerated training compared to higher precision formats.\n",
      "   - Achieved up to 50% reduction in memory footprint compared to traditional FP16/FP32 formats.\n",
      "   - Used fine-grained quantization strategies and increased accumulation precision to maintain accuracy.\n",
      "\n",
      "3. **Load Balancing Strategy**:\n",
      "   - Developed an auxiliary loss-free strategy for load balancing in the MoE architecture.\n",
      "   - Improved performance without the drawbacks of traditional auxiliary loss methods.\n",
      "\n",
      "4. **Custom Training Framework (HAI-LLM)**:\n",
      "   - Developed a custom training framework called HAI-LLM.\n",
      "   - Included several optimizations such as DualPipe algorithm for efficient pipeline parallelism.\n",
      "   - Efficient cross-node all-to-all communication kernels to fully utilize network bandwidth.\n",
      "   - Careful memory optimizations to avoid using costly tensor parallelism.\n",
      "\n",
      "### Latest Research in AI:\n",
      "\n",
      "The research described in Deepseek V3 represents significant advances in AI, particularly in the areas of architecture, training efficiency, and optimization strategies. Here are some key recent advancements in AI research:\n",
      "\n",
      "1. **Next-Generation Architectures**:\n",
      "   - **AI-Next**: Research on next-generation AI architectures, including improved attention mechanisms, transformers, and neural network models.\n",
      "   - **Neural Architecture Search (NAS)**: AI methods for automatically searching and optimizing neural network architectures.\n",
      "\n",
      "2. **Training Efficiency**:\n",
      "   - **Gradient Compression**: Techniques for reducing communication and storage costs during model training.\n",
      "   - **Data-efficient Learning**: Methods for training models with limited data, including transfer learning and few-shot learning.\n",
      "   - **Meta-Learning**: Approaches that allow models to adapt quickly to new tasks without retraining.\n",
      "\n",
      "3. **Hardware-Accelerated Training**:\n",
      "   - **HPC and GPU Acceleration**: Advances in training models on high-performance computing clusters and specialized GPU hardware.\n",
      "   - **TPU and AI Chips**: Development of specialized chips and accelerators optimized for AI tasks.\n",
      "\n",
      "4. **Mixed-Precision Training**:\n",
      "   - **Mixed-Precision Training (MPT)**: Techniques to use lower-precision arithmetic during training to reduce memory and improve speed.\n",
      "   - **Mixed-Precision Transfer Learning**: Methods for transferring knowledge between models trained on different precision levels.\n",
      "\n",
      "5. **Efficient Communication**:\n",
      "   - **All-to-All Communication**: Optimizations for efficient communication patterns in distributed training.\n",
      "   - **Efficient Data Placement**: Techniques for minimizing data movement during training.\n",
      "\n",
      "6. **Load Balancing**:\n",
      "   - **Auxiliary Loss-Free Methods**: Techniques for improving load balancing in distributed systems without the drawbacks of traditional auxiliary loss methods.\n",
      "   - **Cross-Node Communication**: Efficient communication strategies across multiple nodes.\n",
      "\n",
      "7. **Custom Training Frameworks**:\n",
      "   - **Custom Heterogeneous Frameworks**: Development of custom frameworks tailored to specific hardware and task requirements.\n",
      "   - **Just-In-Time (JIT) Compilation**: Techniques for optimizing and compiling code just-in-time for better performance.\n",
      "\n",
      "8. **Parallelism Strategies**:\n",
      "   - **Pipeline Parallelism**: Techniques for overlapping computation and communication to improve throughput.\n",
      "   - **Tensor Parallelism**: Methods for parallelizing across large tensors without synchronization overhead.\n",
      "\n",
      "9. **Memory Optimization**:\n",
      "   - **Fine-Grained Quantization**: Techniques to reduce memory usage by quantizing model parameters to lower bit-widths.\n",
      "   - **Memory-Aware Training**: Methods that optimize training for specific memory constraints.\n",
      "\n",
      "10. **Scalability and Efficiency**:\n",
      "    - **Scalable Training**: Techniques for scaling training to larger and more powerful hardware.\n",
      "    - **Efficient Inference**: Methods for optimizing inference performance and memory usage.\n",
      "\n",
      "These advancements collectively represent significant strides in AI research, contributing to faster, more efficient, and scalable AI models and systems. The specific techniques mentioned in the Deepseek V3 model, such as MoE architecture, mixed precision training, and custom training frameworks, are examples of these broader trends in AI research.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Generate synthetic dataset with default directories\n",
    "    print(\"Generating synthetic dataset...\")\n",
    "    create_synthetic_data(documents_dir=\"./dataset/q3_dataset\", output_dir=\"./data\")\n",
    "\n",
    "    # Fine-tune the model using default parameters\n",
    "    print(\"Fine-tuning Qwen/Qwen2.5-3B-Instruct...\")\n",
    "    fine_tuner = QAFineTuner(\"Qwen/Qwen2.5-3B-Instruct\", \"./data\", \"./fine_tuned_model\")\n",
    "    fine_tuner.load_data()\n",
    "    fine_tuner.prepare_model()\n",
    "    fine_tuner.prepare_datasets()\n",
    "    fine_tuner.train()\n",
    "\n",
    "    # Quantize the model to GGUF format with default settings\n",
    "    print(\"Quantizing the model...\")\n",
    "    quantizer = ModelQuantizer(\n",
    "        model_path=\"./fine_tuned_model/final\",\n",
    "        base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        output_dir=\"./quantized_model\",\n",
    "    )\n",
    "    quantizer.quantize()\n",
    "\n",
    "    # Build the RAG index with default directory\n",
    "    print(\"Building RAG index...\")\n",
    "    rag = RAGSystem(data_dir=\"./data\")\n",
    "    rag.build_index(force_rebuild=True)\n",
    "\n",
    "    # Evaluate the model using default settings (using all available samples)\n",
    "    print(\"Evaluating the model...\")\n",
    "    evaluator = Evaluator(\n",
    "        model_path=os.path.join(\"./quantized_model\", \"model.gguf\"), data_dir=\"./data\"\n",
    "    )\n",
    "    evaluator.evaluate(sample_size=None)\n",
    "\n",
    "    # Run inference using default parameters and a sample query\n",
    "    print(\"Running inference...\")\n",
    "    inference = ModelInference(\n",
    "        model_path=os.path.join(\"./quantized_model\", \"model.gguf\"), use_rag=True\n",
    "    )\n",
    "    default_query = \"What is the latest research in AI?\"\n",
    "    answer = inference.generate_answer(default_query)\n",
    "    print(f\"\\nQuery: {default_query}\\n\")\n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uefbwdZwG7FS",
    "outputId": "1e062ec9-19f5-4057-ea72-1fd154e2e839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/ (stored 0%)\n",
      "  adding: content/.config/ (stored 0%)\n",
      "  adding: content/.config/default_configs.db (deflated 98%)\n",
      "  adding: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
      "  adding: content/.config/active_config (stored 0%)\n",
      "  adding: content/.config/.last_update_check.json (deflated 22%)\n",
      "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
      "  adding: content/.config/logs/ (stored 0%)\n",
      "  adding: content/.config/logs/2025.03.06/ (stored 0%)\n",
      "  adding: content/.config/logs/2025.03.06/14.28.23.979271.log (deflated 92%)\n",
      "  adding: content/.config/logs/2025.03.06/14.28.44.811499.log (deflated 58%)\n",
      "  adding: content/.config/logs/2025.03.06/14.29.03.284363.log (deflated 56%)\n",
      "  adding: content/.config/logs/2025.03.06/14.28.53.350004.log (deflated 86%)\n",
      "  adding: content/.config/logs/2025.03.06/14.29.02.658299.log (deflated 57%)\n",
      "  adding: content/.config/logs/2025.03.06/14.28.54.467455.log (deflated 57%)\n",
      "  adding: content/.config/gce (stored 0%)\n",
      "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
      "  adding: content/.config/configurations/ (stored 0%)\n",
      "  adding: content/.config/configurations/config_default (deflated 15%)\n",
      "  adding: content/.config/config_sentinel (stored 0%)\n",
      "  adding: content/wandb/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060158-wvdqne23/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060158-wvdqne23/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060158-wvdqne23/logs/debug.log (deflated 92%)\n",
      "  adding: content/wandb/run-20250309_060158-wvdqne23/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060158-wvdqne23/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060158-wvdqne23/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061254-32ny8eyq/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061254-32ny8eyq/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061254-32ny8eyq/logs/debug.log (deflated 66%)\n",
      "  adding: content/wandb/run-20250309_061254-32ny8eyq/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061254-32ny8eyq/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061254-32ny8eyq/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052406-ymm1dj2y/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052406-ymm1dj2y/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052406-ymm1dj2y/logs/debug.log (deflated 94%)\n",
      "  adding: content/wandb/run-20250309_052406-ymm1dj2y/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052406-ymm1dj2y/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052406-ymm1dj2y/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051412-iwpbwzfs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051412-iwpbwzfs/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051412-iwpbwzfs/logs/debug.log (deflated 92%)\n",
      "  adding: content/wandb/run-20250309_051412-iwpbwzfs/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051412-iwpbwzfs/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051412-iwpbwzfs/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052111-47csr8az/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052111-47csr8az/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052111-47csr8az/logs/debug.log (deflated 92%)\n",
      "  adding: content/wandb/run-20250309_052111-47csr8az/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052111-47csr8az/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052111-47csr8az/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051848-jnjapd1l/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051848-jnjapd1l/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051848-jnjapd1l/logs/debug.log (deflated 92%)\n",
      "  adding: content/wandb/run-20250309_051848-jnjapd1l/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051848-jnjapd1l/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_051848-jnjapd1l/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052619-qwhn4szo/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052619-qwhn4szo/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052619-qwhn4szo/logs/debug.log (deflated 94%)\n",
      "  adding: content/wandb/run-20250309_052619-qwhn4szo/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052619-qwhn4szo/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052619-qwhn4szo/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/run-3cqr0r0y.wandb (deflated 61%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/logs/debug-internal.log (deflated 72%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/files/output.log (deflated 50%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/files/wandb-summary.json (deflated 37%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064111-3cqr0r0y/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 86%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/run-6wvcajb9.wandb (deflated 68%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/logs/debug-internal.log (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/files/output.log (deflated 48%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/files/wandb-summary.json (deflated 36%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_063205-6wvcajb9/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 86%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/run-ee4jyepw.wandb\n",
      "\tzip warning:  file size changed while zipping /content/wandb/run-20250309_075101-ee4jyepw/run-ee4jyepw.wandb\n",
      " (deflated 75%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/logs/debug-core.log (deflated 57%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/logs/debug.log (deflated 66%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/logs/debug-internal.log (deflated 66%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/files/output.log (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_075101-ee4jyepw/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/run-ftvugbna.wandb (deflated 61%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/logs/debug-internal.log (deflated 72%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/files/output.log (deflated 48%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/files/wandb-summary.json (deflated 38%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062131-ftvugbna/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 86%)\n",
      "  adding: content/wandb/latest-run/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/run-ee4jyepw.wandb (deflated 75%)\n",
      "  adding: content/wandb/latest-run/logs/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/logs/debug-core.log (deflated 57%)\n",
      "  adding: content/wandb/latest-run/logs/debug.log (deflated 66%)\n",
      "  adding: content/wandb/latest-run/logs/debug-internal.log (deflated 66%)\n",
      "  adding: content/wandb/latest-run/files/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/latest-run/files/output.log (deflated 88%)\n",
      "  adding: content/wandb/latest-run/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/latest-run/tmp/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_052900-68xxrwkg/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052900-68xxrwkg/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052900-68xxrwkg/logs/debug.log (deflated 94%)\n",
      "  adding: content/wandb/run-20250309_052900-68xxrwkg/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052900-68xxrwkg/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_052900-68xxrwkg/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/logs/debug-core.log (deflated 58%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/logs/debug.log (deflated 90%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/logs/debug-internal.log (deflated 83%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/run-sh1baqd8.wandb (deflated 82%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/files/output.log (deflated 90%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050206-sh1baqd8/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 81%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/run-t3oitca2.wandb (deflated 63%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/logs/debug-internal.log (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/files/output.log (deflated 62%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/files/wandb-summary.json (deflated 36%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_064526-t3oitca2/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 86%)\n",
      "  adding: content/wandb/run-20250309_060931-ajw5kqra/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060931-ajw5kqra/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060931-ajw5kqra/logs/debug.log (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_060931-ajw5kqra/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060931-ajw5kqra/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060931-ajw5kqra/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/logs/debug-internal.log (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/files/output.log (deflated 50%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/files/wandb-summary.json (deflated 39%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 87%)\n",
      "  adding: content/wandb/run-20250309_065722-fimb46k6/run-fimb46k6.wandb (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_054706-j5qdpptg/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_054706-j5qdpptg/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_054706-j5qdpptg/logs/debug.log (deflated 94%)\n",
      "  adding: content/wandb/run-20250309_054706-j5qdpptg/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_054706-j5qdpptg/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_054706-j5qdpptg/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/logs/debug-core.log (deflated 58%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/logs/debug.log (deflated 74%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/logs/debug-internal.log (deflated 77%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/run-bn68acqu.wandb (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/files/output.log (deflated 56%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_045118-bn68acqu/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 81%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/logs/debug-core.log (deflated 69%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/logs/debug-internal.log (deflated 72%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/files/output.log (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/files/wandb-summary.json (deflated 38%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 87%)\n",
      "  adding: content/wandb/run-20250309_071716-53crc634/run-53crc634.wandb (deflated 74%)\n",
      "  adding: content/wandb/run-20250309_055937-tkwjoh90/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055937-tkwjoh90/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055937-tkwjoh90/logs/debug.log (deflated 93%)\n",
      "  adding: content/wandb/run-20250309_055937-tkwjoh90/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055937-tkwjoh90/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055937-tkwjoh90/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/debug.log (deflated 66%)\n",
      "  adding: content/wandb/run-20250309_053443-xv2sk3a6/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053443-xv2sk3a6/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053443-xv2sk3a6/logs/debug.log (deflated 94%)\n",
      "  adding: content/wandb/run-20250309_053443-xv2sk3a6/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053443-xv2sk3a6/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053443-xv2sk3a6/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/run-76qwqfcf.wandb (deflated 66%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/logs/debug-internal.log (deflated 72%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/files/output.log (deflated 49%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/files/wandb-summary.json (deflated 37%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_065108-76qwqfcf/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/logs/debug-internal.log (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/files/output.log (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/files/wandb-summary.json (deflated 37%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_070123-eb8nws8j/run-eb8nws8j.wandb (deflated 74%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/logs/debug-internal.log (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/files/output.log (deflated 87%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/files/wandb-summary.json (deflated 37%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/run-jucvug3d.wandb (deflated 74%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_072435-jucvug3d/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 87%)\n",
      "  adding: content/wandb/run-20250309_055556-4r17zjln/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055556-4r17zjln/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055556-4r17zjln/logs/debug.log (deflated 93%)\n",
      "  adding: content/wandb/run-20250309_055556-4r17zjln/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055556-4r17zjln/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055556-4r17zjln/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/run-pgo2k51m.wandb (deflated 61%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/logs/debug.log (deflated 68%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/logs/debug-internal.log (deflated 74%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/files/output.log (deflated 47%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/files/wandb-summary.json (deflated 37%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061609-pgo2k51m/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 86%)\n",
      "  adding: content/wandb/run-20250309_044341-f4apl2jj/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044341-f4apl2jj/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044341-f4apl2jj/logs/debug.log (deflated 71%)\n",
      "  adding: content/wandb/run-20250309_044341-f4apl2jj/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044341-f4apl2jj/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044341-f4apl2jj/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/logs/debug-core.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/logs/debug.log (deflated 67%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/logs/debug-internal.log (deflated 72%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/files/output.log (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/files/wandb-summary.json (deflated 36%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_070650-jrd04irh/run-jrd04irh.wandb (deflated 74%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/run-mggkqhti.wandb (deflated 78%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/logs/debug-core.log (deflated 58%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/logs/debug.log (deflated 70%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/logs/debug-internal.log (deflated 66%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/files/output.log (deflated 90%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_073002-mggkqhti/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_055337-2hd8uplq/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055337-2hd8uplq/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055337-2hd8uplq/logs/debug.log (deflated 94%)\n",
      "  adding: content/wandb/run-20250309_055337-2hd8uplq/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055337-2hd8uplq/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_055337-2hd8uplq/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/logs/debug-core.log (deflated 69%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/logs/debug.log (deflated 92%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/logs/debug-internal.log (deflated 88%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/files/output.log (deflated 96%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/files/wandb-summary.json (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/run-hhoj95ft.wandb (deflated 83%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_050735-hhoj95ft/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 87%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/logs/debug-core.log (deflated 59%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/logs/debug.log (deflated 66%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/logs/debug-internal.log (deflated 64%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/files/output.log (deflated 39%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/run-thfrqsyb.wandb (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_061420-thfrqsyb/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053627-gr08g9p7/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053627-gr08g9p7/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053627-gr08g9p7/logs/debug.log (deflated 94%)\n",
      "  adding: content/wandb/run-20250309_053627-gr08g9p7/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053627-gr08g9p7/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_053627-gr08g9p7/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/run-jusab4cu.wandb (deflated 61%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/logs/debug-core.log (deflated 69%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/logs/debug.log (deflated 68%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/logs/debug-internal.log (deflated 74%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/files/output.log (deflated 48%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/files/wandb-summary.json (deflated 37%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_062627-jusab4cu/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 86%)\n",
      "  adding: content/wandb/run-20250309_060545-3lgaqefg/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060545-3lgaqefg/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060545-3lgaqefg/logs/debug.log (deflated 91%)\n",
      "  adding: content/wandb/run-20250309_060545-3lgaqefg/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060545-3lgaqefg/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_060545-3lgaqefg/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/run-e7510xx6.wandb (deflated 77%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/logs/debug-core.log (deflated 69%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/logs/debug.log (deflated 83%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/logs/debug-internal.log (deflated 81%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/files/config.yaml (deflated 73%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/files/output.log (deflated 54%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/files/wandb-summary.json (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20250309_044036-e7510xx6/tmp/code/LLM-Fine-tuning-Challenge-Enhancing-Qwen-2.5-3B-for-AI-Research-QA.ipynb (deflated 81%)\n",
      "  adding: content/wandb/debug-internal.log (deflated 66%)\n",
      "  adding: content/quantized_model/ (stored 0%)\n",
      "  adding: content/quantized_model/tokenizer.json (deflated 81%)\n",
      "  adding: content/quantized_model/merged/ (stored 0%)\n",
      "  adding: content/quantized_model/merged/added_tokens.json (deflated 67%)\n",
      "  adding: content/quantized_model/merged/config.json (deflated 47%)\n",
      "  adding: content/quantized_model/merged/model-00002-of-00002.safetensors (deflated 13%)\n",
      "  adding: content/quantized_model/merged/vocab.json (deflated 61%)\n",
      "  adding: content/quantized_model/merged/tokenizer.json (deflated 81%)\n",
      "  adding: content/quantized_model/merged/tokenizer_config.json (deflated 83%)\n",
      "  adding: content/quantized_model/merged/model.safetensors.index.json (deflated 96%)\n",
      "  adding: content/quantized_model/merged/generation_config.json (deflated 38%)\n",
      "  adding: content/quantized_model/merged/special_tokens_map.json (deflated 69%)\n",
      "  adding: content/quantized_model/merged/model-00001-of-00002.safetensors (deflated 14%)\n",
      "  adding: content/quantized_model/merged/merges.txt (deflated 57%)\n",
      "  adding: content/quantized_model/tokenizer_config.json (deflated 83%)\n",
      "  adding: content/quantized_model/model.gguf"
     ]
    }
   ],
   "source": [
    "!zip -r /content/content.zip /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VEsnLtAmIla"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(\"/content/content.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-MzBn9tIHub"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0095897d2219422a8ee49912cb2840a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06cdc025197d4e7081cccc7b0f393633": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07d170f3d5ed46f38b3fd89db3c6b571": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "09974d45d8a34b3191723395acd933a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a695b2f502e4a7d8878c75aa36da831": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0ab8fdad5e184610a74fea3da0e36091": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8be3848c81d345a6a32f269244141b3a",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aeb22d02a2d44b0b88d5c435e5e6554e",
      "value": 4
     }
    },
    "0ae0d8efcc3841119661313f3164df8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bf312de9de846a9b6fcbec8c133b0ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c7abf7953044ee4a9a46b48c5115457": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cfe4d466ed446cd8b9860ffc2a3ffea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e528df8602b439f9553f4de99b3de97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f80d25024c94e0cba2361976d6e429a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "11c4faaaf87f4e95978d7736df70b98b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_69c58c3cd33f4d1ab65cdfc18a642a9e",
       "IPY_MODEL_13d3874ed389447cb25aaf29f9af6dc0",
       "IPY_MODEL_c7d835034e8c45c18e25e60a5e559d89"
      ],
      "layout": "IPY_MODEL_0ae0d8efcc3841119661313f3164df8f"
     }
    },
    "13d3874ed389447cb25aaf29f9af6dc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da35c74cce134c9982d451ce94eaaf33",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b9d963050a44babbf420c02697ed11d",
      "value": 1
     }
    },
    "14f517d95ad34381a154f99791434629": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "184c18cf770e48969983a0fc0088c855": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18b8497d2fc6462ba6839c1d546f2333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "1a71efde72764ae688f95a8dfc3c9c08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d2446a66ce44341bcf9d0e4d084152d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57a8997217384432a55489e28ef6cb2e",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65c7245f4bde4bca92903df02c448012",
      "value": 15
     }
    },
    "1d5e6247d84045b4942510644c8d7926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "251c4924d2cf492cbbe495f73fcb6720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25e23a0485074053a64f0d9317ddb91a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_304ab46b292e44d8a5035089db59cc4a",
       "IPY_MODEL_293fbc77370a48ff914d0c840aaf317b",
       "IPY_MODEL_d1b6ca54c65b47e5bddb511e059b4403",
       "IPY_MODEL_6d3c6403b2694c28b63c93025f3de3b9",
       "IPY_MODEL_833206dbcdc441bca391157d09b86232"
      ],
      "layout": "IPY_MODEL_e7249ce079da4cd5bb28d3cfebc232e0"
     }
    },
    "270bae39974a4b93be70b8e2f7909915": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18b8497d2fc6462ba6839c1d546f2333",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c32ead0499d43f8ac873986c50d93b7",
      "value": 1
     }
    },
    "2714d10155324ce6a4c14503c7d79498": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "275250ee3d2e4a93b040926ae8e3bb49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27aeeb20d8d34e4d9613308ee9b22f88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27baea7e2317407e83610df61ef847c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27cfe6baa6ad46fba93f7b76d73e1a7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "284cdc77413c406f97395bdf4d57c6c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3fc7bd0dcef4dcfaf2c4be86e5d0b6d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4ebca178bc4d45c289af2c3ec11304c6",
      "value": "â€‡15/15â€‡[00:00&lt;00:00,â€‡194.92â€‡examples/s]"
     }
    },
    "293fbc77370a48ff914d0c840aaf317b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_3fd283f12f1444ddbe04f30bb0329c6b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3c509f7b3cd5436483a52f36a9edb4b3",
      "value": ""
     }
    },
    "2bd06e530e1143e18c59fc12201dc957": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dba3c40dd9a47648ab0f9211fc61e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca29a12766b84416a647e00ab81e3c83",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_35ccdeb4e91a4ad08a2cf622e3744840",
      "value": "â€‡6/0â€‡[00:00&lt;00:00,â€‡374.28â€‡examples/s]"
     }
    },
    "304ab46b292e44d8a5035089db59cc4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8750ce4493a542908a75112341b6c3bc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d16371a97c07429683d3f51dfc4dbae8",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "3151f5730ccc4962a7baed78b56b1971": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb86c293f0924187b05577bafcecdcee",
       "IPY_MODEL_0ab8fdad5e184610a74fea3da0e36091",
       "IPY_MODEL_83951e88e9734934b9ba8b906d42335c"
      ],
      "layout": "IPY_MODEL_e7bc49081c394355ad44d83c567f6c39"
     }
    },
    "33e1803ff2284dbc9040cfec3ad93592": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "348055a6fd0749c382c8b08d51f61d1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35ccdeb4e91a4ad08a2cf622e3744840": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "370189690d554782bbd97cad0df9d1b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a4f6956c7244057bea194a4da13ed3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c32ead0499d43f8ac873986c50d93b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c509f7b3cd5436483a52f36a9edb4b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e34688137754dc890dff558cced54ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f579bb984cf4953957ed1f6ca311441": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fd283f12f1444ddbe04f30bb0329c6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "414ff5b4353646c3bf1d78bcb31639df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4688175e18224050bb9bba25cfab583f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4da832270a4146128240151b58955d64",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_251c4924d2cf492cbbe495f73fcb6720",
      "value": 1
     }
    },
    "46c319439436435dbb0fee9a7516a4bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4da832270a4146128240151b58955d64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ebca178bc4d45c289af2c3ec11304c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f4154b01bc940d6b6195c90766febe4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27baea7e2317407e83610df61ef847c0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8f3399d422fe4ecd81c1868c2a8e2266",
      "value": "Map:â€‡100%"
     }
    },
    "5117d6b06ff7498384f92c17263fd8c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5690e187cc884e1db8555f17d9e26ce2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "57a8997217384432a55489e28ef6cb2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c0db21b600c4d17ac613dc83e44e4ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f1caff87551428081c54ec44df1318e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65c7245f4bde4bca92903df02c448012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66a77e1f57234990b22a0dd23a31d48b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e4eb2ce65c5a4922969c617bd9bb413a",
       "IPY_MODEL_ddd13c80003a4950837b1ff6c937be5c",
       "IPY_MODEL_7042c4ceb2ad4c34a089d39bcd3a7c24"
      ],
      "layout": "IPY_MODEL_0e528df8602b439f9553f4de99b3de97"
     }
    },
    "67e3c4f773fb433caa3d5679920846b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69c58c3cd33f4d1ab65cdfc18a642a9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e34688137754dc890dff558cced54ba",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1d5e6247d84045b4942510644c8d7926",
      "value": "Generatingâ€‡trainâ€‡split:â€‡"
     }
    },
    "6a3609938b6b456aa7c6920da78f2861": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c36d99df80b4aec8f0982d519fb068c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9e537c90972d4ef399d5c5b78d466d6e",
       "IPY_MODEL_c70be734e55346cfb9496d0df49b1c6f",
       "IPY_MODEL_7af43cdaf3604d59a81a2764f5053528"
      ],
      "layout": "IPY_MODEL_27aeeb20d8d34e4d9613308ee9b22f88"
     }
    },
    "6d23e73833864b1f9eae4a788092f9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46c319439436435dbb0fee9a7516a4bd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fd9a0bbef8254a74b9b39213147bf049",
      "value": "Creatingâ€‡jsonâ€‡fromâ€‡Arrowâ€‡format:â€‡100%"
     }
    },
    "6d3c6403b2694c28b63c93025f3de3b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_c5c1ee7fe1d74f08a50deabb7a47a7b1",
      "style": "IPY_MODEL_07d170f3d5ed46f38b3fd89db3c6b571",
      "tooltip": ""
     }
    },
    "6de8c451ca8e4fba9d74773bac463782": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7042c4ceb2ad4c34a089d39bcd3a7c24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09974d45d8a34b3191723395acd933a3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b9a2c3e22e4a499997399d58fe46f488",
      "value": "â€‡2/2â€‡[00:04&lt;00:00,â€‡â€‡2.14s/it]"
     }
    },
    "71d5278ed1d1402f9e83870082734449": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa0af8b1ed83453ab715a8301487b6e8",
       "IPY_MODEL_fdc737fb2ae842afadb26fe9926d70c7",
       "IPY_MODEL_c2b701181f3d4fb4a723c6bc34f720c2"
      ],
      "layout": "IPY_MODEL_5117d6b06ff7498384f92c17263fd8c4"
     }
    },
    "723b7083097f49179884786c4c145034": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "734ab7dd46db4959aa3769a7ab4dd4c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cbc8069c7bab49e99731fba704915906",
       "IPY_MODEL_f52def45289341ffa82930300e341c0c",
       "IPY_MODEL_baaf84171f714d23930dc50efa3bd835"
      ],
      "layout": "IPY_MODEL_f2a01c85e40144119f2624b89502ce51"
     }
    },
    "7484d0b763cf4204bb385d6befc33ff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75b183a37d3b4bea8d02400d96d674a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be079351f7824516aeafdce08a3387fc",
       "IPY_MODEL_1d2446a66ce44341bcf9d0e4d084152d",
       "IPY_MODEL_284cdc77413c406f97395bdf4d57c6c0"
      ],
      "layout": "IPY_MODEL_14f517d95ad34381a154f99791434629"
     }
    },
    "78c0a13b68db47628ac93e9527bd3356": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7af43cdaf3604d59a81a2764f5053528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f579bb984cf4953957ed1f6ca311441",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9ba3af643ec745e989d821ad4ad89910",
      "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡81.88ba/s]"
     }
    },
    "7b9d963050a44babbf420c02697ed11d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7dda3d27453e4a7896714b8369282b15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "803d7d52699446d3a461937e1f012813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "833206dbcdc441bca391157d09b86232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe469e8567224a1c9d2b5a6f7aed4bd9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1a71efde72764ae688f95a8dfc3c9c08",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "83951e88e9734934b9ba8b906d42335c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2714d10155324ce6a4c14503c7d79498",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_df4c9a87f85c457dbf4727eb8c10c7c9",
      "value": "â€‡4/4â€‡[00:11&lt;00:00,â€‡â€‡2.71s/it]"
     }
    },
    "8750ce4493a542908a75112341b6c3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b6ecfbcea944f0d933caf15a5f13875": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8be3848c81d345a6a32f269244141b3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f3399d422fe4ecd81c1868c2a8e2266": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "988c99222264457cab97ef8a9ca689cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9b6b8eb049a24d289acd2588e9f9fb09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ba3af643ec745e989d821ad4ad89910": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bc414b798d549c4a21aced99dfb562c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c041d59aea5443688bb4cf0fcba4937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9da7fe111dfd42f1a499f77b5da3fa3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9e537c90972d4ef399d5c5b78d466d6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1d89fc291c24e728da03d17a27ab785",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ddc800b4d20d4d03ae59a7b5c20b4680",
      "value": "Creatingâ€‡jsonâ€‡fromâ€‡Arrowâ€‡format:â€‡100%"
     }
    },
    "a3fc7bd0dcef4dcfaf2c4be86e5d0b6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa0af8b1ed83453ab715a8301487b6e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06cdc025197d4e7081cccc7b0f393633",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_33e1803ff2284dbc9040cfec3ad93592",
      "value": "Creatingâ€‡jsonâ€‡fromâ€‡Arrowâ€‡format:â€‡100%"
     }
    },
    "ab564f2a1c734326a03159c4c1b56791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d23e73833864b1f9eae4a788092f9f7",
       "IPY_MODEL_4688175e18224050bb9bba25cfab583f",
       "IPY_MODEL_cb01e4b0fbde4c6f94f7c78ab15f8f85"
      ],
      "layout": "IPY_MODEL_0bf312de9de846a9b6fcbec8c133b0ea"
     }
    },
    "aeb22d02a2d44b0b88d5c435e5e6554e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b0216412c402433e9f31cb15c5208c9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b6ecfbcea944f0d933caf15a5f13875",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7484d0b763cf4204bb385d6befc33ff2",
      "value": "Generatingâ€‡trainâ€‡split:â€‡"
     }
    },
    "b2d0e0bd72d04d9f938dfd832dce957b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c7abf7953044ee4a9a46b48c5115457",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5690e187cc884e1db8555f17d9e26ce2",
      "value": 2
     }
    },
    "b362acc5bdc44f5fabffe15e1beea57c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_348055a6fd0749c382c8b08d51f61d1d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_184c18cf770e48969983a0fc0088c855",
      "value": "â€‡6/6â€‡[00:00&lt;00:00,â€‡138.92â€‡examples/s]"
     }
    },
    "b9a2c3e22e4a499997399d58fe46f488": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "baaf84171f714d23930dc50efa3bd835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1ec16bd07de4008aff1156b5426ed75",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9bc414b798d549c4a21aced99dfb562c",
      "value": "â€‡15/0â€‡[00:00&lt;00:00,â€‡937.36â€‡examples/s]"
     }
    },
    "be079351f7824516aeafdce08a3387fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_414ff5b4353646c3bf1d78bcb31639df",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5c0db21b600c4d17ac613dc83e44e4ed",
      "value": "Map:â€‡100%"
     }
    },
    "bf1885a5bbbb44688079028132f3cc06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c1455b2e7b0f47dbae672d40ad3cb699": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c211bc574ad5440da5d4d96ec2864781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2b701181f3d4fb4a723c6bc34f720c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eea5e6322c33402eb75f6d0bc16173b9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5f1caff87551428081c54ec44df1318e",
      "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡77.57ba/s]"
     }
    },
    "c5c1ee7fe1d74f08a50deabb7a47a7b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c70be734e55346cfb9496d0df49b1c6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1455b2e7b0f47dbae672d40ad3cb699",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a695b2f502e4a7d8878c75aa36da831",
      "value": 1
     }
    },
    "c7d835034e8c45c18e25e60a5e559d89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0095897d2219422a8ee49912cb2840a5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d8a901f571b64416ab8479384d8a7028",
      "value": "â€‡6/0â€‡[00:00&lt;00:00,â€‡414.33â€‡examples/s]"
     }
    },
    "ca29a12766b84416a647e00ab81e3c83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca9fbe3fea5a4d9ba70d0f69e35773ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb01e4b0fbde4c6f94f7c78ab15f8f85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67e3c4f773fb433caa3d5679920846b1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6de8c451ca8e4fba9d74773bac463782",
      "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡88.19ba/s]"
     }
    },
    "cbc8069c7bab49e99731fba704915906": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78c0a13b68db47628ac93e9527bd3356",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d81db81fac854bc4bc17c9de580c7f74",
      "value": "Generatingâ€‡trainâ€‡split:â€‡"
     }
    },
    "d16371a97c07429683d3f51dfc4dbae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1b6ca54c65b47e5bddb511e059b4403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_6a3609938b6b456aa7c6920da78f2861",
      "style": "IPY_MODEL_c211bc574ad5440da5d4d96ec2864781",
      "value": true
     }
    },
    "d1b7a3db9d984690a67d63ee4eed2740": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0216412c402433e9f31cb15c5208c9e",
       "IPY_MODEL_270bae39974a4b93be70b8e2f7909915",
       "IPY_MODEL_2dba3c40dd9a47648ab0f9211fc61e7b"
      ],
      "layout": "IPY_MODEL_370189690d554782bbd97cad0df9d1b1"
     }
    },
    "d1ec16bd07de4008aff1156b5426ed75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d52305c00196472ebe8f72d5038d4329": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7519f78d7e14a19ae273010ce2e9b3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f4154b01bc940d6b6195c90766febe4",
       "IPY_MODEL_f062efc9248c484e9de0956fea123753",
       "IPY_MODEL_b362acc5bdc44f5fabffe15e1beea57c"
      ],
      "layout": "IPY_MODEL_f3924e36572b46bbba4dfee0ba081112"
     }
    },
    "d81db81fac854bc4bc17c9de580c7f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8a901f571b64416ab8479384d8a7028": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da30c7f6f52a48f5b9b1e49886241c9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7dda3d27453e4a7896714b8369282b15",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d52305c00196472ebe8f72d5038d4329",
      "value": "â€‡2/2â€‡[00:04&lt;00:00,â€‡â€‡2.11s/it]"
     }
    },
    "da35c74cce134c9982d451ce94eaaf33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "dda01da9cff141ee92cdd8186ebd48dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ea954d402c854417aa5cec3881c461d2",
       "IPY_MODEL_b2d0e0bd72d04d9f938dfd832dce957b",
       "IPY_MODEL_da30c7f6f52a48f5b9b1e49886241c9a"
      ],
      "layout": "IPY_MODEL_e0502f1c70b24511adf779ca5c649ddf"
     }
    },
    "ddc800b4d20d4d03ae59a7b5c20b4680": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ddd13c80003a4950837b1ff6c937be5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca9fbe3fea5a4d9ba70d0f69e35773ae",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf1885a5bbbb44688079028132f3cc06",
      "value": 2
     }
    },
    "df4c9a87f85c457dbf4727eb8c10c7c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0502f1c70b24511adf779ca5c649ddf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4eb2ce65c5a4922969c617bd9bb413a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c041d59aea5443688bb4cf0fcba4937",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_275250ee3d2e4a93b040926ae8e3bb49",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "e7249ce079da4cd5bb28d3cfebc232e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "e7bc49081c394355ad44d83c567f6c39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea954d402c854417aa5cec3881c461d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cfe4d466ed446cd8b9860ffc2a3ffea",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9b6b8eb049a24d289acd2588e9f9fb09",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "eea5e6322c33402eb75f6d0bc16173b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f062efc9248c484e9de0956fea123753": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27cfe6baa6ad46fba93f7b76d73e1a7e",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9da7fe111dfd42f1a499f77b5da3fa3d",
      "value": 6
     }
    },
    "f1d89fc291c24e728da03d17a27ab785": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a01c85e40144119f2624b89502ce51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3924e36572b46bbba4dfee0ba081112": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f52def45289341ffa82930300e341c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_988c99222264457cab97ef8a9ca689cb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0f80d25024c94e0cba2361976d6e429a",
      "value": 1
     }
    },
    "fb86c293f0924187b05577bafcecdcee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bd06e530e1143e18c59fc12201dc957",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_803d7d52699446d3a461937e1f012813",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "fd9a0bbef8254a74b9b39213147bf049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdc737fb2ae842afadb26fe9926d70c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_723b7083097f49179884786c4c145034",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a4f6956c7244057bea194a4da13ed3d",
      "value": 1
     }
    },
    "fe469e8567224a1c9d2b5a6f7aed4bd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
