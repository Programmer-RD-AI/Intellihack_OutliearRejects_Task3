{"title":"open-source-week","chunk_id":0,"context":"202502 Open-Source Week\n\nWe're a tiny team @deepseek-ai pushing our limits in AGI exploration. Starting this week , Feb 24, 2025 we'll open-source 5 repos \u2013 one daily drop \u2013 not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency. These are humble building blocks of our online service: documented, deployed and battle-tested in production. No vaporware, just sincere code that moved our tiny yet ambitious dream forward. Why? Because every line shared becomes collective momentum that accelerates the journey. Daily unlocks begin soon. No ivory towers - just pure garage-energy and community-driven innovation \ud83d\udd27\n\nStay tuned \u2013 let's geek out in the open together. Day 1 - FlashMLA\n\nEfficient MLA Decoding Kernel for Hopper GPUs\nOptimized for variable-length sequences, battle-tested in production\n\n\ud83d\udd17 FlashMLA GitHub Repo\n\u2705 BF16 support\n\u2705 Paged KV cache (block size 64)\n\u26a1 Performance: 3000 GB\/s memory-bound | BF16 580 TFLOPS compute-bound on H800\n\nDay 2 - DeepEP\n\nExcited to introduce DeepEP - the first open-source EP communication library for MoE model training and inference. \ud83d\udd17 DeepEP GitHub Repo\n\u2705 Efficient and optimized all-to-all communication\n\u2705 Both intranode and internode support with NVLink and RDMA\n\u2705 High-throughput kernels for training and inference prefilling\n\u2705 Low-latency kernels for inference decoding\n\u2705 Native FP8 dispatch support\n\u2705 Flexible GPU resource control for computation-communication overlapping\n\nDay 3 - DeepGEMM\n\nIntroducing DeepGEMM - an FP8 GEMM library that supports both dense and MoE GEMMs, powering V3\/R1 training and inference. \ud83d\udd17 DeepGEMM GitHub Repo\n\u26a1 Up to 1350+ FP8 TFLOPS on Hopper GPUs\n\u2705 No heavy dependency, as clean as a tutorial\n\u2705 Fully Just-In-Time compiled\n\u2705 Core logic at ~300 lines - yet outperforms expert-tuned kernels across most matrix sizes\n\u2705 Supports dense layout and two MoE layouts\n\nDay 4 - Optimized Parallelism Strategies\n\n\u2705 DualPipe - a bidirectional pipeline parallelism algorithm for computation-communication overlap in V3\/R1 training. \ud83d\udd17 GitHub Repo\n\n\u2705 EPLB - an expert-parallel load balancer for V3\/R1. \ud83d\udd17 GitHub Repo\n\n\ud83d\udcca Analyze computation-communication overlap in V3\/R1. \ud83d\udd17 GitHub Repo\n\nDay 5 - 3FS, Thruster for All DeepSeek Data Access\n\nFire-Flyer File System (3FS) - a parallel file system that utilizes the full bandwidth of modern SSDs and RDMA networks. \u26a1 6.6 TiB\/s aggregate read throughput in a 180-node cluster\n\u26a1 3.66 TiB\/min throughput on GraySort benchmark in a 25-node cluster\n\u26a1 40+ GiB\/s peak throughput per client node for KVCache lookup\n\ud83e\uddec Disaggregated architecture with strong consistency semantics\n\u2705 Training data preprocessing, dataset loading, checkpoint saving\/reloading, embedding vector search & KVCache lookups for inference in V3\/R1\n\n\ud83d\udce5 3FS \u2192 https:\/\/github.com\/deepseek-ai\/3FS\n\u26f2 Smallpond - data processing framework on 3FS \u2192 https:\/\/github.com\/deepseek-ai\/smallpond\n\nDay 6 - One More Thing: DeepSeek-V3\/R1 Inference System Overview\n\nOptimized throughput and latency via:\n\ud83d\udd27 Cross-node EP-powered batch scaling\n\ud83d\udd04 Computation-communication overlap\n\u2696\ufe0f Load balancing\n\nProduction data of V3\/R1 online services:\n\u26a1 73.7k\/14.8k input\/output tokens per second per H800 node\n\ud83d\ude80 Cost profit margin 545%","question":"What does the text say about Open-Source Week We're?","answer":"202502 Open-Source Week\n\nWe're a tiny team @deepseek-ai pushing our limits in AGI exploration. Starting this week , Feb 24, 2025 we'll open-source 5 repos \u2013 one daily drop \u2013 not because we've made grand claims, but simply as developers sharing our small-but-sincere progress with full transparency"}
